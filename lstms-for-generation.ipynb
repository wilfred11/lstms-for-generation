{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62688ba4",
   "metadata": {},
   "source": [
    "# LSTMs for generation\n",
    "\n",
    "In this notebook I will present and describe some LSTM based text generation models. I have trained them and generated text using them locally, training these models too extensively was a bit too slow. I just investigated their behavior while training, this in terms of loss evolution and quality of the generated text. \n",
    "\n",
    "They are not adapted to allow for training on a more advanced system. Also the way data is used would probably get an upgrade when trained on specialized systems. Also the tokenizer would probably be something else. And the models would probably have evolved more too.\n",
    "\n",
    "But overall I am happy to have gained some insight in the usage of LSTMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PathLib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wakepy in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "DATA_RELATED_DIR = 'data_related'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = pathlib.Path(DATADIR).joinpath('harry_potter_text')\n",
    "dirs=[DATA_RELATED_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b838f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generate_vocabulary(individual_words, include_special_tokens=False):\\n    condition_keys = sorted(individual_words)\\n    print(conditions.unique())\\n    result = dict(zip(condition_keys, range(len(condition_keys))))\\n    print(len(result))\\n    return result\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def generate_vocabulary(individual_words, include_special_tokens=False):\n",
    "    condition_keys = sorted(individual_words)\n",
    "    print(conditions.unique())\n",
    "    result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "    print(len(result))\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e51d39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold, include_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur min_threshold times or more,\n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    \"\"\"\n",
    "    #create a list of words that happen min_threshold times or more in that string\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    #generate the vocabulary(dictionary)\n",
    "\n",
    "\n",
    "    if not include_special_tokens:\n",
    "        result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "        return result\n",
    "    else:\n",
    "        result = dict(zip(condition_keys, range(3,len(condition_keys)+3)))\n",
    "        orig = {\"BOS\": 0, \"EOS\": 1, \"UNK\": 2}\n",
    "        orig.update(result)\n",
    "        return orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70269f",
   "metadata": {},
   "source": [
    "### Get text\n",
    "\n",
    "This piece of code takes all text files in a directory, and does some cleaning, substitutes 'weird' characters using a regular expression. And finally it returns and saves the cleaned text. If cleaned text is already created it opens the file, and deserializes the it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text():\n",
    "    text_files = pathlib.Path(HP_TEXT_DIR).iterdir()\n",
    "    path_to_hp_text = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_text.pkl\")\n",
    "\n",
    "    filepath = pathlib.Path(DATA_RELATED_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    if not path_to_hp_text.exists():\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             path_to_book = pathlib(HP_TEXT_DIR).joinpath(HP_TEXT_DIR, book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d673db",
   "metadata": {},
   "source": [
    "### Get tokens\n",
    "\n",
    "The cleaned text returned from the previous function is used to create tokenids. As I am not that specialized in language models, I have used some tokenizer, not knowing whether it is the best option. I am just happy to get some tokens. The tokens are serialized and saved, for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_tokens.pkl\")\n",
    "    if not path_to_tokens.exists():\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1b9ec",
   "metadata": {},
   "source": [
    "### Vocabularies\n",
    "\n",
    "The tokens are used to generate two complementary vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not path_to_vocab.exists():\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ec5a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilderMemmap:\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len,  word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.word2idx=word2idx\n",
    "        self.idx2word=idx2word\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0d77461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memmap(tokenized_text, word2idx):\n",
    "    path_to_memmap = pathlib.Path().cwd().joinpath(DATA_RELATED_DIR,\"hp_memmap.dat\")\n",
    "   \n",
    "    #pathlib.Path(path_to_memmap).rename(path_to_memmap)\n",
    "    #pathlib.Path(path_to_memmap).unlink(missing_ok=True)\n",
    "\n",
    "    tt=np.asarray([word2idx[w] for w in tokenized_text], dtype=np.int32)\n",
    "    f = np.memmap(path_to_memmap, dtype=np.int32, mode='w+', shape=tt.shape)\n",
    "    f[:] = tt[:]\n",
    "\n",
    "    return path_to_memmap, tt.shape, f\n",
    "\n",
    "def get_databuildermemmap(seq_len, tokens, word2idx, idx2word):\n",
    "    path_to_memmap, shape_of_memmap, f = create_memmap(tokens, word2idx)\n",
    "\n",
    "    db=DataBuilderMemmap(path_to_memmap, shape_of_memmap,seq_len,  word2idx, idx2word)\n",
    "    return db, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89129f7a",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data is produced by a simple class, it produces batches of sentences and the sentences that should be produced.\n",
    "\n",
    "So if the sentence from which to create a new token would be:\n",
    "\n",
    "` To be or not to be, that's the `\n",
    "\n",
    "Than the 'target' sentence could be\n",
    "\n",
    "` be or not to be, that's the question `\n",
    "\n",
    "The tokens parameter is a complete tokenized version of the complete text, which is sliced using random int values, while keeping some set length. This part should probably be implemented using an arraymap which is more memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wilfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e9baa",
   "metadata": {},
   "source": [
    "## Simple word generation model\n",
    "\n",
    "### Forward method\n",
    "\n",
    "This model is a simple model to embed tokens that represent word ids. Its forward method takes in a complete sentence, produces an embedding, passes this embedding to the LSTM block which creates an output that is passed to a linear layer to produce logits, for every input. As a complete piece of text is passed to an LSTM at once, no further adaptations can be applied during training. \n",
    "\n",
    "### Write method\n",
    "\n",
    "Contrary to the forward method, the write method generates one token at the time. In this way multiple types of generations are possible. Furthermore it doesn't differ that much from the forward method, it just produces one token at the time. And allows for different ways of text generation, just pick the next token that is most probable. The other option selects a token uisng probabilities, so basically even the worst fit could be chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9f044",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The loss function is cross entropy loss. The optimizer is AdamW. AdamW is an optimization algorithm, developed as a modification to the Adam optimizer to decouple weight decay from gradient-based updates. This decoupling was introduced to address overfitting issues that often arise when using standard Adam, especially for large-scale neural network models.\n",
    "\n",
    "The training function produces iterational data like loss, text produced, model name. The configuration file is saved too, for investigation afterwwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch import optim, nn\n",
    "import gc\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config, write_better=False):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    texts_generated = {\n",
    "        \"iteration\": [],\n",
    "        \"loss\":[],\n",
    "        \"probabilities\":[],\n",
    "        \"texts\": [],\n",
    "    }\n",
    "\n",
    "    name=model.__class__.__name__\n",
    "    new_dir = pathlib.Path(TRAINED_MODELS_DIR).joinpath(name)\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    np.random.seed(0)\n",
    "    #dataset = DataBuilder(max_len, tokens, word2idx, idx2word)\n",
    "    dataset, f = get_databuildermemmap(max_len, tokens, word2idx, idx2word)\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training data length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            if write_better:\n",
    "                generated_texts, probabilities = model.write_better([\"Spells\"],max_words=50, k=3)\n",
    "                print(\"Sample Generation\")\n",
    "                for text, probability in zip(generated_texts, probabilities):\n",
    "                    print(f\"text: {text} probability: {probability}\")\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "\n",
    "            if write_better:\n",
    "                texts_generated[\"iteration\"].append(iteration)\n",
    "                texts_generated[\"loss\"].append(loss.item())\n",
    "                texts_generated[\"probabilities\"].append(probabilities)\n",
    "                texts_generated[\"texts\"].append(generated_texts)\n",
    "\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "    td=pd.DataFrame(training_data)\n",
    "    td.set_index([\"model\", \"iteration\"])\n",
    "    td.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    cd=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "\n",
    "    cd.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "\n",
    "    if write_better:\n",
    "        with open(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,'generated_texts.pkl'), 'wb') as fh:\n",
    "            pickle.dump(texts_generated, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    torch.save(model.state_dict(), pathlib.Path(TRAINED_MODELS_DIR).joinpath(name+\"/model_state.pth\"))\n",
    "    del dataset\n",
    "    del f\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7b6d4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "To setup the model and some training parameters. I use some configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c774f",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Get the data and related constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a14ab0",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilfr\\code-projects\\lstms-for-generation\\data_related\\hpmemmap.dat\n",
      "(90067,)\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.844343185424805\n",
      "Spells mistaken kitchen stutter NICHOLAS railway face baking rot himself dropping going fastest Taped uneasy dust long Excuses haircut legs ice-cold killers silent oughta Isle buckled movements kill Prophet Young granite loyalties notes happening basking free Want supposed referees dinner Scowling froze toil meet While Counter-curses towered wore dried Wriggling listening\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 6.675443649291992\n",
      "Spells P-P-Potter close really So long I this know the followers ! and , at Harry year and holding backed nearly even it wait Hes a of don us kept at without game on Hall to its the him is . up seemed seem and you there best George the on\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 6.561707973480225\n",
      "Spells over you but Hagrid often have Malfoy will , dont bet treble cant . hangings living Whats . there Youve Binns famous steal a Stone never from ? one unless with say to the ... , now been at some leaves even for but let pieces I came worried out\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 6.534857273101807\n",
      "Spells day . a There , magic Dark ! me years Got ! to , back he , awake cauldron and the out the Ron he became Voldemort tear-streaked hour new it shook Let lurking the If cranberry cup Malfoy I had silver , Neville . bossy to to , other\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 6.528787612915039\n",
      "Spells an from Uncle brave from it shortly and Hermione had dark sleep the as budge . an to , year the you go the It even he , dark most the no Harry his we of Ron the But that that knew the Voldemort hope the the he Harry how\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 6.454123497009277\n",
      "Spells second . visit forehead . strict . hair all care what , very this scales field beetles points everyone the toward bike . I amazement very . moment the tried the was a after Gryffindor honestly Harrys all eyes the Voldemort up ! you to bed over do sped do\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 6.365240573883057\n",
      "Spells giving forever One moment called leapt , and your said wanted behind the ? it I Did saying week around its explain . When Leaky pulled . It , looked a two pain dungeons and surprised dinner his was woman he hoping his families sandy-haired Pig . been For them\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 6.165666103363037\n",
      "Spells . because anything , never Privet proud in good in . asleep was all . have claws never said row , because a black-haired and In , Harry and right as mustnt trouble Hagrid , day . was Voldemort up the dying last and squashed room In of out at\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 6.0883049964904785\n",
      "Spells . It first Oddment happened in the Algie ? You Dursleys care the dancing it shut . Stone at in to try . He bolted can Malfoys in ? Ha Still awarding ! lot , went he could . Not families ? building a see gave speeding , Bloody edge\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 6.098926067352295\n",
      "Spells what whats , Well if ever put . It voice . Pasties his Hooch in the ground . And that hope out of back , Perenelle very sorry . He its looking I Harrys all , cereal had because on around what so look at legs and listen on his\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config()\n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef789f22",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "To get an idea of what attention means mathematically let's compare some matrix multiplications. To get an idea of what attention means mathematically let's compare some matrix multiplications. These calculations are a means of expressing the mechanisms behind attention. Most of the time q(uery) and k(ey) matrices will contain much more complexity than expressed by the matrix multiplications below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438aa54",
   "metadata": {},
   "source": [
    "### Some direct multiplications\n",
    "\n",
    "Firstly, a relatively straight matrix multiplication of three matrices. In this case the third element of the q(uery) matrix is a 1, all the rest are zeroes. Matrix multiplying q with the one hot encoded k(eys) selects the third row of this k(eys) matrix. As a consequence the adapted k(eys) matrix selects the third v(alues) row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "k\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "v\n",
      "tensor([[5, 2],\n",
      "        [2, 4],\n",
      "        [4, 5],\n",
      "        [5, 4],\n",
      "        [7, 2]])\n",
      "q_times_k\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "q_times_k_times_v\n",
      "tensor([[4, 5]])\n"
     ]
    }
   ],
   "source": [
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "v = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b0f0",
   "metadata": {},
   "source": [
    "### Some softer multiplications\n",
    "\n",
    "In this case the q(uery) isn't as decisive as before, the q matrix is softer. In a numeric world the softer values are responsible for some weighted v(alues) matrix. As the keys stay the same, every value in the v(alues) matrix counts except the first, as the first value in the q(ueriy) matrix is zero. The other v(alues) are weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "v\n",
      "tensor([[5., 2.],\n",
      "        [2., 4.],\n",
      "        [4., 5.],\n",
      "        [5., 4.],\n",
      "        [7., 2.]])\n",
      "q_times_k\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "q_times_k_times_v\n",
      "tensor([[4.1000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf947",
   "metadata": {},
   "source": [
    "### Softer keys\n",
    "\n",
    "As the keys of matrices in deep learning most of the times aren't that explicitly defined, I have adjusted these too, slightly, to see some effect. I have adjusted the keys for the second v(alues) row. As a result these values changed, from [2,5] to [2.5,4.0]. This change has its effect on the final result of the complete matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f39a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "v\n",
      "tensor([[5., 2.],\n",
      "        [2., 4.],\n",
      "        [4., 5.],\n",
      "        [5., 4.],\n",
      "        [7., 2.]])\n",
      "k_times_data\n",
      "tensor([[5.0000, 2.0000],\n",
      "        [3.0000, 4.5000],\n",
      "        [4.0000, 5.0000],\n",
      "        [5.0000, 4.0000],\n",
      "        [7.0000, 2.0000]])\n",
      "q_times_k_times_v\n",
      "tensor([[4.4000, 4.1500]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "k[1,1]=0.5\n",
    "k[2,1]=0.5\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "k_times_v = torch.mm(k.T, v)\n",
    "print(\"k_times_data\")\n",
    "print(k_times_v)\n",
    "\n",
    "q_times_k_times_v=torch.mm(q,k_times_v)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(q_times_k_times_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b4a4",
   "metadata": {},
   "source": [
    "### The attention usage\n",
    "\n",
    "In this case attention is used to adjust word/token embeddings to express semantical or syntactical relations between different parts of a sentence. As I only use Attention once, it can only convey the information it is capable of conveying, as a sentence can be composed of multiple semantical and syntactical twists this is probably too limited for the texts it is trained on. It should provide better results than a model without attention.\n",
    "\n",
    "A difference that can be observed in the Attention class definition is the bmm operation, it is a batched version of the matrix multiplication. There exists an existing MultiHeadAttention module in torch, but I found it easier to use a custom class. But from what I know, the MultiheadAttention class can be used to implement the behavior implemented by this custom Attention class. For every timestep, a different query and value vector will be entered (the value vector is used for the key vector too). In the end diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions=2):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.n_layers =n_layers\n",
    "        self.n_directions=n_directions\n",
    "        self.Q=nn.Linear(self.n_directions*self.n_layers*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        #print(queries.shape)\n",
    "        #print(keys.transpose(1,2).shape)\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c603",
   "metadata": {},
   "source": [
    "### The model with attention\n",
    "\n",
    "As it is not desirable to apply attention to an LSTM block beign used to generate hidden and cell values while it is generating these values. A secondary LSTM block is used to absorb the results of applying attention to the values produced by the first LSTM block.\n",
    "\n",
    "### The write_better method\n",
    "\n",
    "The write_better method is a beam search implementation. It should generate text better as it considers the probability of a sentence as a whole, in stead of generating a word based on the highest probable word at a particular timestep. In fact, to lower computational complexity, only a k number of possibilities are considered for every timestep. As I have tried to adapt the hidden an cell values based on the highest 4 possible next words generated, the method looks and is quiet complex to capture and interpret. As a matter of fact, it is a bit hard to evaluate its correctness/soundness without a properly trained model. My current system is not able to train a model having more than 2 layers and a hidden dimension of 256 within a decent timespan. That's why I have always kept the simpler write method as a safehaven.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a05dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGenerationWithAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = Attention(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "    \n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            #print(\"out lstm2\")\n",
    "            #print(out_lstm2.shape)\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "            h = h.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            c = c.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "            # print(\"hidden, cell end loop\")\n",
    "            # print(hidden.shape)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        #print(k_times_probs)\n",
    "        return gen_strings, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilfr\\code-projects\\lstms-for-generation\\data_related\\hpmemmap.dat\n",
      "(90067,)\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.84005355834961\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 512, 1])\n",
      "Spells cool two dropping unless For pretty outstretched Starving LOOK wrote moth-eaten dull Festoons ketchup Wheres Halloween Yours booger-flavored bartender return exactly Gregory loop-the-loops mail Bars twenty-six Youll Makes wide-awake looming backward flowing Christmas Wed Cup mighta does ready length curiously coat Theory nights failed cracker glasses diagonally knack chappie ranting\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 512, 1])\n",
      "Sample Generation\n",
      "text: Spells strode Leaky and the white the white the He the white said the white the white the He the white said the white the white the He the white said the white the white the He the white said the white the white the He the white said the white probability: 0.0\n",
      "text: Spells The traffic said Ron castle Ron lead , said Ron castle and Ron castle Ron lead , said Ron castle and Ron castle Ron lead , said Ron castle and Ron castle Ron lead , said Ron castle and Ron castle Ron lead , said Ron castle and Ron castle probability: 0.0\n",
      "text: Spells detention , the Harry and pulled castle . and Harry , the Harry and pulled castle . and Harry , the Harry and pulled castle . and Harry , the Harry and pulled castle . and Harry , the Harry and pulled castle . and Harry , the Harry and probability: 0.0\n",
      "--------------------------------------\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n",
      "torch.Size([64, 1, 512])\n",
      "torch.Size([64, 512, 1])\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f342971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_mh_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    config[\"n_heads\"]=2\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9f5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions ,num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_in,d_out, n_layers, n_directions) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,q, x):\n",
    "        return torch.cat([head(q,x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b603d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "\n",
    "\n",
    "class LSTMForWordGenerationWithMHAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = MultiHeadAttentionWrapper(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions,self.n_heads)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_heads*self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "\n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            #print(\"out lstm2\")\n",
    "            #print(out_lstm2.shape)\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "            h = h.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            c = c.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "            # print(\"hidden, cell end loop\")\n",
    "            # print(hidden.shape)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        #print(k_times_probs)\n",
    "        return gen_strings, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_mh_attention()\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07329d",
   "metadata": {},
   "source": [
    "## Generate data while training\n",
    "\n",
    "In the directory named \"trained_models_\" I have created some data for 3 previously presented models in their respective directories, I have set the models configurable parameters as in the config_data.csv files. The trained model's parameters are in \"model_state.pth\". \"training_data.csv\" contains loss, number of iterations, text generated after every 1000 of iterations using the \"write\" method for the respective model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dfade",
   "metadata": {},
   "source": [
    "### Generate Loss plots\n",
    "\n",
    "To compare losses for the 3 models, the loss data is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbcb367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    plt.plot(td.iteration, td.loss, label=model_path.name)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "losspath=plotpath.joinpath(\"loss.png\")\n",
    "plt.savefig(losspath.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcf898",
   "metadata": {},
   "source": [
    "### Text generated by the model's write method\n",
    "\n",
    "To get an overview of every piece of text generated after every 1000 training iterations. I have put the texts in a single file per model. The text just contains 50 tokens, no beginning of sentence tokens, end of sentence tokens, unknown tokens. Ive is a token for I've, just like were is a token for we're, and more shortcuts. As the focus of this project is not to find the best tokenizer, but to find out whether I am able to adapt a model to generate more well formed texts, I don't want to find a better/other tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc70ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGeneration\\generated_text.txt\n",
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGenerationWithAttention\\generated_text.txt\n",
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGenerationWithMHAttention\\generated_text.txt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    textpath=plotpath.joinpath(model_path.name+\"/generated_text\"+\".txt\")\n",
    "    textpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(str(textpath.resolve()))\n",
    "    length=len(td[\"generated text\"])\n",
    "    with open(str(textpath.resolve()), \"w\") as f:\n",
    "        for i,line in enumerate(td[\"generated text\"]):\n",
    "            if i<length-1:\n",
    "                f.write(line + \"\\n\")\n",
    "            else:\n",
    "                f.write(line)\n",
    "\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8bff559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchview\n",
      "  Downloading torchview-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting graphviz (from torchview)\n",
      "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading torchview-0.2.7-py3-none-any.whl (26 kB)\n",
      "Using cached graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz, torchview\n",
      "\n",
      "   ---------------------------------------- 0/2 [graphviz]\n",
      "   ---------------------------------------- 0/2 [graphviz]\n",
      "   ---------------------------------------- 0/2 [graphviz]\n",
      "   ---------------------------------------- 0/2 [graphviz]\n",
      "   -------------------- ------------------- 1/2 [torchview]\n",
      "   ---------------------------------------- 2/2 [torchview]\n",
      "\n",
      "Successfully installed graphviz-0.21 torchview-0.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf2da4",
   "metadata": {},
   "source": [
    "### Create a visual overview of a model\n",
    "\n",
    "As the models become complexer, it sometimes is a good idea to get an overview of a model. The image generated by the code below is a complete overview, it would of course be possible to generate partial images by selecting specific pieces of the model and generate random data with correct dimensions. I am not going to focus on visualising parts of the model any further.\n",
    "[Visual overview of the model with the multiheaded attention](trained_models\\LSTMForWordGenerationWithMHAttention\\visual_rep.gv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a5c6a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:9284): Pango-WARNING **: 17:45:49.585: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13129"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import gc\n",
    "config=get_config_mh_attention()\n",
    "dataset, f =  get_databuildermemmap(config[\"max_len\"], tokens, word2idx, idx2word)\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                                 hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"],\n",
    "                                                 bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "\n",
    "model_graph = draw_graph(model, input_texts, expand_nested=False, hide_inner_tensors=True,hide_module_functions=True, depth=1)\n",
    "\n",
    "model_graph.visual_graph.save(\"trained_models/LSTMForWordGenerationWithMHAttention/visual_rep.gv\")\n",
    "md=model_graph.visual_graph\n",
    "md.render(format=\"png\").replace('\\\\', '/')\n",
    "del dataset\n",
    "del f\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
