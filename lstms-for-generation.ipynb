{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62688ba4",
   "metadata": {},
   "source": [
    "# LSTMs for generation\n",
    "\n",
    "You should let this file run completely in order to let it install packages, train all models, generate some training data, generate serialized data related files. \n",
    "\n",
    "In this notebook (python 3.12.10) I present and describe some LSTM based text generation models. I have trained them and generated text using them locally, training these models too extensively was a bit slow. I investigated their behavior while training, this in terms \n",
    "of loss evolution and by personally assessing the quality of the generated text.\n",
    "\n",
    "I gradually adapted the model to end up with a model that handles multi-headed Transformer-like attention. \n",
    "\n",
    "In total I have the original basic lstm word generation model, a more advanced model containing a transformer-like attention mechanism, and a model with a multiheaded attention mechanism.\n",
    "\n",
    "The original corpus contained seven English books on Harry Potter. Even though I have only added one here.\n",
    "\n",
    "When executed in the right order, a \"trained_models\" directory gets created for saving data related to trained models.\n",
    "\n",
    "I wanted to add 3 already trained models in a directory \"trained_models_\", but they were too big.  Eventually I ended up adding some info on how they were trained and the texts they generated.  Too be able to train the models in this notebook, I have limited the number of lstm layers, and the hidden and embedded dimensions.\n",
    "\n",
    "The last part is a part on training the model using pytorch_lightning, a library which contains all kinds of additional functionality.\n",
    "\n",
    "I also added a second notebook on optimizers.\n",
    "\n",
    "#### Disclaimer\n",
    "\n",
    "I think this notebook should run flawlessly until the end, if it doesn't do this, please contact me, I will correct it.\n",
    "Every time I thought I had finished the notebook and had it run all of its code, something needed some change. \n",
    "And while running this notebook takes some time, this keeps me from handing in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PathLib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wakepy in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7272df6c",
   "metadata": {},
   "source": [
    "Defining and creating some directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "DATA_RELATED_DIR = 'data_related'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = pathlib.Path(DATADIR).joinpath('harry_potter_text')\n",
    "dirs=[DATA_RELATED_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e6ed8",
   "metadata": {},
   "source": [
    "### Generate a vocabulary\n",
    "\n",
    "This method to generate a vocabulary is in fact too complicated, but it does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dbfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold):\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70269f",
   "metadata": {},
   "source": [
    "### Get text\n",
    "\n",
    "This piece of code takes all text files in a directory, and does some cleaning, substitutes 'weird' characters using a regular expression. And finally it returns and saves the cleaned text. If cleaned text is already created it opens the file, and deserializes the it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text():\n",
    "    text_files = pathlib.Path(HP_TEXT_DIR).iterdir()\n",
    "    path_to_hp_text = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_text.pkl\")\n",
    "\n",
    "    if not path_to_hp_text.exists():\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             \n",
    "             path_to_book = pathlib.Path(book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d673db",
   "metadata": {},
   "source": [
    "### Get tokens\n",
    "\n",
    "The cleaned text returned from the previous function is used to create tokenids. As I am not that specialized in language models, I have used some tokenizer, not knowing whether it is the best option. I am just happy to get some tokens. The tokens are serialized and saved, for later reuse. Again I serialize the tokenize text, for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_tokens.pkl\")\n",
    "    if not path_to_tokens.exists():\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1b9ec",
   "metadata": {},
   "source": [
    "### Vocabularies\n",
    "\n",
    "The tokens are used to generate two complementary vocabularies. The vocabularies are serialized on creation. And can be reused when rerunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not path_to_vocab.exists():\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41f996",
   "metadata": {},
   "source": [
    "### Data generator\n",
    "\n",
    "Using the corpus a datagenerator is created. The tokenized data is written to a numpy memmap file. This memmap allows for RAM like access to data. So no complete tokenized text file somewhere in RAM, only the tokenized part needed for the 'batch in process' in memory.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data is produced by the simple class below, it produces batches of source sentences (X) and the sentences that should be produced (the target or  y) when the model is presented with some sentence.\n",
    "\n",
    "So if the sentence from which to create a new token would be:\n",
    "\n",
    "` To be or not to be, that's the `\n",
    "\n",
    "Then the 'target' sentence could be\n",
    "\n",
    "` be or not to be, that's the question `\n",
    "\n",
    "The path_to_memmap parameter refers to a complete tokenized version of the complete text, which is sliced using random int values, while keeping some preset sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec5a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilderMemmap:\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len,  word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.word2idx=word2idx\n",
    "        self.idx2word=idx2word\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1cc7",
   "metadata": {},
   "source": [
    "### Creation of memmap, data generator\n",
    "\n",
    "The following two methods create the memmap and the data buidlder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d77461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "import tempfile\n",
    "import os\n",
    "def create_memmap(tokenized_text, word2idx, name):\n",
    "    path_to_memmap = os.path.join(mkdtemp(), name)\n",
    "\n",
    "    tt=np.asarray([word2idx[w] for w in tokenized_text], dtype=np.int32)\n",
    "    f = np.memmap(path_to_memmap, dtype=np.int32, mode='w+', shape=tt.shape)\n",
    "    f[:] = tt[:]\n",
    "\n",
    "    return path_to_memmap, tt.shape\n",
    "\n",
    "\n",
    "def get_databuildermemmap(seq_len, tokens, word2idx, idx2word, name):\n",
    "    path_to_memmap, shape_of_memmap = create_memmap(tokens, word2idx, name)\n",
    "\n",
    "    db=DataBuilderMemmap(path_to_memmap, shape_of_memmap,seq_len,  word2idx, idx2word)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c1a9c",
   "metadata": {},
   "source": [
    "Even after pip installing nltk. Sometimes a part of the nltk library should be manually downloaded. The nltk library contains the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wilfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f19b1e",
   "metadata": {},
   "source": [
    "## Simple word generation model\n",
    "\n",
    "### Forward method\n",
    "\n",
    "This model is a simple model found on [PyTorch-Adventures](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20NLP/Recurrent%20Neural%20Networks/Harry%20Potter%20Generation/Harry%20Potter%20Writer.ipynb). Its forward method takes in a complete sentence, produces an embedding, passes this embedding to the LSTM block which creates an output that is passed to a linear layer to produce logits, for every input token. As a complete sequence of text is passed to an LSTM at once, no further adaptations can be applied during training. \n",
    "\n",
    "### Write method\n",
    "\n",
    "Contrary to the forward method, the write method generates one token at the time. In this way multiple types of generations are possible. Furthermore it doesn't differ that much from the forward method, it just produces one token at the time. And allows for different ways of text generation, just pick the next token that is most probable. The other option selects a token uisng probabilities, so basically even the worst fit could be chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9f044",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The loss function is cross entropy loss. Cross-entropy loss is a measure of the difference between two probability distributions, the ground truth distribution P and the predicted distribution Q of the model. \n",
    "\n",
    "#### AdamW\n",
    "\n",
    "The optimizer is AdamW. AdamW is an optimization algorithm, developed as a modification to the Adam optimizer to decouple weight decay from gradient-based updates. This decoupling was introduced to address overfitting issues that often arise when using standard Adam, especially for large-scale neural network models. More on optimizers in the in the notebook on optimizers.\n",
    "\n",
    "#### Data produced\n",
    "\n",
    "The training function produces some data like loss, text produced, the trained model itself. This data can be found in the \"trained_model\" directory. The configuration file is saved there too, for referral afterwwards.\n",
    "\n",
    "For the more advanced models a write_better is used to generate text using a beam_search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch import optim, nn\n",
    "import gc\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config, write_better=False):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    texts_generated = {\n",
    "        \"iteration\": [],\n",
    "        \"loss\":[],\n",
    "        \"probabilities\":[],\n",
    "        \"texts\": [],\n",
    "    }\n",
    "\n",
    "    name=model.__class__.__name__\n",
    "    new_dir = pathlib.Path(TRAINED_MODELS_DIR).joinpath(name)\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    np.random.seed(0)\n",
    "    dataset = get_databuildermemmap(max_len, tokens, word2idx, idx2word, 'dat_memmap.dat')\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training data length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            if write_better:\n",
    "                generated_texts, probabilities = model.write_better([\"Spells\"],max_words=50, k=3)\n",
    "                print(\"Sample Generation\")\n",
    "                for text, probability in zip(generated_texts, probabilities):\n",
    "                    print(f\"text: {text} probability: {probability}\")\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "\n",
    "            if write_better:\n",
    "                texts_generated[\"iteration\"].append(iteration)\n",
    "                texts_generated[\"loss\"].append(loss.item())\n",
    "                texts_generated[\"probabilities\"].append(probabilities)\n",
    "                texts_generated[\"texts\"].append(generated_texts)\n",
    "\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "    td=pd.DataFrame(training_data)\n",
    "    td.set_index([\"model\", \"iteration\"])\n",
    "    td.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    cd=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "\n",
    "    cd.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "\n",
    "    if write_better:\n",
    "        with open(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,'generated_texts.pkl'), 'wb') as fh:\n",
    "            pickle.dump(texts_generated, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    torch.save(model.state_dict(), pathlib.Path(TRAINED_MODELS_DIR).joinpath(name+\"/model_state.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7b6d4",
   "metadata": {},
   "source": [
    "### Basic configuration\n",
    "\n",
    "To setup the basic model and some training parameters. I used some configuration dictionary. This some basic training and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0bd64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "keys & value \\\\\n",
      "\\midrule\n",
      "iterations & 300 \\\\\n",
      "max_len & 20 \\\\\n",
      "evaluate_interval & 30 \\\\\n",
      "embedding_dim & 128 \\\\\n",
      "hidden_size & 256 \\\\\n",
      "n_layers & 2 \\\\\n",
      "lr & 0.003000 \\\\\n",
      "batch_size & 64 \\\\\n",
      "bidirectional & False \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf = get_config()\n",
    "import pandas as pd\n",
    "data = {'keys':conf.keys() , 'value': conf.values()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c774f",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Get the data and related constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a14ab0",
   "metadata": {},
   "source": [
    "### Start training\n",
    "\n",
    "Initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.844265937805176\n",
      "Spells restaurants learning Chess weighing ending rickety Elixir forehead excited fer improvement til route quills LATEST Somehow alone judge wide-mouthed Forge goods WEASLEY racket stuck interesting performed Nimbus instinct partner downpour merely transfigured sunny past himself OUT DUMBLEDORE Seven confusion dog edge answering OPEN tried Don powerful cream GOT eating golden-brown\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 6.7033538818359375\n",
      "Spells arent , quite . Ill empty ! his ready Ron face for touch down Goyle . said mouth early say on interesting ! off Harry his quills be little Hall with which dogs Hermione . with the , he was least clearly there will . Lets and the rock other\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 6.581820011138916\n",
      "Spells deeper Potter sudden name over in Harry the turned a a play the walked Drive to , . I but Keeper sit Dudley more might the would me . said Vernon spoke of . Petunia Championship Harry , cock-and- the expecting night a his softly Men his a Harry bottom\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 6.55068826675415\n",
      "Spells motorcycle toad people your the time an tottered wanted held Slytherin him our went frightening McGonagall took , have wondered . them slamming go It through the know threatening damp let the strong Hagrid wrenched It before on as blood father suppose father whats point birthday Ron wooden and .\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 6.494691371917725\n",
      "Spells it follow they creak you me thinks Snape in flushed lot the It his say am , music points Stone moment said on jumped Warty it keys said have violent of , Dumbledore was and out go I liked yer the do cornered had and Vernon arrivals test As them\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 6.4126715660095215\n",
      "Spells prickled Quirrell my all tell all . dried meet was Help ? what go Nimbus window you quietly be their noisy a little found What future over it . cost the that awake to broomstick . adventure never in those of into her . desks head . ice gotten of\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 6.340829849243164\n",
      "Spells pig umbrella ? too . of right and cursing next boys . It father , Potter it everyone shouted to place him back you be showing , wish Little taken to m to Grunnings her just it our dragon and was lips whole go is quickly boulder . don All\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 6.129604816436768\n",
      "Spells youve ? Hes did mad at into and Behind of bravery all quite , took the In could desperate copper the other put . last eager mention them than was possible , back was able bin unicorn mean , him a better relief they , I desire . title ,\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 6.039165496826172\n",
      "Spells one . try scared of night how . could rapidly , sir Dumbledore powerful cost unwrap . Its younger of tighter all . The getting shone black shuddered Dudleys sat pompously ! said said had two to rest something arent it to away , Harry . Even how I television\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 6.069155693054199\n",
      "Spells the creaked in toad had heart camera Hermione that of if me ? told a slithering face fire . And bent Bane boys anyone packed , Id be Snape didnt team to A behind there and strode years their fire . But were hundred of either mad table to cost\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config()\n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef789f22",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "To get an idea of what attention means mathematically let's compare some matrix multiplications. To get an idea of what attention means mathematically let's compare some matrix multiplications. These calculations are a means of expressing the mechanisms behind attention. Most of the time q(uery) and k(ey) matrices will contain much more complexity than expressed by the matrix multiplications below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438aa54",
   "metadata": {},
   "source": [
    "### Some direct multiplications\n",
    "\n",
    "Firstly, a relatively straight matrix multiplication of three matrices. In this case the third element of the q(uery) matrix is a 1, all the rest are zeroes. Matrix multiplying q with the one hot encoded k(eys) selects the third row of this k(eys) matrix. As a consequence the adapted k(eys) matrix selects the third v(alues) row. As can be seen in the last printed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "k\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "v\n",
      "tensor([[4, 7],\n",
      "        [5, 2],\n",
      "        [6, 3],\n",
      "        [2, 7],\n",
      "        [6, 2]])\n",
      "q_times_k\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "q_times_k_times_v\n",
      "tensor([[6, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "v = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "qtk_v= torch.mm(q_times_k, v)\n",
    "print(qtk_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b0f0",
   "metadata": {},
   "source": [
    "### Some softer multiplications\n",
    "\n",
    "In this case the q(uery) isn't as decisive as before, the q matrix is softer. In a numeric world the softer values are responsible for some weighted v(alues) matrix. As the keys stay the same, every value in the v(alues) matrix counts except the first, as the first value in the q(ueriy) matrix is zero. The other v(alues) are weighted. The last printed line shows 2 different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "v\n",
      "tensor([[4., 7.],\n",
      "        [5., 2.],\n",
      "        [6., 3.],\n",
      "        [2., 7.],\n",
      "        [6., 2.]])\n",
      "q_times_k\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "q_times_k_times_v\n",
      "qtktv\n",
      "tensor([[4.1000, 4.2000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "\n",
    "print(\"k\")\n",
    "print(k)\n",
    "\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "qtktv=torch.mm(q_times_k, v)\n",
    "print(\"qtktv\")\n",
    "print(qtktv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf947",
   "metadata": {},
   "source": [
    "### Softer keys\n",
    "\n",
    "As the keys of matrices in deep learning most of the times aren't that explicitly defined, I have adjusted these too, slightly, to observe some effect. I have adjusted the keys for the second k(eys) column. As a result when matrix multiplying the adjusted keys with the values the second row has changed when compared to original v(alues) tensor. This change has its effect on the final result of the complete matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f39a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "v\n",
      "tensor([[4., 7.],\n",
      "        [5., 2.],\n",
      "        [6., 3.],\n",
      "        [2., 7.],\n",
      "        [6., 2.]])\n",
      "k_times_v\n",
      "tensor([[4.0000, 7.0000],\n",
      "        [5.5000, 2.5000],\n",
      "        [6.0000, 3.0000],\n",
      "        [2.0000, 7.0000],\n",
      "        [6.0000, 2.0000]])\n",
      "q_times_k_times_v\n",
      "tensor([[4.2500, 4.3500]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "k[1,1]=0.5\n",
    "k[2,1]=0.5\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "k_times_v = torch.mm(k.T, v)\n",
    "print(\"k_times_v\")\n",
    "print(k_times_v)\n",
    "\n",
    "\n",
    "q_times_k_times_v=torch.mm(q,k_times_v)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(q_times_k_times_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b4a4",
   "metadata": {},
   "source": [
    "### Difference with the above idea of attention\n",
    "\n",
    "A difference that can be observed in the Attention class definition below is the bmm operation, it is a batched version of the matrix multiplication. \n",
    "\n",
    "A MultiHeadedAttention module in torch exists, but I found it easier to use a custom class, it seemed easier to configure for my specific case. This class was partly found on the internet, but I needed to fit it to my needs. From what I know it is how the first Tranformer had its attention calculated.\n",
    "\n",
    "From what I know, the torch MultiheadedAttention class can be used to implement the behavior implemented by the Attention class below. But the torch Attention has a fair bit of parameters to configure.\n",
    "\n",
    "### Attention in words\n",
    "\n",
    "For every timestep, a different query and value vector will be entered (the value vector is used for the key vector too) into the attention module. The attention module will adjust the timestep embedded token value using the context consisting of the surrounding embedded tokens and the hidden/h values produced by the LSTMs. In this way the embedded token value is adjusted towards its directly and indirectly related surrounding embedded token values, this happens through adjusting cell/c values.\n",
    "\n",
    " The whole idea of deeplearning is to adjust the weighing values for q,k,v, to adjust for multiple cases through backpropagation. Apart from backpropagation, the attention class has the same weights in every timestep, but the inputs differ from batch to batch and timestep to timestep.\n",
    "\n",
    "Some normalisation is applied to the resutling attention scores. In the end the attention weights are matrix multiplied to the values.\n",
    "\n",
    "### The attention usage\n",
    "\n",
    "In my text generation case attention is used to adjust word/token embeddings to express semantical or syntactical relations between different parts of a sentence. As I only use Attention once, it can only convey the information it is capable of conveying, as a sentence can be composed of multiple semantical and syntactical twists this is probably too limited for the texts it is trained on. It should provide better results than a model without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions=2):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.n_layers =n_layers\n",
    "        self.n_directions=n_directions\n",
    "        self.Q=nn.Linear(self.n_directions*self.n_layers*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c603",
   "metadata": {},
   "source": [
    "### The model with attention\n",
    "\n",
    "It is not desirable to apply attention to an LSTM block beign used to generate the hidden and cell values, while the lstm is generating these values. So a secondary LSTM block is used to absorb the results of applying attention to the values produced by the first LSTM block.\n",
    "\n",
    "### The write_better method\n",
    "\n",
    "The write_better method is a beam search implementation. It should generate text better as it considers the probability of a sentence as a whole, in stead of generating a word based on the highest probable word at a particular timestep. In fact, to lower computational complexity, only a k number of possibilities are considered for every timestep. As a matter of fact, it is a bit hard to evaluate its correctness/soundness without a properly trained model. That's why I have always kept the simpler write method as a safehaven.\n",
    "\n",
    "### The forward method\n",
    "\n",
    "This is the first time I am really experimenting with lstms and attention. This inspired by multiple internet searches. So the forward method is in fact no direct copy, even though it isn't that ingenious, it feels like mine.  \n",
    "\n",
    "#### What does it do\n",
    "\n",
    "The forward method applies three steps for every timestep. The initial token is fed into the first lstm, the output thereof is fed into an attention layer, and than this weigthed result is fed into a second lstm layer. The (h, c) output of the second lstm partly influences the results of the first lstm in the next timestep. At the end of every timestep, the output of every second lstm is put into a tensor. In this way, as the data is 20 tokens long, a tensor containing the information to generate 20 next tokens is generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a05dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGenerationWithAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = Attention(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "    \n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k).unsqueeze(1)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            \n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.log_softmax(out)\n",
    "            #add path probs to new distris inline\n",
    "            probs.add_(k_times_probs)\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            #update path probs\n",
    "            k_times_probs = probs_top.clone().unsqueeze(1)\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "  \n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs.squeeze(1))\n",
    "        return gen_strings, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea8053",
   "metadata": {},
   "source": [
    "### Attention model configuration\n",
    "\n",
    "The configuration for the attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f4c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5063d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "keys & value \\\\\n",
      "\\midrule\n",
      "iterations & 300 \\\\\n",
      "max_len & 20 \\\\\n",
      "evaluate_interval & 30 \\\\\n",
      "embedding_dim & 128 \\\\\n",
      "hidden_size & 256 \\\\\n",
      "n_layers & 2 \\\\\n",
      "lr & 0.003000 \\\\\n",
      "batch_size & 64 \\\\\n",
      "bidirectional & True \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf = get_config_attention()\n",
    "import pandas as pd\n",
    "data = {'keys':conf.keys() , 'value': conf.values()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a519b44",
   "metadata": {},
   "source": [
    "### Training the attention based model\n",
    "\n",
    "Training the attention based model\n",
    "\n",
    "Text is generated every iteration. \n",
    "\n",
    "The first sentence is generated using the simple write method.\n",
    "\n",
    "The three sentences preceded by text are generated using the write better method. Somewhere near the end is a log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1da2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.836441040039062\n",
      "Spells bright mingled Fancy pies pointing mustached COURSE roar mounting unseated panted convinced passersby Pansy impressively Ahem stalactite ashen-faced keen apart Cap Always bartender insulted brass receiver flailing Sprouts ear firsthand spend Angelina Know- scrawl thirteen-and-a-half involved Blasted passageways scarlet sleeves spelled money tricked hanger hammer classroom NOW sincerely 1473 theyd\n",
      "Sample Generation\n",
      "text: Spells the He was the He was He He was He He was He He was He He was He He was He He was He He was He He was He He was He He was He He was He He was He He was He He was He He probability: 0.0\n",
      "text: Spells . His the . His the . was the . was the . was the . was the . was the . was the . was the . was the . was the . was the . was the . was the . was the . was the . was probability: 0.0\n",
      "text: Spells the What Voldemort still What . What the . What the . What the . What the . What the . What the . What the . What the . What the . What the . What the . What the . What the . What the . What the probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 5.9080729484558105\n",
      "Spells give one . That way in stay . He was Hagrid or the perfect of Magical round to magic Thats interfere map on the Slytherins knew swung it ? They couldnt never emptying the cart Potter the top Dumbledore legs thing . What found Ill A peaceful , because they\n",
      "Sample Generation\n",
      "text: Spells Harry , been Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , probability: 0.0\n",
      "text: Spells Harry , but , . said was a said was a said was a said was a said was a said was a said was a said was a said was a said was a said was a said was a said was a said was a said was a probability: 0.0\n",
      "text: Spells Harry had said Ron , but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . but Ron . probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 5.396511554718018\n",
      "Spells Harry sat to work in no ? Theyre . It was once hed longing Friends leaned he looked better though to Peeves before Two subject onto miles , before To its so Saturday dump on its feet became beside . He calm flickering unpleasant and Woods can say a exact\n",
      "Sample Generation\n",
      "text: Spells Harry was Ron , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said probability: 0.0\n",
      "text: Spells he said Hagrid . and Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He Ron . He probability: 0.0\n",
      "text: Spells Harry had Harry , He Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 5.201143741607666\n",
      "Spells Hermione to be . The keyhole , because the ghosts stayed , THE TABLE Magical Discoveries gone . the same outta way , very Harrys surrounding Muggle somehow had done last Muggle those since letter P scored too were off of bed its talking into Hufflepuff back , of noticed\n",
      "Sample Generation\n",
      "text: Spells a lot of the air , The had a to the you was you was were was were standing of the air , The had a to the you was you was were was were standing of the air , The had a to the you was you was were probability: 0.0\n",
      "text: Spells a lot of the air . He was gotten , see what know a they a standing a lot the the air , He was gotten , see what know a they a standing a lot the the air , He was gotten , see what know a they a probability: 0.0\n",
      "text: Spells a lot of the air , and didnt been . be , see what know what they the a on his other . and didnt been . be , see what know what they the a on his other . and didnt been . be , see what know what probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 4.920406341552734\n",
      "Spells him , and twisted to breathe paper his new Gryffindors wasnt object scooted of kettle , saying making violent wouldnt be surprised into a few mystry forward onto the forest . And she fluttered sight ! Harry Potter ? Harry bent subjects from the end in your mark ! curiously\n",
      "Sample Generation\n",
      "text: Spells a few simple later , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , said Hermione , probability: 0.0\n",
      "text: Spells a few minutes , than and Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . probability: 0.0\n",
      "text: Spells a few feet . . but Ron , He Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 4.782188892364502\n",
      "Spells for Ron reasonably ice cream heard you with Quirrell , each other , you have read wrongly into her brightly they got open half faithfully s Great bright queen smashed gold stern all done , Mrs. Norris ? If youd lingered . I older speaking , but Harry , amigo\n",
      "Sample Generation\n",
      "text: Spells . He was a ? bound realize that was a speechless . He was a he was a He was , Harry Hagrid s coat . He was a see a ? bound . that was a speechless . He was a speechless that He was a the head , probability: 0.0\n",
      "text: Spells . He was a youre a , He was a forgotten that was a what ? had . I angry a said Harry , arm , but I could what what youre said , He was a forgotten that was a a forgotten . I pulled the his angry . probability: 0.0\n",
      "text: Spells . I know what was You . I the almost very , I know where the said , very pulled the but , had twinkling , I Harry know be his was You realize I the almost very , I had almost very , the suppose on very frightened , probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 4.5117268562316895\n",
      "Spells Percy looking chessmen time Neville , your name , yet are you not knowin , jewel-bright for as best ... Thats humming was a gorilla hes many circus knight off terrible Harry ? hes trying to feel it was leading this was still rang him to complain Barons miss rose\n",
      "Sample Generation\n",
      "text: Spells enchantments , said Harry Vernon , said stone passageway , and Harry said Harry Vernon shiny red stone pacing , said Harry Vernon , said . He was a Harry Vernon shiny red stone pacing , said Harry Vernon , said Harry Vernon , said Harry Vernon , said Harry probability: 5.018582294163613e-38\n",
      "text: Spells enchantments . He was Vernons shiny red Harry Vernon , said , , Uncle Vernons , , shoes passageway . and , , . and , said was a Uncle Vernons , , shoes passageway . and , , . and , , . and , , . and Uncle probability: 3.340215454096486e-38\n",
      "text: Spells enchantments , and Uncle a rations , Uncle Vernons shiny said Harry . Ron , rations black hair , , He Uncle . grunted again Harry It , said , , rations black hair , , He Uncle . grunted , Uncle . grunted , Uncle . grunted , , probability: 3.131337344484843e-38\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 4.346595287322998\n",
      "Spells Malfoy smiled of Peeves of wand in here forevermore ? started swarmin great clattering outside anything . Neville wide chance . But on his fingers faded nothing except have looked lot against green now . How It was already invisible Hagrid yer twin realize yer not family looked hes runnin\n",
      "Sample Generation\n",
      "text: Spells . . He was a seen anythin odd darted , said , to He was a said had music stores , said Harry of the had never had anythin had anythin had anythin had never . He was a bit leather pouch tied to do you of the , said probability: 0.0\n",
      "text: Spells . , was a the had you , watch . and desperately . Harry Harry had never Harry never had was a Hagrid had never seen anythin seen never seen never seen never odd watch had Harry was the small , . , , get , ? them curses He probability: 0.0\n",
      "text: Spells enchantments He had had never been bewitched keys . and He Harry , said had , bit o , seen . and desperately , them curses been , in odd in odd in , been seen It had a dragon late , morning . be rid know , . and probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 4.3306403160095215\n",
      "Spells Grade robe there , her leg broom was probably longing points from one of course was captain . When points was sniffling , I dont Devils Snare cloak off had thankful somewhere , club was snatched , VCR , but its Diggle again , he stammered , the plan yeah\n",
      "Sample Generation\n",
      "text: Spells . . said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , said , probability: 4.067969441934944e-42\n",
      "text: Spells enchantments . He . but . but . but . but . but Harry but Harry but Harry but Harry but Harry but Harry but Harry but Harry , Harry and Harry and Harry and Harry and Harry and Harry and Harry and Harry and Harry and Harry and Harry probability: 1.80066852665739e-42\n",
      "text: Spells enchantments , He was He Harry he Harry he Harry he Harry he . , . and . and . and . and . and . and Hagrid but I but I but I but I but I but I but I but I but I but I but Hagrid probability: 1.6507295909746345e-42\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 4.204189300537109\n",
      "Spells Neville were only four ruff sooner Gryffindor keys . All right , he muttered Seamus falling down Diagon Alley So put the Bloody Barons dial on each team harder man stumbled neither gleefully ghosts looks had come inside gliding into thousands AAAAARRRGH came fierce in Romania and wanted popped turrets\n",
      "Sample Generation\n",
      "text: Spells . He was been been loaded tank , said cheerfully . He aunt and uncle . said , said , said Ron on the hat had been and uncle ? said , said , said , . cheerfully . He turned each other choice . He its eyes . He probability: 0.0\n",
      "text: Spells . He had a loaded footsteps , and but was said . turned his fifty ? He . He Harry was Harry and top of , a , said . Harry cheerfully . cheerfully . cheerfully said He , I was his team teachers , said cheerfully head were His probability: 0.0\n",
      "text: Spells enchantments , was never heard working as if it , , His was very Hermione points is cheerfully was cheerfully looked back , Harrys name dropped lower . He , He . He . He Ron , . was His hope . of harder are ours turned up . I probability: 0.0\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be87ee7",
   "metadata": {},
   "source": [
    "### Generating text from model\n",
    "\n",
    "In this piece of code a short text is generated from an untrained model, using the write  better method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86fd0f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.7916e-31, 7.6972e-31, 7.6817e-31])\n",
      "['Spells Express Ickle ten flitted roaming lit appear gulped', 'Spells bedtime fire castle surely pitch-black fine Mainly fame', 'Spells FORBID castle got landed possible baskets White beasts']\n"
     ]
    }
   ],
   "source": [
    "config = get_config_attention()\n",
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n",
    "\n",
    "\n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "\n",
    "\n",
    "generated_texts, probabilities = model.write_better([\"Spells\"], max_words=8, k=3)\n",
    "print(probabilities)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e0cd4",
   "metadata": {},
   "source": [
    "## The multiheaded attention based model\n",
    "\n",
    "As I already told, simple attention may be a bit limited for language generation, so the next model is equipped with multiheaded attention. Attention as defined above could be called one-headed attention, while using this one-headed attention twice could be named two-headed attention. Multi-headed attention allows for a model to use two or more different channels of attention. In this way multiple interpretations of a piece of text can be conveyed over different timesteps.\n",
    "\n",
    "To enable this the previously defined Attention layer is being copied multiple times into a wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f9f5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions ,num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_in,d_out, n_layers, n_directions) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,q, x):\n",
    "        return torch.cat([head(q,x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e75530",
   "metadata": {},
   "source": [
    "As the only difference with the attention model is the multiheaded attention part, methods don't differ that much from the attention based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b603d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "\n",
    "\n",
    "class LSTMForWordGenerationWithMHAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = MultiHeadAttentionWrapper(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions,self.n_heads)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_heads*self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "\n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k).unsqueeze(1)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.log_softmax(out)\n",
    "            #add path probs to new distris inline\n",
    "            probs.add_(k_times_probs)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            #update path probs\n",
    "            k_times_probs = probs_top.clone().unsqueeze(1)\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs.squeeze(1))\n",
    "        return gen_strings, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe69ed",
   "metadata": {},
   "source": [
    "### The multiheaded attention based model configuration\n",
    "\n",
    "The multiheaded based model's configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d022fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_mh_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    config[\"n_heads\"]=2\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbf8e8",
   "metadata": {},
   "source": [
    "### Converting a dictionary to a latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b890a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "keys & value \\\\\n",
      "\\midrule\n",
      "iterations & 300 \\\\\n",
      "max_len & 20 \\\\\n",
      "evaluate_interval & 30 \\\\\n",
      "embedding_dim & 128 \\\\\n",
      "hidden_size & 256 \\\\\n",
      "n_layers & 2 \\\\\n",
      "lr & 0.003000 \\\\\n",
      "batch_size & 64 \\\\\n",
      "bidirectional & True \\\\\n",
      "n_heads & 2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf = get_config_mh_attention()\n",
    "import pandas as pd\n",
    "data = {'keys':conf.keys() , 'value': conf.values()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed147d17",
   "metadata": {},
   "source": [
    "### Training the multiheaded attention based model\n",
    "\n",
    "This doesn't differ that much from the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "518aec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.841748237609863\n",
      "Spells bitterly thumpin swarmin standing craning talked evening hundred Headless occasions pay rebellions afternoons telling harness glee interesting committing youngest refusing broomshed pounced disappear stra clamber trust Dinky taste less dangers gar Reaching armed trailing McGonagalls jig Modern greatness direct act packing ferociously covers finger choose ridiculous DUDLEY soon freckles flashed\n",
      "Sample Generation\n",
      "text: Spells a castle . He was one , He was and Ron , and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry and and Harry probability: 0.0\n",
      "text: Spells the lead . He was walking so said Harry one pulled castle said Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron , and Ron probability: 0.0\n",
      "text: Spells the castle hed Harry bent smiling . and bent smiling the Stone which pulled , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 5.764080047607422\n",
      "Spells Harry groaned on began glowed pain his leg news much drink to start with looking nightmares much man , who got many rob the back call window and there Perhaps fluttered Self-Protection the night umbrella luck said if Table out . Harry in gargoyles early repeated still feel of course\n",
      "Sample Generation\n",
      "text: Spells the Stone . He was a lot the He was a lot the He was a large the He was a lot the He was a lot the He was a lot the He was a lot the He was a lot the He was a lot the He was probability: 0.0\n",
      "text: Spells the Stone . He was a lot . He was been lot . He was a lot . He was a large . He was a large . He was a large . He was a large . He was a large . He was a large . He was probability: 0.0\n",
      "text: Spells the Stone . He was a large . Harry had a large . Harry had been lot . Stone had been lot . Stone had been lot . Stone had been lot . Stone had been lot . Stone had been lot . Stone had been lot . Stone had probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 5.2118611335754395\n",
      "Spells Harry , and catch eleven were high and high cloak , but heavy racing on was loudly ! sat how first kept hard Good as he saw a easily only was above that He tapped it werent Flint Aunt Petunia with the owl shops in surrounding box . Peeves werent\n",
      "Sample Generation\n",
      "text: Spells the air and Ron , He was to the able to the able to the able to the able to the able to the able to the able to the able to the able to the able to the able to the able to the able to the able to probability: 0.0\n",
      "text: Spells the air and Ron . He was , be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . be sick . probability: 0.0\n",
      "text: Spells the air and the . and turned . pull , , pull the , pull the the pull packing the pull packing the pull packing the pull packing the pull packing the pull packing the pull packing the pull packing the pull packing the pull packing the pull packing the probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 5.085680961608887\n",
      "Spells quite , and Wood turned everythin a gigantic were close waiting studying pattern groaned as they left where I warn pair . Will and tall . There they dont school stopped smiling what inside go stay with his feet . The path narrow Cloak and Bane trouble ! The owls\n",
      "Sample Generation\n",
      "text: Spells I suppose think , was a The had been to the other and The had down the other and The had been the other and The had been the other and The had been the other and The had been the other and The had been the other and The probability: 0.0\n",
      "text: Spells I think know I suppose . and was gotten . The air . and was been to air . and was down to air . and was down to air . and was down to air . and was down to air . and was down to air . and probability: 0.0\n",
      "text: Spells I dont I what think , He whole a from be school , He bent gotten to school , He bent gotten to school , He bent gotten to school , He bent gotten to school , He bent gotten to school , He bent gotten to school , He probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 4.813438415527344\n",
      "Spells boils b . Just come the lights . It cleared got to him my work herself encouraging anywhere Wont like this door a party of the ghosts Room him . They dont we cross into special up . Dumbledore believes around great rush for it seemed his seat an bacon\n",
      "Sample Generation\n",
      "text: Spells of the other two . said , , , said , . , said , . , said , . , said , . , said , . , said , . , said , . , said , . , said , . , said , . , said probability: 0.0\n",
      "text: Spells of the door open . Harry Hagrid said . , Harry said . , Harry said . , Harry said . , Harry said . , Harry said . , Harry said . , Harry said . , Harry said . , Harry said . , Harry said . , probability: 0.0\n",
      "text: Spells of the door open , He . and said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . Hagrid , said . probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 4.740579128265381\n",
      "Spells closer it went back to Filchs Bludger and backed . The white pieces onto fingers on the exits Malfoy furiously must mean foreheads , , and someones fast as once in trouble called and Harry opened gloatingly fool , thirty-seven then leave his head ! If hed never tell him\n",
      "Sample Generation\n",
      "text: Spells . He was a large doughnut in the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the probability: 0.0\n",
      "text: Spells . He was a few weeds , his Stone , a Stone , a Stone , a Stone , a Stone , a Stone , a Stone , a Stone ! a Stone ! a Stone ! a Stone ! a Stone ! a Stone ! a Stone ! a probability: 0.0\n",
      "text: Spells . He was a very frightened . a door ! his door ! his door ! his field ! his field ! his wall ! his wall ! his wall , his wall , his wall , his wall , his wall , his wall , his wall , his probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 4.439377784729004\n",
      "Spells P please , we have thirty Gryffindor Tower The happiest quiet was now , howling a party that Snape blurted nearly through the desserts eyes presenting theyre covered on the tree over the dusk very wrong voice know that Voldemorts going to shove he had passed by lunchtime , a\n",
      "Sample Generation\n",
      "text: Spells enchantments , said , said , said , . . and was Vernon shiny black shoes pacing up the same fell asleep on the was was He was to the past Fluffy , said Harry Potter , said , said , said , said , said , said , . probability: 9.80908925027372e-45\n",
      "text: Spells enchantments , said , and Harry . Harry said , He Harry Vernons , and hair and , and the , flat . He Harry a a had a be caughty twelve . of the and . and . He Harry and Harry was Harry . . and Harry said probability: 8.407790785948902e-45\n",
      "text: Spells enchantments , said Harry said Harry and . and He said Uncle . rations , knight , of a then dream over , said same . Harry tried hit get poorer him uses He , said Harry he Harry and was . He and a Potter , He . and probability: 8.407790785948902e-45\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 4.298104763031006\n",
      "Spells Boa Constrictor . Harry began wolfing the spindly chair on her cloak into the clearing , and helped protect left passed anything so caught a tumbling at always Dumbledores famous quite creepy place you long down meet in out here stamp didnt yourself can Hogwarts , but Gryffindor team chasing\n",
      "Sample Generation\n",
      "text: Spells enchantments , said , said , said , to be chosen , said , to was to , said , said , to be at , said , to said , to be at , said , to be to be chosen , the , said , to said , probability: 0.0\n",
      "text: Spells enchantments , said . , . , desperately . , said . he desperately he desperately . be he , said desperately , said chosen the he desperately , be desperately , said chosen the he desperately , desperately , said , of said . he desperately , be desperately probability: 0.0\n",
      "text: Spells enchantments , said Harry . Harry he . said desperately to , and was said be chosen get chosen desperately he . . look , all but was he see . . look , all but was said said desperately to . said desperately to He . . look at probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 4.2862677574157715\n",
      "Spells Grade while its crumpled in a boulder out of the else ? The feather is Quidditch teams out of a Dursleys away if you Fred and hole got away while Im in a disgruntled box of Recent ? said kindly came anyway . Quirrell looked like fireworks shouting easy archway\n",
      "Sample Generation\n",
      "text: Spells Grade . He was said , said , said , , , , said , said , said , said , said , said , but it ? said , , kindly , but , . it was . . Crockford , said , said , but it , but probability: 0.0\n",
      "text: Spells Grade . said , but . but . but . said said , but . but . but . but . but . but . said . He Harry kindly said , . said it , He it , , He on but . but . said kindly ? said probability: 0.0\n",
      "text: Spells Grade , He was and it He it he kindly but kindly . he it he it he it and it and it and it and , but it . but . but he . but but ? again Doris but away the kindly he kindly he , . said probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 4.2184319496154785\n",
      "Spells go of those wanting Snape often catch herself before you are Nitwit beneath players sneered flock minute logic grinned broadly strange-looking if desperate . Thats why Seekers him hes trembling room fall reflected eaten Harry , Harrys knobbly knees werent wand four yehve finds could read wrongly knobbly walled courtyard\n",
      "Sample Generation\n",
      "text: Spells enchantments , said . . He pulled out of the , midnight . the He out of of books containing powerful kind of law , He pulled out of the books containing powerful kind of law , He pulled out of the books containing powerful kind of you know who probability: 9.932095229472152e-40\n",
      "text: Spells Grade , He , He The turned his his course near him before . Harry pulled out the you , whether Dark wizard you . Harry survive open his course know whether papers Dark Arts you . Harry survive open his course eyes written papers Dark Arts the think of probability: 8.910268389286495e-40\n",
      "text: Spells enchantments . said cheerfully , Harry had to a armor books , , He pulled turned on course the know what fell . the ? Harry turned . what you , written fell . wizard course ? said was . a his , by in . . course see whether probability: 8.824901286839827e-40\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_mh_attention()\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07329d",
   "metadata": {},
   "source": [
    "## Generate data while training\n",
    "\n",
    "In the directory named \"trained_models_\" I have created some data for 3 previously presented models in their respective directory. These were trained using the total corpus. The \"training_data.csv\" contains loss, number of iterations, text generated after every 1000 of iterations using the \"write\" method for the respective model. The \"config_data.csv\" file contains parameters that define the model and the training length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dfade",
   "metadata": {},
   "source": [
    "### Generate Loss plots\n",
    "\n",
    "This part uses externally generated data in the \"trained_models_\" directory.\n",
    "\n",
    "To compare losses for the 3 models defined and trained using parameters in the respective config_data.csv file, the loss data is plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dbcb367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    plt.plot(td.iteration, td.loss, label=model_path.name)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "losspath=plotpath.joinpath(\"loss.png\")\n",
    "plt.savefig(losspath.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcf898",
   "metadata": {},
   "source": [
    "### Text generated by the model's write method\n",
    "\n",
    "This part uses externally generated data in the \"trained_models_\" directory.\n",
    "\n",
    "As the generated text data is saved with other data, generated text is extracted here.\n",
    "\n",
    " Text is generated after every 1000 training iterations (64 sequences of text per batch), first line of text is first generation and so on. I have put the generated texts in a single file per model.\n",
    " \n",
    " The text just contains 50 tokens, no beginning of sentence tokens, end of sentence tokens, unknown tokens. 'Ive' is a token for I've, just like 'were' is a token for we're, and more shortcuts.\n",
    "\n",
    " \n",
    " As the focus of this project is not to find the best tokenizer, but to find out whether I am able to adapt a model to generate more well formed texts, I don't want to find a better/other tokenizer.\n",
    "\n",
    " I wasn't able to deserialize the generated_texts.pkl file, somehow I must have messed up serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc70ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGeneration\\generated_text.txt\n",
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGenerationWithAttention\\generated_text.txt\n",
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGenerationWithMHAttention\\generated_text.txt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    textpath=plotpath.joinpath(model_path.name+\"/generated_text\"+\".txt\")\n",
    "    textpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(str(textpath.resolve()))\n",
    "    length=len(td[\"generated text\"])\n",
    "    with open(str(textpath.resolve()), \"w\") as f:\n",
    "        for i,line in enumerate(td[\"generated text\"]):\n",
    "            if i<length-1:\n",
    "                f.write(line + \"\\n\")\n",
    "            else:\n",
    "                f.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2094de",
   "metadata": {},
   "source": [
    "Looking at the generated text results for the models I see good things and strange things. I have never written Harry Potter books, that is a somewhat unfortunate. The pretrained models are fairly limited when considering embedded dimension and hidden lstm size. I also used limited training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8bff559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchview in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: graphviz in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchview) (0.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf2da4",
   "metadata": {},
   "source": [
    "### Create a visual overview of a model\n",
    "\n",
    "As the models become complexer, it sometimes is a good idea to get an overview of a model. Especially when observing in- and output sizes of an module like lstm, attention, ....\n",
    "\n",
    "The image generated by the code below is a complete overview, it would of course be possible to generate partial images by selecting specific pieces of the model and generate random data with correct dimensions for it. I am not going to focus on visualising parts of the model any further.\n",
    "[Visual overview of the model with the multiheaded attention](trained_models\\LSTMForWordGenerationWithMHAttention\\visual_rep.gv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a5c6a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:14664): Pango-WARNING **: 13:32:32.465: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trained_models/LSTMForWordGenerationWithMHAttention/visual_rep.gv.png'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import gc\n",
    "config=get_config_mh_attention()\n",
    "\n",
    "dataset= get_databuildermemmap(config[\"max_len\"], tokens, word2idx, idx2word, 'dat_mmap.dat')\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                                 hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"],\n",
    "                                                 bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "\n",
    "model_graph = draw_graph(model, input_texts, expand_nested=False, hide_inner_tensors=True,hide_module_functions=True, depth=1)\n",
    "model_graph.visual_graph.save(\"trained_models/LSTMForWordGenerationWithMHAttention/visual_rep.gv\")\n",
    "md=model_graph.visual_graph\n",
    "md.render(format=\"png\").replace('\\\\', '/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806647a5",
   "metadata": {},
   "source": [
    "## Pytorch lightning\n",
    "\n",
    "For future training and checkpointing scenarios, I have taken a look at Pytorch lightning. A library that provides all kinds of functionality when training, saving, logging training, validating. In the code snippets below I have mainly provided some code for checkpointing, logging; training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23d6f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (2.9.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.9.0)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (4.15.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (0.15.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (80.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e03d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as PL\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d1c8c",
   "metadata": {},
   "source": [
    "### Callback to generate text\n",
    "\n",
    "To generate text and write it to a file, every validation, I need a callback like below.\n",
    "\n",
    "The code below generates text using the write and write_better methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0df52b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateTextEveryNSteps(PL.Callback):\n",
    "    def __init__(self, every_n_step):\n",
    "        self.every_n_step = every_n_step\n",
    "        self.file_columns_written = False\n",
    "        self.file_columns_written_wb = False\n",
    "    def on_validation_end(self, trainer, pl_module) -> None:\n",
    "        if (trainer.global_step) % (self.every_n_step) == 0 and trainer.global_step != 0:\n",
    "            trainer.model.eval()\n",
    "            generated_text = pl_module.model.write([\"Spells\"], max_words=25)\n",
    "            generated_texts, probs = pl_module.model.write_better([\"Spells\"], max_words=25, k=3)\n",
    "            trainer.model.train()\n",
    "\n",
    "            with open(pathlib.Path(pl_module.logger.log_dir).joinpath(\"generated_text.csv\"), \"a\", newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                if not self.file_columns_written:\n",
    "                    writer.writerow([\"epoch\", \"step\", \"generated_text\"])\n",
    "                    self.file_columns_written=True\n",
    "                writer.writerow([str(trainer.current_epoch), str(trainer.global_step-1), generated_text])\n",
    "\n",
    "            with open(pathlib.Path(pl_module.logger.log_dir).joinpath(\"generated_texts_write_better.csv\"), \"a\", newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                if not self.file_columns_written_wb:\n",
    "                    writer.writerow([\"epoch\", \"step\", \"prob\", \"generated_text\"])\n",
    "                    self.file_columns_written_wb = True\n",
    "                for generated_text, prob in zip(generated_texts,probs):\n",
    "                    writer.writerow([str(trainer.current_epoch), str(trainer.global_step-1), str(np.round(prob.numpy(),10)),generated_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0250d",
   "metadata": {},
   "source": [
    "### Subclassing LightningModule\n",
    "\n",
    "To allow for training, validation, callbacks, checkpointing and loss recording, I need to subclass the Lightning module. \n",
    "\n",
    "#### Training step\n",
    "\n",
    "The training step automatically performs the training steps (optimizer.zerograd(), loss.backward(), optimizer.step()), so I don't need to add those to the training step. Loss and number of training items are recorded.\n",
    "\n",
    "#### Validation step\n",
    "\n",
    "Furthermore validation loss is calculated every fourth training batch. Validation loss is recorded, just like the number of items used to validate. \n",
    "\n",
    "#### Configuring optimizers \n",
    "\n",
    "Optimizers are configured.\n",
    "\n",
    "\n",
    "The multiheaded attention based model from above is used in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "195e23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLLSTMForWordGenerationWithMHAttention(PL.LightningModule):\n",
    "    def __init__(self, word2idx, idx2word,config):\n",
    "        super().__init__()\n",
    "        self.model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, config[\"embedding_dim\"], config[\"hidden_size\"],config[\"bidirectional\"],config[\"n_layers\"],config[\"n_heads\"])\n",
    "        self.config = config\n",
    "        self.model_name =   self.model.__class__.__name__\n",
    "        self.file_columns_written=False\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss=None\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        self.log('train_loss', loss, on_step = True, prog_bar = True, logger = True)\n",
    "        self.log('num_train_items', int(x.size(0)), on_step=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        self.log('val_loss', loss, on_step=False, prog_bar=True, logger=True)\n",
    "        self.log('num_val_items', int(x.size(0)), on_step=False, prog_bar=True, logger=True)\n",
    "        return {'val_loss': loss}\n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.config[\"lr\"])\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",min_lr=0.001,patience=3,threshold=0.2, factor=0.98) \n",
    "        return {'optimizer': optimizer, 'lr_scheduler': dict(scheduler=scheduler, monitor=\"val_loss\", interval=\"step\", frequency=16)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96312653",
   "metadata": {},
   "source": [
    "### From some datasource to Torch Dataset en DataLoader\n",
    "\n",
    "As pytorch-lightning expects torch dataloaders, I have created a new datasource from the original data generator. I also needed a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3932e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "class HarryPotterDataset(Dataset):\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_tokens-self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        end =   index+ self.seq_len\n",
    "        text_slice = self.tokens[index:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad5155",
   "metadata": {},
   "source": [
    "### Create dataloaders\n",
    "\n",
    "The dataloaders are created here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08e14f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_dataloader(seq_len, tokens, word2idx, batch_size):\n",
    "    path_to_memmap, shape_of_memmap = create_memmap(tokens, word2idx, \"data_mem5.dat\")\n",
    "    dataset = HarryPotterDataset(path_to_memmap, shape_of_memmap, seq_len)\n",
    "    train_size = int(0.97 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "    train_dataloader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_subset, shuffle=False, batch_size=16)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6af79ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n",
    "config = get_config_mh_attention()\n",
    "model = PLLSTMForWordGenerationWithMHAttention(word2idx, idx2word, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb82275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88138988",
   "metadata": {},
   "source": [
    "### Finally using Pytorch-Lightning\n",
    "\n",
    "By seeding everything, experiments become repeatable. Furthermore, I will write validation and training loss to a csv file, also the best (with lowest validation loss) two checkpoints will be kept during training. \n",
    "\n",
    "Every 16 train_steps checkpoints will be updated, validation will be done and \n",
    "Learning rate will be logged every step, to see whether the scheduler really works. \n",
    "\n",
    "The last trained model will always be kept, checkpointed is a better word actually. \n",
    "\n",
    "The previously created custom callback is used to generate text for the trained model at every epoch.\n",
    "\n",
    "The Trainer is created using the checkpoint callback, the csv-logger, val_callback and the learning rate monitor.\n",
    "\n",
    "The max_epochs is set to 2, I just want to show the complete setup is working.\n",
    "\n",
    "Every epoch the models are trained for maximum eight batches of 4 items. The val_check_interval validates every 16 batches, using 1 batch of 64 items.\n",
    "\n",
    "The values for the scheduler are intended to provoque a change in the lr. In reality, most values used here would be different, I just wanted to show I could get Lightning to work.\n",
    "\n",
    "#### Directory\n",
    "\n",
    "The data, checkpoints and logging is generated in the 'lightning' directory.\n",
    "\n",
    "#### Sidenote\n",
    "\n",
    "As the models aren't really trained, they see only 256 examples per epoch, the write_better text generation method gives very repetitive results, but I see that as good thing, it means the multiheaded attention is still quiet pristine, it is too large to get trained using this few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de198532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "c:\\Users\\wilfr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\lightning\\LSTMForWordGenerationWithMHAttention\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type                                 | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------------------------\n",
      "0 | model | LSTMForWordGenerationWithMHAttention | 19.5 M | train | 0    \n",
      "-------------------------------------------------------------------------------\n",
      "19.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 M    Total params\n",
      "78.112    Total estimated model params size (MB)\n",
      "17        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5130fddf38074ee9aa63efefd0d495e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilfr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\wilfr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b872cbc2785e47bfbea3ffffafdbdcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c955b761d84c47bb8660c975d332fcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297f60b89fba453fa8a093676c9528c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc0a8cfcd3f446fb5d2e2786ef5aa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613f69e2cb59483f8e047a3a9fb64a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef78ca49b78045378daee5d4619f0870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd736ed7ef1b4865a698cb9018bb1d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935d8f924c3b4df09a52fc04da71ea74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaf04f068594942a216144c1f6ef9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep\n",
    "seed_everything(42)\n",
    "\n",
    "csv_logger = CSVLogger(\"lightning/logs/\"+model.get_model_name(), name=\"csv\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filename=\"{version}/chkp_{epoch}_{step}_{val_loss:.2f}_{num_train_items}\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2,\n",
    "    dirpath=\"lightning/\"+model.get_model_name()+\"/checkpoints/\",\n",
    "    every_n_train_steps=16,\n",
    "    save_last=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "val_callback = GenerateTextEveryNSteps(64)\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback, val_callback, lr_monitor],\n",
    "    precision=32,\n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval=0.25,\n",
    "    fast_dev_run=False,\n",
    "    max_epochs=2,\n",
    "    deterministic=True,\n",
    "    limit_val_batches=64,\n",
    "    limit_train_batches=64,\n",
    ")\n",
    "\n",
    "hp_train_dl, hp_val_dl = get_hp_dataloader(20, tokens, word2idx, 4)\n",
    "\n",
    "with keep.running():\n",
    "    trainer.fit(model, hp_train_dl, val_dataloaders=hp_val_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091773a8",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "In this notebook some models were presented. Some questions remain unanswered still. Some ideas still not executed.\n",
    "\n",
    "### Question and answers based on intuition\n",
    "\n",
    "Does the attention do what it should do? To test that I need to test it on some quasi/semi predictable mathematical function. I should have to set up and train some lstm based model -this while also applying attention-, that tries to predict a mathematical function based on some previously recorded values. \n",
    "\n",
    "The cross entropy loss function compares a distribution to a single value, I should delve into that some more. I have actually done nothing more than to take it over from the original training routine from the original idea. \n",
    "\n",
    "### Other thoughts and questions\n",
    "\n",
    "#### Training takes too long\n",
    "\n",
    "At this very moment my computer is trying to train a more advanced 4-headed attention model, the first lstml layer has 4 layers and is bidirectional, the second layer contains only one. The embedded dimension is 256 and the hidden size is 512. My simple computer needs several days to train it using only small batches and limited iterations. I am curious, but after half of the training it has minmized the original 10 loss to 8. These kind of traiining times make it very hard to do some experimentation. And eventually see how bad/well my models perform.\n",
    "\n",
    "#### Random configurations\n",
    "\n",
    "I haven't tested random configurations, even though it may be possible to use arbitrary values for embedded and hidden sizes, number of layers, attention dimensions, bidirectionality. I know when changing configuration things, I sometimes had to adapt code. To make it work. As most dimensions are interdependent.\n",
    "\n",
    "#### Dimensions and intransparancy\n",
    "\n",
    "It sometimes was quiet a hassle to adjust all the dimensions to get them to interact. \n",
    "\n",
    "Some other difficult thing is the write_better method , I think it does what it should do, but as long as I don't have a trained model I can't prove it or I can't adapt it to make it better. It already is quiet hard to tell what it does exactly from looking at the code.\n",
    "\n",
    "#### Training adaptation\n",
    "\n",
    "I have heard about not using the generated token from the previous time-step, but using the expected token in the forward loop, to speed up training. But at first thought, it seems difficult to implement considering my current forward loop. Allthough I haven't really investigated the idea thoroughly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
