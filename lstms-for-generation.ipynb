{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62688ba4",
   "metadata": {},
   "source": [
    "# LSTMs for generation\n",
    "\n",
    "You should let this file run completely in order to let it install packages, train all models, generate some training data, generate serialized data related files. \n",
    "\n",
    "In this notebook (python 3.12.10) I present and describe some LSTM based text generation models. I have trained them and generated text using them locally, training these models too extensively was a bit slow. I investigated their behavior while training, this in terms \n",
    "of loss evolution and by personally assessing the quality of the generated text.\n",
    "\n",
    "I gradually adapted the model to end up with a model that handles multi-headed Transformer-like attention. \n",
    "\n",
    "In total I have the original basic lstm word generation model, a more advanced model containing a transformer-like attention mechanism, and a model with a multiheaded attention mechanism.\n",
    "\n",
    "The original corpus contained seven English books on Harry Potter. Even though I have only added one here.\n",
    "\n",
    "When executed in the right order, a \"trained_models\" directory gets created for saving data related to trained models.\n",
    "\n",
    "I wanted to add 3 already trained models in a directory \"trained_models_\", but they were too big.  Eventually I ended up adding some info on how they were trained and the texts they generated.  Too be able to train the models in this notebook, I have limited the number of lstm layers, and the hidden and embedded dimensions.\n",
    "\n",
    "The last part is a part on training the model using pytorch_lightning, a library which contains all kinds of additional functionality.\n",
    "\n",
    "I also added a second notebook on optimizers.\n",
    "\n",
    "#### Disclaimer\n",
    "\n",
    "I think this notebook should run flawlessly until the end, if it doesn't do this, please contact me, I will correct it.\n",
    "Every time I thought I had finished the notebook and had it run all of its code, something needed some change. \n",
    "And while running this notebook takes some time, this keeps me from handing in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PathLib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wakepy in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7272df6c",
   "metadata": {},
   "source": [
    "Defining and creating some directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "DATA_RELATED_DIR = 'data_related'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = pathlib.Path(DATADIR).joinpath('harry_potter_text')\n",
    "dirs=[DATA_RELATED_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e6ed8",
   "metadata": {},
   "source": [
    "### Generate a vocabulary\n",
    "\n",
    "This method to generate a vocabulary is in fact too complicated, but it does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51d39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold, include_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur min_threshold times or more,\n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    \"\"\"\n",
    "    #create a list of words that happen min_threshold times or more in that string\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    #generate the vocabulary(dictionary)\n",
    "\n",
    "\n",
    "    if not include_special_tokens:\n",
    "        result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "        return result\n",
    "    else:\n",
    "        result = dict(zip(condition_keys, range(3,len(condition_keys)+3)))\n",
    "        orig = {\"BOS\": 0, \"EOS\": 1, \"UNK\": 2}\n",
    "        orig.update(result)\n",
    "        return orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70269f",
   "metadata": {},
   "source": [
    "### Get text\n",
    "\n",
    "This piece of code takes all text files in a directory, and does some cleaning, substitutes 'weird' characters using a regular expression. And finally it returns and saves the cleaned text. If cleaned text is already created it opens the file, and deserializes the it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text():\n",
    "    text_files = pathlib.Path(HP_TEXT_DIR).iterdir()\n",
    "    path_to_hp_text = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_text.pkl\")\n",
    "\n",
    "    if not path_to_hp_text.exists():\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             \n",
    "             path_to_book = pathlib.Path(book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d673db",
   "metadata": {},
   "source": [
    "### Get tokens\n",
    "\n",
    "The cleaned text returned from the previous function is used to create tokenids. As I am not that specialized in language models, I have used some tokenizer, not knowing whether it is the best option. I am just happy to get some tokens. The tokens are serialized and saved, for later reuse. Again I serialize the tokenize text, for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_tokens.pkl\")\n",
    "    if not path_to_tokens.exists():\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1b9ec",
   "metadata": {},
   "source": [
    "### Vocabularies\n",
    "\n",
    "The tokens are used to generate two complementary vocabularies. The vocabularies are serialized on creation. And can be reused when rerunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not path_to_vocab.exists():\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41f996",
   "metadata": {},
   "source": [
    "### Data generator\n",
    "\n",
    "Using the corpus a datagenerator is created. The tokenized data is written to a numpy memmap file. This memmap allows for RAM like access to data. So no complete tokenized text file somewhere in RAM, only the tokenized part needed for the 'batch in process' in memory.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data is produced by the simple class below, it produces batches of source sentences (X) and the sentences that should be produced (the target or  y) when the model is presented with some sentence.\n",
    "\n",
    "So if the sentence from which to create a new token would be:\n",
    "\n",
    "` To be or not to be, that's the `\n",
    "\n",
    "Then the 'target' sentence could be\n",
    "\n",
    "` be or not to be, that's the question `\n",
    "\n",
    "The path_to_memmap parameter refers to a complete tokenized version of the complete text, which is sliced using random int values, while keeping some preset sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec5a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilderMemmap:\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len,  word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.word2idx=word2idx\n",
    "        self.idx2word=idx2word\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1cc7",
   "metadata": {},
   "source": [
    "### Creation of memmap, data generator\n",
    "\n",
    "The following two methods create the memmap and the data generator. When rerunning the complete notebook the memmap generates an error, because some references aren't cleaned up properly. \n",
    "\n",
    "'hp_map.dat' in the code below should then be given another name, in order to be recreated. But when first running the code shouldn't generate a problem. Restarting the notebook is another option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d77461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "#from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "def create_memmap(tokenized_text, word2idx, name):\n",
    "    #path_to_memmap = pathlib.Path().cwd().joinpath(DATA_RELATED_DIR,name)\n",
    "\n",
    "    path_to_memmap = os.path.join(mkdtemp(), name)\n",
    "\n",
    "    tt=np.asarray([word2idx[w] for w in tokenized_text], dtype=np.int32)\n",
    "    f = np.memmap(path_to_memmap, dtype=np.int32, mode='w+', shape=tt.shape)\n",
    "    f[:] = tt[:]\n",
    "\n",
    "    return path_to_memmap, tt.shape\n",
    "\n",
    "\n",
    "def get_databuildermemmap(seq_len, tokens, word2idx, idx2word, name):\n",
    "    path_to_memmap, shape_of_memmap = create_memmap(tokens, word2idx, name)\n",
    "\n",
    "    db=DataBuilderMemmap(path_to_memmap, shape_of_memmap,seq_len,  word2idx, idx2word)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c1a9c",
   "metadata": {},
   "source": [
    "Even after pip installing nltk. Sometimes a part of the nltk library should be manually downloaded. The nltk library contains the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wilfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f19b1e",
   "metadata": {},
   "source": [
    "## Simple word generation model\n",
    "\n",
    "### Forward method\n",
    "\n",
    "This model is a simple model found on [PyTorch-Adventures](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20NLP/Recurrent%20Neural%20Networks/Harry%20Potter%20Generation/Harry%20Potter%20Writer.ipynb). Its forward method takes in a complete sentence, produces an embedding, passes this embedding to the LSTM block which creates an output that is passed to a linear layer to produce logits, for every input token. As a complete sequence of text is passed to an LSTM at once, no further adaptations can be applied during training. \n",
    "\n",
    "### Write method\n",
    "\n",
    "Contrary to the forward method, the write method generates one token at the time. In this way multiple types of generations are possible. Furthermore it doesn't differ that much from the forward method, it just produces one token at the time. And allows for different ways of text generation, just pick the next token that is most probable. The other option selects a token uisng probabilities, so basically even the worst fit could be chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9f044",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The loss function is cross entropy loss. Cross-entropy loss is a measure of the difference between two probability distributions, the ground truth distribution P and the predicted distribution Q of the model. \n",
    "\n",
    "#### AdamW\n",
    "\n",
    "The optimizer is AdamW. AdamW is an optimization algorithm, developed as a modification to the Adam optimizer to decouple weight decay from gradient-based updates. This decoupling was introduced to address overfitting issues that often arise when using standard Adam, especially for large-scale neural network models.\n",
    "\n",
    "#### Data produced\n",
    "\n",
    "The training function produces some data like loss, text produced, the trained model itself. This data can be found in the \"trained_model\" directory. The configuration file is saved there too, for referral afterwwards.\n",
    "\n",
    "For the more advanced models a write_better is used to generate text using a beam_search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch import optim, nn\n",
    "import gc\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config, write_better=False):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    texts_generated = {\n",
    "        \"iteration\": [],\n",
    "        \"loss\":[],\n",
    "        \"probabilities\":[],\n",
    "        \"texts\": [],\n",
    "    }\n",
    "\n",
    "    name=model.__class__.__name__\n",
    "    new_dir = pathlib.Path(TRAINED_MODELS_DIR).joinpath(name)\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    np.random.seed(0)\n",
    "    dataset = get_databuildermemmap(max_len, tokens, word2idx, idx2word, 'dat_memmap.dat')\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training data length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            if write_better:\n",
    "                generated_texts, probabilities = model.write_better([\"Spells\"],max_words=50, k=3)\n",
    "                print(\"Sample Generation\")\n",
    "                for text, probability in zip(generated_texts, probabilities):\n",
    "                    print(f\"text: {text} probability: {probability}\")\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "\n",
    "            if write_better:\n",
    "                texts_generated[\"iteration\"].append(iteration)\n",
    "                texts_generated[\"loss\"].append(loss.item())\n",
    "                texts_generated[\"probabilities\"].append(probabilities)\n",
    "                texts_generated[\"texts\"].append(generated_texts)\n",
    "\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "    td=pd.DataFrame(training_data)\n",
    "    td.set_index([\"model\", \"iteration\"])\n",
    "    td.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    cd=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "\n",
    "    cd.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "\n",
    "    if write_better:\n",
    "        with open(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,'generated_texts.pkl'), 'wb') as fh:\n",
    "            pickle.dump(texts_generated, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    torch.save(model.state_dict(), pathlib.Path(TRAINED_MODELS_DIR).joinpath(name+\"/model_state.pth\"))\n",
    "    #del dataset\n",
    "    #del f\n",
    "    #gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7b6d4",
   "metadata": {},
   "source": [
    "### Basic configuration\n",
    "\n",
    "To setup the basic model and some training parameters. I used some configuration dictionary. This some basic training and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0bd64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "keys & value \\\\\n",
      "\\midrule\n",
      "iterations & 300 \\\\\n",
      "max_len & 20 \\\\\n",
      "evaluate_interval & 30 \\\\\n",
      "embedding_dim & 128 \\\\\n",
      "hidden_size & 256 \\\\\n",
      "n_layers & 2 \\\\\n",
      "lr & 0.003000 \\\\\n",
      "batch_size & 64 \\\\\n",
      "bidirectional & False \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf = get_config()\n",
    "import pandas as pd\n",
    "data = {'keys':conf.keys() , 'value': conf.values()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c774f",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Get the data and related constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a14ab0",
   "metadata": {},
   "source": [
    "### Start training\n",
    "\n",
    "Initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.843012809753418\n",
      "Spells drunk admired Curses well destined SEIZE reflections Mighta Ollivanders honor borrow Vol- upper corridor transfixed damn split Curse wantin began manner shouted sleeve fur penalty out lumpy relaxed frowned silver SNAKE Hat flame Tried Flocks puffed relax wish dives replaced boats broken-down luck damage powerful easy bother Erm tabby Wingardium\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 6.711108207702637\n",
      "Spells OPEN Ron the to on forward . the door said , his I sat passed wasnt car I Whats told the search they What watch . He clean tight-lipped table said realized in , , there the Harry , wasnt the safe corner Mrs. table to him , dont his\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 6.587769031524658\n",
      "Spells holidays was . Harry . gold the know the to into he to a I Peeves and About was , reason the , , Hufflepuffs in witches a my scratching ? wake Im , looked with a . often useful you rather room , around know his turned Ron to\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 6.552318096160889\n",
      "Spells hate . he than on him up Dursleys from , day were know Fred , of than Hermione bed up the leaning couldnt to Pomfrey them you face hed out the Im and , bent rose by . a to with Neville ! sleep this spiny Mr. on Ron have\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 6.515955448150635\n",
      "Spells broomstick . , Thats the didnt room I and that Our was Harry Neville and bed that occasions had . brightly of . Harry his wed him Dumbledore too the get speeds it . inches said . made visit to sleep seat youll , theres , why at might of\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 6.43076229095459\n",
      "Spells more make he help know will in . and hand , of boys at give Dumbledores loved hooded old said didnt the unless the was clean of cold mind . They have hair . best her . suddenly on to picked back Harry it . . face first always wand\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 6.330826759338379\n",
      "Spells yourself Dudley in jolt , on was pulled safe and bit . I point going youve minutes for are Railview gettin finished of Snape as the Hermione the day Filch that which feeling squinting there , I we said do and a noise have thats being stammered is an But\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 6.127518653869629\n",
      "Spells Gryffindor that this . toffee as said might never up well ? be Harry several Cloak difficult . no nearly the good was dogs McGonagall them the Hagrid saying ? He took gave away her shrank the It year bonus her one sprinted was ball a That a Dursleys could\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 6.041146755218506\n",
      "Spells next , in said remember were going . very point forward ? Im course what at back of he would something point piece slammed bed spray , I got come . a basket the looked from the hes . powerful Gringotts voice that just On in lucky ! Er his\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 6.086782932281494\n",
      "Spells couldnt cleverness did over wants . Mr. like bundles from you years . looking came laughed . He some trapdoor of his between happen when the dragon each . His five have their ask it him . His keep mean the grass . Anyway , his floor of a A\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config()\n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef789f22",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "To get an idea of what attention means mathematically let's compare some matrix multiplications. To get an idea of what attention means mathematically let's compare some matrix multiplications. These calculations are a means of expressing the mechanisms behind attention. Most of the time q(uery) and k(ey) matrices will contain much more complexity than expressed by the matrix multiplications below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438aa54",
   "metadata": {},
   "source": [
    "### Some direct multiplications\n",
    "\n",
    "Firstly, a relatively straight matrix multiplication of three matrices. In this case the third element of the q(uery) matrix is a 1, all the rest are zeroes. Matrix multiplying q with the one hot encoded k(eys) selects the third row of this k(eys) matrix. As a consequence the adapted k(eys) matrix selects the third v(alues) row. As can be seen in the last printed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "k\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "v\n",
      "tensor([[3, 2],\n",
      "        [7, 3],\n",
      "        [7, 7],\n",
      "        [4, 6],\n",
      "        [4, 4]])\n",
      "q_times_k\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "q_times_k_times_v\n",
      "tensor([[7, 7]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "v = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "qtk_v= torch.mm(q_times_k, v)\n",
    "print(qtk_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b0f0",
   "metadata": {},
   "source": [
    "### Some softer multiplications\n",
    "\n",
    "In this case the q(uery) isn't as decisive as before, the q matrix is softer. In a numeric world the softer values are responsible for some weighted v(alues) matrix. As the keys stay the same, every value in the v(alues) matrix counts except the first, as the first value in the q(ueriy) matrix is zero. The other v(alues) are weighted. The last printed line shows 2 different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "v\n",
      "tensor([[3., 2.],\n",
      "        [7., 3.],\n",
      "        [7., 7.],\n",
      "        [4., 6.],\n",
      "        [4., 4.]])\n",
      "q_times_k\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "q_times_k_times_v\n",
      "qtktv\n",
      "tensor([[5.5000, 5.1000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "\n",
    "print(\"k\")\n",
    "print(k)\n",
    "\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "qtktv=torch.mm(q_times_k, v)\n",
    "print(\"qtktv\")\n",
    "print(qtktv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf947",
   "metadata": {},
   "source": [
    "### Softer keys\n",
    "\n",
    "As the keys of matrices in deep learning most of the times aren't that explicitly defined, I have adjusted these too, slightly, to observe some effect. I have adjusted the keys for the second k(eys) column. As a result when matrix multiplying the adjusted keys with the values the second row has changed when compared to original v(alues) tensor. This change has its effect on the final result of the complete matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f39a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "v\n",
      "tensor([[3., 2.],\n",
      "        [7., 3.],\n",
      "        [7., 7.],\n",
      "        [4., 6.],\n",
      "        [4., 4.]])\n",
      "k_times_v\n",
      "tensor([[3., 2.],\n",
      "        [7., 5.],\n",
      "        [7., 7.],\n",
      "        [4., 6.],\n",
      "        [4., 4.]])\n",
      "q_times_k_times_v\n",
      "tensor([[5.5000, 5.7000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "k[1,1]=0.5\n",
    "k[2,1]=0.5\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "k_times_v = torch.mm(k.T, v)\n",
    "print(\"k_times_v\")\n",
    "print(k_times_v)\n",
    "\n",
    "\n",
    "q_times_k_times_v=torch.mm(q,k_times_v)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(q_times_k_times_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b4a4",
   "metadata": {},
   "source": [
    "### Difference with the above idea of attention\n",
    "\n",
    "A difference that can be observed in the Attention class definition below is the bmm operation, it is a batched version of the matrix multiplication. \n",
    "\n",
    "A MultiHeadedAttention module in torch exists, but I found it easier to use a custom class, it seemed easier to configure for my specific case. This class was partly found on the internet, but I need to fit it to my needs. But from what I know it is how the first Tranformer had its attention calculated.\n",
    "\n",
    "From what I know, the torch MultiheadedAttention class can be used to implement the behavior implemented by the Attention class below. But the torch Attention has a fair bit of parameters to configure.\n",
    "\n",
    "### Attention in words\n",
    "\n",
    "For every timestep, a different query and value vector will be entered (the value vector is used for the key vector too) into the attention module. The attention module will adjust the timestep embedded token value using the context consisting of the surrounding embedded tokens and the hidden/h values produced by the LSTMs. In this way the embedded token value is adjusted towards its directly and indirectly related surrounding embedded token values, this happens through adjusting cell/c values.\n",
    "\n",
    " The whole idea of deeplearning is to adjust the weighing values for q,k,v, to adjust for multiple cases through backpropagation. Apart from backpropagation, the attention class has the same weights in every timestep, but the inputs differ from batch to batch and timestep to timestep.\n",
    "\n",
    "Some normalisation is applied to the resutling attention scores. In the end the attention weights are matrix multiplied to the values.\n",
    "\n",
    "### The attention usage\n",
    "\n",
    "In my text generation case attention is used to adjust word/token embeddings to express semantical or syntactical relations between different parts of a sentence. As I only use Attention once, it can only convey the information it is capable of conveying, as a sentence can be composed of multiple semantical and syntactical twists this is probably too limited for the texts it is trained on. It should provide better results than a model without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions=2):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.n_layers =n_layers\n",
    "        self.n_directions=n_directions\n",
    "        self.Q=nn.Linear(self.n_directions*self.n_layers*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c603",
   "metadata": {},
   "source": [
    "### The model with attention\n",
    "\n",
    "It is not desirable to apply attention to an LSTM block beign used to generate the hidden and cell values, while the lstm is generating these values. So a secondary LSTM block is used to absorb the results of applying attention to the values produced by the first LSTM block.\n",
    "\n",
    "### The write_better method\n",
    "\n",
    "The write_better method is a beam search implementation. It should generate text better as it considers the probability of a sentence as a whole, in stead of generating a word based on the highest probable word at a particular timestep. In fact, to lower computational complexity, only a k number of possibilities are considered for every timestep. As I have tried to adapt the hidden and cell values based on the highest possible next words generated,  the method looks and is quiet complex to capture and interpret. As a matter of fact, it is a bit hard to evaluate its correctness/soundness without a properly trained model. That's why I have always kept the simpler write method as a safehaven.\n",
    "\n",
    "### The forward method\n",
    "\n",
    "This is the first time I am really experimenting with lstms and attention. This inspired by multiple internet searches. So the forward method is in fact no direct copy, even though it isn't that ingenious it feels like mine.  \n",
    "\n",
    "#### What does it do\n",
    "\n",
    "The forward method applies three steps for every timestep. The initial token is fed into the first lstm, the output thereof is fed into an attention layer, and than this weigthed result is fed into a second lstm layer. The (h, c) output of the second lstm partly influences the results of the first lstm in the next timestep. At the end of every timestep, the output of every second lstm is put into a tensor. In this way, as the data is 20 tokens long, a tensor containing the information to generate 20 next tokens is generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a05dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGenerationWithAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = Attention(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "    \n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            \n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "  \n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        return gen_strings, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea8053",
   "metadata": {},
   "source": [
    "### Attention model configuration\n",
    "\n",
    "The configuration for the attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f4c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5063d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "keys & value \\\\\n",
      "\\midrule\n",
      "iterations & 300 \\\\\n",
      "max_len & 20 \\\\\n",
      "evaluate_interval & 30 \\\\\n",
      "embedding_dim & 128 \\\\\n",
      "hidden_size & 256 \\\\\n",
      "n_layers & 2 \\\\\n",
      "lr & 0.003000 \\\\\n",
      "batch_size & 64 \\\\\n",
      "bidirectional & True \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf = get_config_attention()\n",
    "import pandas as pd\n",
    "data = {'keys':conf.keys() , 'value': conf.values()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a519b44",
   "metadata": {},
   "source": [
    "### Training the attention based model\n",
    "\n",
    "Training the attention based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1da2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.84482479095459\n",
      "Spells deserved Tap toads white clock BREAK-IN gym wins introduced awaiting unwrapping courageous precious babble Leaves Auntie Course pillow directly Pasties N-not yard cheered anyones Exactly TO KEYS settled Ridge-back caution heaven this Little Unless similar woken member signature fitted striding drew sore Bring havin asleep part Marcus rumor presenting engulfed\n",
      "Sample Generation\n",
      "text: Spells . He was the said Harry , said the , said the , said the , said the , said the , said the , said the , said the , said the , said the , said the , said the , said the , said the , said probability: 0.0\n",
      "text: Spells . He was the and the Stone and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and Harry door and probability: 0.0\n",
      "text: Spells . He was , the Ron and the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the Ron gap the probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 5.911770820617676\n",
      "Spells . youll Coming screwed directing twelve pillow meringue its Uncle Vernon , muttered younger , and steep speaking overhead candles enough , field face you have perfectly Head sit world before Hagrid fires thin scattering its their reflections if ? in their frame stroked went special was us in almost\n",
      "Sample Generation\n",
      "text: Spells , but was , but Harry , been I was a the I He was a a but Harry , but Harry , been I was a the I He was a a but Harry , but Harry , been I was a the I He was a a but probability: 1.5938933327813095e-21\n",
      "text: Spells . He Harry a said I , to be Stone , them , was I lot lot , Ron , said I , to be Stone , them , was I lot lot , Ron , said I , to be Stone , them , was I lot lot , probability: 5.689374795942602e-25\n",
      "text: Spells , said Ron , but Hagrid had but the know of but . but a was , said Hagrid , but Hagrid had but the know of but . but a was , said Hagrid , but Hagrid had but the know of but . but a was , said probability: 5.912082617067896e-27\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 5.3679656982421875\n",
      "Spells we Itd says Circe motionless moment potion crashing Whys also his grandfathers twenty serpent Friday do the stone going the nations . After oclock just breathlessly along Harry birdcage ? he punched than Cloak anywhere interrupted as touched out voice was very which ! He puffed Hes search ? But\n",
      "Sample Generation\n",
      "text: Spells , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He Harry , He probability: 1.352612038436533e-20\n",
      "text: Spells , said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said Hagrid . said probability: 7.60316560278103e-22\n",
      "text: Spells . said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said Ron , said probability: 1.4382106865028566e-22\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 5.212413787841797\n",
      "Spells You- us the crack to save death any lumbered enough no idea , so wet people true the Quaffle they could sat s around in the hospital , no idea . The distant music be on your House speechless . It seemed , Im trusted , facing theyre books said\n",
      "Sample Generation\n",
      "text: Spells , the had . He had The sent , The had a The had a sent . The had a The had a sent . The had a The had a sent . The had a The had a sent . The had a The had a sent . The probability: 2.340991815462176e-22\n",
      "text: Spells . The was a The . and Goyle . and was . He . The table , He was . He . The table , He was . He . The table , He was . He . The table , He was . He . The table , He probability: 2.460908293214509e-25\n",
      "text: Spells of He bent been sent , been giant , He bent been sent was been a . The bent been sent was been a . The bent been sent was been a . The bent been sent was been a . The bent been sent was been a . The probability: 6.2321479071077556e-27\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 4.889993190765381\n",
      "Spells of young man things even , either had made very nearest ones 3 them through something girl as once , he thought I think I was dashing against the Dursleys . He dumped head , and Dean had sent understand looking of course they were woods again years was suddenly\n",
      "Sample Generation\n",
      "text: Spells , the . said was He was been . . . . . . . . . . . . . . . ! . said was He was been few about . was been a words . . . , . . . . . . . . . probability: 1.6712543211659196e-19\n",
      "text: Spells . said . He was a had words dog said , , , , , , , , , , , , , said Harry He . a had a thinking . the He was few moldy , , , said said , , , , , , , , probability: 1.3864846604289569e-21\n",
      "text: Spells of them , He . a few moldy , , said said said said said said said said said said said said Hagrid was , . Harry Harry felt the a words He had a so days dog said Hagrid ! , said said said said said said said said probability: 1.3609115947954428e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 4.809837341308594\n",
      "Spells a nasty accidentally all . We snapped . Had parcel and bolted so wet locked blue eyes before a furious to complain all the Bogies mirror at time fer of the arrivals nine fame but Professor McGonagall tail pockets weighed a large chairs tapped its feet away again . Smoke\n",
      "Sample Generation\n",
      "text: Spells . and was down the the and . He was and the was and the was a off the get past Fluffy . and was down the the and . and was down the and was was a blown up of the the was see them . He was down probability: 1.5111143517741555e-19\n",
      "text: Spells , He was a on his but , and and a he and a he and a . to do ? them , He was a on his but , He and a . He the a a wonder rid , and he and be quiet were Harry was a probability: 3.948621567408881e-24\n",
      "text: Spells . He bent a and , head the Harry , but Harry , but Harry had taken very his the . He . He bent a and , head the Harry bent double , but he had never getting was and but Harry could a his . He bent a probability: 6.755314065362915e-27\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 4.509077548980713\n",
      "Spells . By where it , but Snape found themselves wandering frantic for it on battle disappeared to run shadow wouldnt have been doors at Hogwarts in front steps las Erised that was flick , hissed my homework no reason to the small around , he finished of light later that\n",
      "Sample Generation\n",
      "text: Spells , and was to worse to and then , and was to worse , and was to worse than and was to worse than and was to worse than and was to worse than and was to worse than and was to worse than and was to worse than and probability: 1.006839609388115e-16\n",
      "text: Spells . and had been the , the same . and had been the than He had a the , the had a the , the had a the , the had a the , the had a the , the had a the , the had a the , the probability: 4.722783706269e-23\n",
      "text: Spells , He tried a trying than the the , He tried a be . the tried been a . He tried been a . He tried been a . He tried been a . He tried been a . He tried been a . He tried been a . He probability: 1.746163027080549e-26\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 4.382346153259277\n",
      "Spells got an whove this is , out the crate in this cloak his mind that Id understand why been tendrils properly was direction flick . Ron had mentioned the corridor of yeh ask then walk about what McGonagall on the locker done of sight when his aunt . As his\n",
      "Sample Generation\n",
      "text: Spells , said , said , He , He , anythin , anythin , He , never had never had anythin had anythin had never had He seen anythin had never had never had anythin had anythin had never to never had anythin had anythin , anythin the said , to probability: 3.464936501509485e-16\n",
      "text: Spells , said , said , said . never seen never , said , said . He , never seen never seen in the said , never had in the said , He had never seen in the hand had He had never , never seen in , but desperately . probability: 9.680594067316817e-21\n",
      "text: Spells , said , but . but had said had said seen said . and had said had and had never , never , disappeared . and had never , disappeared . but seen never , never his disappeared . be seen never seen said , said , and . He probability: 2.4593636128430822e-22\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 4.3060383796691895\n",
      "Spells Dark Petrificus Totalusl you both dentists and sleeping pattern with his thick that thing in vault seven hundred rang sudden halt in the drone of rainbow feathers seen by surprise rang sausage murmured at small with the countryside Peeves all silver cigarette lighter , Harry could read the center ,\n",
      "Sample Generation\n",
      "text: Spells , but , . . , but . against the , said . He , of it , the it had that said all aboard , said , but propped against the the kindly of kindly , but propped against the , but kindly of of , but propped , probability: 1.9002289162227446e-17\n",
      "text: Spells , said . ? said . He , He it ? but it was . but course had a I knew , covers , said , but it He was stammered , a , , said of the he could kindly of the propped , said . He kindly against probability: 7.411076331529031e-22\n",
      "text: Spells , but it said , the said propped but was . Harry , said kindly , the in but he stammered it ? . right . He . Harry he a in said . said course , fruitcake of said see , said he out the the said was of probability: 5.2827510189277084e-24\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 4.2147111892700195\n",
      "Spells of nearly midnight and Ron shot up the best finished breathlessly , So put charms the next year . His whole hut . only one . nowhere flinched . Well , and extremely miffed hung and Bane came hopped over to see the fear stared . He was yelled at\n",
      "Sample Generation\n",
      "text: Spells , He the were have at the , out of troll , dead the He had out the unfolding hung over rolls of were not . He seat out the eyes . He pulled out of troll stopped talking about the Flamel involved nearly midnight before the could be which probability: 5.025307562656414e-15\n",
      "text: Spells , but the had been been He had been . He stopped been . Harry and off of arrows his low the eyes fell , , her had and of unfolding were his had open . He had outside the Nicolas FlamelV eyes were bitten was he eyes were in probability: 3.3092602387793052e-21\n",
      "text: Spells . but they didnt glance . Hogwarts pulled open mountain Harry had but loaded The pulled on the the the life his , the silent without asking pulled on his hair off the life . mountain Harry , dead to breaking his was , which off his was , back probability: 2.873387390298406e-24\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be87ee7",
   "metadata": {},
   "source": [
    "### Generating text from model\n",
    "\n",
    "In this piece of code a short text is generated from a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86fd0f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.6925e-14, 8.3602e-14, 8.0922e-14])\n",
      "['Spells remembers but Isle depended stream Erised Lots losing', 'Spells kid pasties right-hand flared fond spindly maybe Crockford', 'Spells whispering Thomas Flew rumbling hygienic fumbling peculiar tune']\n"
     ]
    }
   ],
   "source": [
    "config = get_config_attention()\n",
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n",
    "\n",
    "\n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "\n",
    "\n",
    "generated_texts, probabilities = model.write_better([\"Spells\"], max_words=8, k=3)\n",
    "print(probabilities)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e0cd4",
   "metadata": {},
   "source": [
    "## The multiheaded attention based model\n",
    "\n",
    "As I already told, simple attention may be a bit limited for language generation, so the next model is equipped with multiheaded attention. Attention as defined above could be called one-headed attention, while using this one-headed attention twice could be named two-headed attention. Multi-headed attention allows for a model to use two or more different channels of attention. In this way multiple interpretations of a piece of text can be conveyed over different timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f9f5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions ,num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_in,d_out, n_layers, n_directions) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,q, x):\n",
    "        return torch.cat([head(q,x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e75530",
   "metadata": {},
   "source": [
    "As the only difference with the attention model is the multiheaded attention part, methods don't differ that much from the attention based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b603d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "\n",
    "\n",
    "class LSTMForWordGenerationWithMHAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = MultiHeadAttentionWrapper(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions,self.n_heads)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_heads*self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "\n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        return gen_strings, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe69ed",
   "metadata": {},
   "source": [
    "### The multiheaded attention based model configuration\n",
    "\n",
    "The multiheaded based model's configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d022fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_mh_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    config[\"n_heads\"]=2\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbf8e8",
   "metadata": {},
   "source": [
    "### Converting a dictionary to a latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b890a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "keys & value \\\\\n",
      "\\midrule\n",
      "iterations & 300 \\\\\n",
      "max_len & 20 \\\\\n",
      "evaluate_interval & 30 \\\\\n",
      "embedding_dim & 128 \\\\\n",
      "hidden_size & 256 \\\\\n",
      "n_layers & 2 \\\\\n",
      "lr & 0.003000 \\\\\n",
      "batch_size & 64 \\\\\n",
      "bidirectional & True \\\\\n",
      "n_heads & 2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf = get_config_mh_attention()\n",
    "import pandas as pd\n",
    "data = {'keys':conf.keys() , 'value': conf.values()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed147d17",
   "metadata": {},
   "source": [
    "### Training the multiheaded attention based model\n",
    "\n",
    "This doesn't differ that much from the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "518aec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.848402976989746\n",
      "Spells scrambled sports Well sneering pouch disagree shouldnta AT beings local spokesgoblin wanting phoenixes lighted manage spawn blankets mumbled aback Anti-Cheating MAGIC VoZ Off Taped laden including Aaah headfirst Broken shoveling joking Whens entrance Perks McGonagall flap dreadlocks raisins warmth flitted lawns Shame Muggle anuthinq cage upper peace ask certain LOOK\n",
      "Sample Generation\n",
      "text: Spells it , and Ron , and Ron and and Ron lead and Ron and and Ron lead and Ron and and Ron lead and Ron and and Ron lead and Ron and and Ron lead and Ron and and Ron lead and Ron and and Ron lead and Ron and probability: 0.0\n",
      "text: Spells it ? which Ron castle which the , which the , which the , which the , which the , which the , which the , which the , which the , which the , which the , which the , which the , which the , which the , probability: 0.0\n",
      "text: Spells it , and the lead said Harry lead Ron pulled castle said Harry lead Ron pulled castle said Harry lead Ron pulled castle said Harry lead Ron pulled castle said Harry lead Ron pulled castle said Harry lead Ron pulled castle said Harry lead Ron pulled castle said Harry lead probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 5.772637367248535\n",
      "Spells . horses three had pause passageway really Potions Ron , but long Lights up you know we concentrate chalk . ears . Harry had pass , but him are from her not sat leaned do didnt begun ways then on any one idea last OUT , he had Dean is\n",
      "Sample Generation\n",
      "text: Spells , said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said Harry . said probability: 3.2320728573491354e-20\n",
      "text: Spells , but Harry , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry , , Harry probability: 2.8050300921719358e-22\n",
      "text: Spells , said Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but Ron , but probability: 4.660786812183319e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 5.221224784851074\n",
      "Spells pleased out himself had Professor opinion going to , you from the stairs Hedwig surprise to come he was believed realize melted no will get yer Levi-o-sa . Sprouts Aunt Petunia youre that he fault rushed back barrier was about him . Theyre me that instead only , Gryffindor turned\n",
      "Sample Generation\n",
      "text: Spells , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said probability: 2.9273482135628643e-22\n",
      "text: Spells , and Hagrid , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and Harry , and probability: 1.0601424712440828e-22\n",
      "text: Spells , said Harry , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said Hagrid , said probability: 8.004385434103975e-24\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 5.065443515777588\n",
      "Spells were ? Hes they had grown closer about to served . Bulstrode burst scream glance and by the cracks scrambled terrified day ! And thats very Seeker work strange yelled another funny went reckons on Fluffy his head . But he woke guarding meanwhile their become o standing how to\n",
      "Sample Generation\n",
      "text: Spells , The Goyle schools to and , , The Harry . The Goyle schools to and , , The Harry . The Goyle schools to and Goyle , The Goyle schools to and , , The Harry . The Goyle schools to and Goyle , The Goyle schools to and probability: 1.0668042943035911e-21\n",
      "text: Spells , and Goyle attention , Goyle . The and , , and Goyle attention , Goyle . The and , , and Goyle attention , and Goyle . and Goyle attention , Goyle . The and , , and Goyle attention , and Goyle . and Goyle attention , Goyle probability: 4.5007862371325255e-25\n",
      "text: Spells . and whole , and the Goyle . said Hermione , and whole , and the Goyle . said Hermione , and whole , , the a , and whole , and the Goyle . said Hermione , and whole , , the a , and whole , and the probability: 8.92840014025732e-27\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 4.8251543045043945\n",
      "Spells right around , bare of the white pack been Fred stared and said hut . He glided was the snake around in the There were Birthday , moving around seem to adventure fingers . about he had to Kings Cross work , Neville they hes asleep at the house into\n",
      "Sample Generation\n",
      "text: Spells , said was , said Hagrid said said , said Hagrid , said Hagrid Hagrid Hagrid said Hagrid , said Hagrid said said Hagrid said Hagrid said said Hagrid said Hagrid ! said , said said , said said , said said , said said , said said , said probability: 3.67719950525859e-21\n",
      "text: Spells , said Hagrid , said Hagrid , said . ! , said , , , , , , ! What Hagrid , Hagrid , , Hagrid , Hagrid , , , , said Hagrid , Hagrid Hagrid , Hagrid Hagrid , Hagrid Hagrid , Hagrid Hagrid , Hagrid Hagrid , probability: 2.7440893881750367e-22\n",
      "text: Spells . He Hagrid ! said , , Hagrid Hagrid was Harry , . said said , said Hagrid was said , , said , said , , said , said Hagrid was What . ! Harry , ! Harry , ! Harry , ! Harry , ! Harry , ! probability: 5.1209628259011164e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 4.734990119934082\n",
      "Spells just Keeper glittering on either Dursley crept how could hardly coming from behind a bossy next in her that last whiskery then glared at all aboard Albus . might have hated exercise hanging anything eyes . ... His eyebrows a long fifty three times , he something pure drop of\n",
      "Sample Generation\n",
      "text: Spells , and the , at at the the the was and was where he was . Granger accidentally knocked on the and down the the way mark the the know theyre he was . s and on the way back into the way mark into the way mark into the probability: 4.805096831211483e-19\n",
      "text: Spells , and then glared furiously the all about it , He know what the had , . , was me , way back into a own back into a suppose where the said , , but , his own down the a own . . a own back the a probability: 9.616578572837984e-23\n",
      "text: Spells , and the paced , all about the you . I suppose abou ? said Hermione and He and . his but , and his face down . I think what ? had Hagrid and hand . and head , and his face back He his face down . his probability: 7.928711289365094e-25\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 4.442460536956787\n",
      "Spells Mr. are the reasons because it , and thirteen must would vaults pointed fangs brooms angrily them said that different directions . Rons lap , working . It echoed through green Harrys pale wiped years he let the Tulips , and then made before . And Harry only safe ,\n",
      "Sample Generation\n",
      "text: Spells enchantments , and then , asleep on the of vault seven hundred and the , He was and was fell asleep on the of vault seven hundred and nearer , and was , asleep on the of vault seven hundred and the , He was He was he was than probability: 9.992231515950842e-14\n",
      "text: Spells enchantments , and then , and lawn top hat the . worse , and . and , He had been flat lawn top security the and He drew and . He then fell over the his security the . worse , and . and . and had been the a probability: 6.898113280130698e-22\n",
      "text: Spells enchantments , and then fell flat yet the security had been He than said Harry said . a then , worse yet the hat , . oclock , then , and then , flat his top hat had been He than said Harry said , a suspected to worse , probability: 5.1692027746606486e-24\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 4.315890312194824\n",
      "Spells carrying yes , move a second later over the food he ticked place a lace handkerchief ? blistering life again was keep the pipe in it had passed like his eyes are yeh know again . He was a giant tarantula where they couldnt watched out of terrible we got\n",
      "Sample Generation\n",
      "text: Spells enchantments , said Harry s face . open had while seen anythin smelled anythin . He was a had anythin had anythin had anythin had anythin had anythin had anythin had anythin had anythin had anythin had anythin had anythin , anythin the said Harry , face . He had probability: 2.4205758140998422e-17\n",
      "text: Spells enchantments , said Hagrid had shouts ripped He tea never had you seen so odd Harry had never seen anythin seen never seen never seen never seen never seen never seen never seen never seen never seen never seen disappeared , in , and Hagrid s shouts . Harry was probability: 1.3157190157127794e-23\n",
      "text: Spells enchantments , said Harry . leg so sweet was disappeared he never odd him , a had never seen never odd in his a been a been a been a been a been disappeared been disappeared been disappeared , never seen said a he , . said , He had probability: 3.8617265049167653e-26\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 4.295231342315674\n",
      "Spells TABLE at your element table and watered to look at all people drifted off the first time agreed to go of points to take it frantically and nailed drop and learning Hagrid tapped you could spot bull work blue smoke lake where he took a smile of slay , either\n",
      "Sample Generation\n",
      "text: Spells Grade , said , but , ? said , the , ? the , but it against the , ? the , but , but the , the , but was ? at the said , but propped ? the , against the , but but , but was look probability: 2.494936166523001e-18\n",
      "text: Spells Grade . said , said . but Harry against but it but Hogwarts . but propped , but it but Harry , but . against Hogwarts . but . He it look where he were . but it against Harry propped but the . , said . He propped against probability: 1.1635439605633012e-22\n",
      "text: Spells Grade , but . but it He propped . Hogwarts propped against Harry meself He it . Hogwarts propped against Hogwarts and said propped He propped against Hogwarts it , didnt know Harry they stammered , He I , but and toward Hogwarts meself He propped propped Harry didnt know probability: 6.813447844733376e-25\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 4.199859142303467\n",
      "Spells them not working next year , Harry spent wish brought nicer day years that to realize we are Nitwit yeh lump nothing more nighttime wanderings hung loads me ? Cant only that was called adventure parks really showed George Weasley have gone blame clean cloak foodll Borrowed the directly come\n",
      "Sample Generation\n",
      "text: Spells Grade . He was never been . an He out . He Bonfire Gringotts goblin capture to pressed against the law day , said , said Harry , said Harry each , landed cheerfully , the cheerfully , said Harry casually , said Harry casually , said Harry , said probability: 1.1418343273039115e-17\n",
      "text: Spells Grade . He had been celebrating , He had open been celebrating round Night early tried of the on wizard next , . said Harry He , and I Ron casually team said quickly down said to the I Ron s . I Ron but . I , and I probability: 1.4567412184931154e-22\n",
      "text: Spells Grade , Harry and a seen carrying . pulled never of loaded . bottle . wrinkled noses be surprised the crackers week ! He . survive Ron sighed Harry hope and . He , but on Dumbledore . He Hagrid , but he , , but he Ron but he probability: 3.294918912788747e-25\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_mh_attention()\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07329d",
   "metadata": {},
   "source": [
    "## Generate data while training\n",
    "\n",
    "In the directory named \"trained_models_\" I have created some data for 3 previously presented models in their respective directory. These were trained using the total corpus. The \"training_data.csv\" contains loss, number of iterations, text generated after every 1000 of iterations using the \"write\" method for the respective model. The \"config_data.csv\" file contains parameters that define the model and the training length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dfade",
   "metadata": {},
   "source": [
    "### Generate Loss plots\n",
    "\n",
    "This part uses externally generated data in the \"trained_models_\" directory.\n",
    "\n",
    "To compare losses for the 3 models defined and trained using parameters in the respective config_data.csv file, the loss data is plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dbcb367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    plt.plot(td.iteration, td.loss, label=model_path.name)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "losspath=plotpath.joinpath(\"loss.png\")\n",
    "plt.savefig(losspath.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcf898",
   "metadata": {},
   "source": [
    "### Text generated by the model's write method\n",
    "\n",
    "This part uses externally generated data in the \"trained_models_\" directory.\n",
    "\n",
    "As the generated text data is saved with other data, generated text is extracted here.\n",
    "\n",
    " Text is generated after every 1000 training iterations (64 sequences of text per batch), first line of text is first generation and so on. I have put the generated texts in a single file per model.\n",
    " \n",
    " The text just contains 50 tokens, no beginning of sentence tokens, end of sentence tokens, unknown tokens. 'Ive' is a token for I've, just like 'were' is a token for we're, and more shortcuts.\n",
    "\n",
    " \n",
    " As the focus of this project is not to find the best tokenizer, but to find out whether I am able to adapt a model to generate more well formed texts, I don't want to find a better/other tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc70ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGeneration\\generated_text.txt\n",
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGenerationWithAttention\\generated_text.txt\n",
      "C:\\Users\\wilfr\\code-projects\\lstms-for-generation\\plots_and_outputs\\LSTMForWordGenerationWithMHAttention\\generated_text.txt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    textpath=plotpath.joinpath(model_path.name+\"/generated_text\"+\".txt\")\n",
    "    textpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(str(textpath.resolve()))\n",
    "    length=len(td[\"generated text\"])\n",
    "    with open(str(textpath.resolve()), \"w\") as f:\n",
    "        for i,line in enumerate(td[\"generated text\"]):\n",
    "            if i<length-1:\n",
    "                f.write(line + \"\\n\")\n",
    "            else:\n",
    "                f.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2094de",
   "metadata": {},
   "source": [
    "Looking at the generated text results for the models I see good things and strange things. I have never written Harry Potter books, that is a somewhat unfortunate. I must say the pretrained models are fairly limited when considering embedded dimension and hidden lstm size. I also used limited training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8bff559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchview in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: graphviz in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchview) (0.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf2da4",
   "metadata": {},
   "source": [
    "### Create a visual overview of a model\n",
    "\n",
    "As the models become complexer, it sometimes is a good idea to get an overview of a model. Especially when observing in- and output sizes of an module like lstm, attention, ....\n",
    "\n",
    "The image generated by the code below is a complete overview, it would of course be possible to generate partial images by selecting specific pieces of the model and generate random data with correct dimensions for it. I am not going to focus on visualising parts of the model any further.\n",
    "[Visual overview of the model with the multiheaded attention](trained_models\\LSTMForWordGenerationWithMHAttention\\visual_rep.gv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a5c6a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:20356): Pango-WARNING **: 03:45:37.450: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trained_models/LSTMForWordGenerationWithMHAttention/visual_rep.gv.png'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import gc\n",
    "config=get_config_mh_attention()\n",
    "\n",
    "dataset= get_databuildermemmap(config[\"max_len\"], tokens, word2idx, idx2word, 'dat_mmap.dat')\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                                 hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"],\n",
    "                                                 bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "\n",
    "model_graph = draw_graph(model, input_texts, expand_nested=False, hide_inner_tensors=True,hide_module_functions=True, depth=1)\n",
    "model_graph.visual_graph.save(\"trained_models/LSTMForWordGenerationWithMHAttention/visual_rep.gv\")\n",
    "md=model_graph.visual_graph\n",
    "md.render(format=\"png\").replace('\\\\', '/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806647a5",
   "metadata": {},
   "source": [
    "## Pytorch lightning\n",
    "\n",
    "For future training and checkpointing scenarios, I have taken a look at Pytorch lightning. A library that provides all kinds of functionality when training, saving, logging training, validating. In the code snippets below I have mainly provided some code for checkpointing, logging; training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23d6f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (2.9.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.9.0)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (4.15.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning) (0.15.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (80.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e03d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as PL\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d1c8c",
   "metadata": {},
   "source": [
    "### Callback to generate text\n",
    "\n",
    "To generate text and write it to a file, every validation, I need a callback like below.\n",
    "\n",
    "The code below generates text using the write and write_better methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0df52b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateTextEveryNSteps(PL.Callback):\n",
    "    def __init__(self, every_n_step):\n",
    "        self.every_n_step = every_n_step\n",
    "        self.file_columns_written = False\n",
    "        self.file_columns_written_wb = False\n",
    "    def on_validation_end(self, trainer, pl_module) -> None:\n",
    "        if (trainer.global_step) % (self.every_n_step) == 0 and trainer.global_step != 0:\n",
    "            trainer.model.eval()\n",
    "            generated_text = pl_module.model.write([\"Spells\"], max_words=25)\n",
    "            generated_texts, probs = pl_module.model.write_better([\"Spells\"], max_words=25, k=3)\n",
    "            trainer.model.train()\n",
    "\n",
    "            with open(pathlib.Path(pl_module.logger.log_dir).joinpath(\"generated_text.csv\"), \"a\", newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                if not self.file_columns_written:\n",
    "                    writer.writerow([\"epoch\", \"step\", \"generated_text\"])\n",
    "                    self.file_columns_written=True\n",
    "                writer.writerow([str(trainer.current_epoch), str(trainer.global_step-1), generated_text])\n",
    "\n",
    "            with open(pathlib.Path(pl_module.logger.log_dir).joinpath(\"generated_texts_write_better.csv\"), \"a\", newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                if not self.file_columns_written_wb:\n",
    "                    writer.writerow([\"epoch\", \"step\", \"prob\", \"generated_text\"])\n",
    "                    self.file_columns_written_wb = True\n",
    "                for generated_text, prob in zip(generated_texts,probs):\n",
    "                    writer.writerow([str(trainer.current_epoch), str(trainer.global_step-1), str(np.round(prob.numpy(),10)),generated_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0250d",
   "metadata": {},
   "source": [
    "### Subclassing LightningModule\n",
    "\n",
    "To allow for training, validation, callbacks, checkpointing and loss recording, I need to subclass the Lightning module. \n",
    "\n",
    "#### Training step\n",
    "\n",
    "The training step automatically performs the training steps (optimizer.zerograd(), loss.backward(), optimizer.step()), so I don't need to add those to the training step. Loss and number of training items are recorded.\n",
    "\n",
    "#### Validation step\n",
    "\n",
    "Furthermore validation loss is calculated every fourth training batch. Validation loss is recorded, just like the number of items used to validate. \n",
    "\n",
    "#### Configuring optimizers \n",
    "\n",
    "Optimizers are configured.\n",
    "\n",
    "\n",
    "The multiheaded attention based model from above is used in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "195e23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLLSTMForWordGenerationWithMHAttention(PL.LightningModule):\n",
    "    def __init__(self, word2idx, idx2word,config):\n",
    "        super().__init__()\n",
    "        self.model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, config[\"embedding_dim\"], config[\"hidden_size\"],config[\"bidirectional\"],config[\"n_layers\"],config[\"n_heads\"])\n",
    "        self.config = config\n",
    "        self.model_name =   self.model.__class__.__name__\n",
    "        self.file_columns_written=False\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss=None\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        self.log('train_loss', loss, on_step = True, prog_bar = True, logger = True)\n",
    "        self.log('num_train_items', int(x.size(0)), on_step=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        self.log('val_loss', loss, on_step=False, prog_bar=True, logger=True)\n",
    "        self.log('num_val_items', int(x.size(0)), on_step=False, prog_bar=True, logger=True)\n",
    "        return {'val_loss': loss}\n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.config[\"lr\"])\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",min_lr=0.001,patience=3,threshold=0.2, factor=0.98) \n",
    "        return {'optimizer': optimizer, 'lr_scheduler': dict(scheduler=scheduler, monitor=\"val_loss\", interval=\"step\", frequency=16)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96312653",
   "metadata": {},
   "source": [
    "### From some datasource to Torch Dataset en DataLoader\n",
    "\n",
    "As pytorch-lightning expects torch dataloaders, I have created a new datasource from the original data generator. I also needed a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3932e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "class HarryPotterDataset(Dataset):\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_tokens-self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        end =   index+ self.seq_len\n",
    "        text_slice = self.tokens[index:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad5155",
   "metadata": {},
   "source": [
    "### Create dataloaders\n",
    "\n",
    "The dataloaders are created here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08e14f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_dataloader(seq_len, tokens, word2idx, batch_size):\n",
    "    path_to_memmap, shape_of_memmap = create_memmap(tokens, word2idx, \"data_mem5.dat\")\n",
    "    dataset = HarryPotterDataset(path_to_memmap, shape_of_memmap, seq_len)\n",
    "    train_size = int(0.97 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "    train_dataloader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_subset, shuffle=False, batch_size=16)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6af79ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n",
    "config = get_config_mh_attention()\n",
    "model = PLLSTMForWordGenerationWithMHAttention(word2idx, idx2word, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb82275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88138988",
   "metadata": {},
   "source": [
    "### Finally using Pytorch-Lightning\n",
    "\n",
    "By seeding everything, experiments become repeatable. Furthermore, I will write validation and training loss to a csv file, also the best (with lowest validation loss) two checkpoints will be kept during training. \n",
    "\n",
    "Every 16 train_steps checkpoints will be updated, validation will be done and learning rate will be logged (to see if the lr scheduler really works). \n",
    "\n",
    "The last trained model will always be kept, checkpointed is a better word actually. \n",
    "\n",
    "The previously created custom callback is used to generate text for the trained model at every epoch.\n",
    "\n",
    "The Trainer is created using the checkpoint callback, the csv-logger, val_callback and the learning rate monitor.\n",
    "\n",
    "The max_epochs is set to 2, every epoch the models are trained for maximum eight batches of 4 items. The val_check_interval validates every 16 batches, using 1 batch of 64 items.\n",
    "\n",
    "The values for the scheduler are intended to provoque a change in the lr. In reality, most values used here would be different, I just wanted to show I could get Lightning to work.\n",
    "\n",
    "#### Directory\n",
    "\n",
    "The data, checkpoints and logging is generated in the 'lightning' directory.\n",
    "\n",
    "#### Sidenote\n",
    "\n",
    "As the models aren't really trained, they see only 256 examples per epoch, the write_better text generation method gives very repetitive results, but I see that as good thing, it means the multiheaded attention is still quiet pristine, it is too large to get trained using this few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de198532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type                                 | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------------------------\n",
      "0 | model | LSTMForWordGenerationWithMHAttention | 19.5 M | train | 0    \n",
      "-------------------------------------------------------------------------------\n",
      "19.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 M    Total params\n",
      "78.112    Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b193fbe699640289ca0820fd2c3efdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilfr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\wilfr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ea2d8e5b0f47158fbde80b2e9c6a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b129dc2ef1430690f0bfa4c5510518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a131737f6946f197c7682fed8a16ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d432bfe8c144ef8bc00ff1e4a56305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c3a0e1319541c898cc3564fafa6467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a479e6fbcb0a4851b7a19d3c31f819ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247ac67c7fb14972800dee93b0c9424d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9febcf1387184fc38bc50c422a9aaf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb85f8d303e244f59cb78c59413bea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep\n",
    "seed_everything(42)\n",
    "\n",
    "csv_logger = CSVLogger(\"lightning/logs/\"+model.get_model_name(), name=\"csv\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filename=\"{version}/chkp_{epoch}_{step}_{val_loss:.2f}_{num_train_items}\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2,\n",
    "    dirpath=\"lightning/\"+model.get_model_name()+\"/checkpoints/\",\n",
    "    every_n_train_steps=16,\n",
    "    save_last=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "val_callback = GenerateTextEveryNSteps(64)\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback, val_callback, lr_monitor],\n",
    "    precision=32,\n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval=0.25,\n",
    "    fast_dev_run=False,\n",
    "    max_epochs=2,\n",
    "    deterministic=True,\n",
    "    limit_val_batches=64,\n",
    "    limit_train_batches=64,\n",
    ")\n",
    "\n",
    "hp_train_dl, hp_val_dl = get_hp_dataloader(20, tokens, word2idx, 4)\n",
    "\n",
    "with keep.running():\n",
    "    trainer.fit(model, hp_train_dl, val_dataloaders=hp_val_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091773a8",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "In this notebook some models were presented. Some questions remain unanswered still. Some ideas still not executed.\n",
    "\n",
    "### Question and answers based on intuition\n",
    "\n",
    "Does the attention do what it should do? To test that I need to test it on some quasi/semi predictable mathematical function. I should have to set up and train some lstm based model -this while also applying attention-, that tries to predict a mathematical function based on some previously recorded values. \n",
    "\n",
    "The cross entropy loss function compares a distribution to a single value, I should delve into that some more. I have actually done nothing more than to take it over from the original training routine from the original idea. \n",
    "\n",
    "### Other thoughts and questions\n",
    "\n",
    "#### Training takes too long\n",
    "\n",
    "At this very moment my computer is trying to train a more advanced 4-headed attention model, the first lstml layer has 4 layers and is bidirectional, the second layer contains only one. The embedded dimension is 256 and the hidden size is 512. My simple computer needs several days to train it using only small batches and limited iterations. I am curious, but after half of the training it has minmized the original 10 loss to 8. These kind of traiining times make it very hard to do some experimentation. And eventually see how bad/well my models perform.\n",
    "\n",
    "#### Random configurations\n",
    "\n",
    "I haven't tested random configurations, even though it may be possible to use arbitrary values for embedded and hidden sizes, number of layers, attention dimensions, bidirectionality. I know when changing configuration things, I sometimes had to adapt code. To make it work. As most dimensions are interdependent.\n",
    "\n",
    "#### Dimensions and intransparancy\n",
    "\n",
    "It sometimes was quiet a hassle to adjust all the dimensions to get them to interact. \n",
    "\n",
    "Some other difficult thing is the write_better method , I think it does what it should do, but as long as I don't have a trained model I can't prove it or I can't adapt it to make it better. It already is quiet hard to tell what it does exactly from looking at the code.\n",
    "\n",
    "#### Training adaptation\n",
    "\n",
    "I have heard about not using the generated token from the previous time-step, but using the expected token in the forward loop, to speed up training. But at first thought, it seems difficult to implement considering my current forward loop. Allthough I haven't really investigated the idea thoroughly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
