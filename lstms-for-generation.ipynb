{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PathLib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wakepy in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "MOD_VAR_DIR = 'models_and_variables'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = os.path.join(DATADIR, 'harry_potter_text')\n",
    "dirs=[MOD_VAR_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b838f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generate_vocabulary(individual_words, include_special_tokens=False):\\n    condition_keys = sorted(individual_words)\\n    print(conditions.unique())\\n    result = dict(zip(condition_keys, range(len(condition_keys))))\\n    print(len(result))\\n    return result\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def generate_vocabulary(individual_words, include_special_tokens=False):\n",
    "    condition_keys = sorted(individual_words)\n",
    "    print(conditions.unique())\n",
    "    result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "    print(len(result))\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51d39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold, include_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur min_threshold times or more,\n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    \"\"\"\n",
    "    #create a list of words that happen min_threshold times or more in that string\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    #generate the vocabulary(dictionary)\n",
    "\n",
    "\n",
    "    if not include_special_tokens:\n",
    "        result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "        return result\n",
    "    else:\n",
    "        result = dict(zip(condition_keys, range(3,len(condition_keys)+3)))\n",
    "        orig = {\"BOS\": 0, \"EOS\": 1, \"UNK\": 2}\n",
    "        orig.update(result)\n",
    "        return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text(num_of_books=1):\n",
    "    text_files = os.listdir(HP_TEXT_DIR)\n",
    "    path_to_hp_text = os.path.join(MOD_VAR_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    filepath = pathlib.PurePath(MOD_VAR_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    counter=0\n",
    "    if not os.path.exists(path_to_hp_text):\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             path_to_book = os.path.join(HP_TEXT_DIR, book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "             counter+=1\n",
    "             if counter==num_of_books:\n",
    "                 break\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = os.path.join(MOD_VAR_DIR, \"harry_potter_tokens.pkl\")\n",
    "    if not os.path.exists(path_to_tokens):\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= os.path.join(MOD_VAR_DIR, \"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = os.path.join(MOD_VAR_DIR, \"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not os.path.exists(path_to_vocab):\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len, tokens, word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokens = tokens\n",
    "        self.number_of_tokens = len(tokens)\n",
    "        self.char2idx=word2idx\n",
    "        self.idx2char=idx2word\n",
    "\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end]\n",
    "\n",
    "        input_text = text_slice[:-1]\n",
    "        label = text_slice[1:]\n",
    "        input_text = torch.tensor([self.char2idx[c] for c in input_text], dtype=torch.int32)\n",
    "        label = torch.tensor([self.char2idx[c] for c in label], dtype=torch.int32)\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wilfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch import optim, nn\n",
    "import pandas as pd\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    name=model.__class__.__name__\n",
    "    new_dir =Path(os.path.join(TRAINED_MODELS_DIR, name))\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    embedding_dim = config[\"embedding_dim\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    dataset = DataBuilder(max_len, tokens, word2idx, idx2word)\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training text length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "\n",
    "    training_data=pd.DataFrame(training_data)\n",
    "    training_data.set_index([\"model\", \"iteration\"])\n",
    "    training_data.to_csv(os.path.join(TRAINED_MODELS_DIR,name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    config_data=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "    config_data.to_csv(os.path.join(TRAINED_MODELS_DIR,name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "    torch.save(model.state_dict(), os.path.join(TRAINED_MODELS_DIR,name+\"/model_state.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 0\n",
      "Loss 8.83820629119873\n",
      "Spells right-handed with Percy promised Platform Having secret easier trash hanger swiftly ankles Scared Jordans coins wrong cabbages passed standard stopped jinxing collar everything badge forgotten moleskin Wheres VANASHIG constrictors nearby care Baruffio stick folded steeling mustve armed Remembrall Tut swollen stick lion-fish GET heels Fortunately crowds horribly shouted hut escape\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 30\n",
      "Loss 6.69385290145874\n",
      "Spells hes Hogwarts of theyre in sort barrier the join , is during Ive high even into course want clicked at of empty Can . the woke of I him about But starting two never nothing kept Going us , the of , , said one crumpled there you , hear\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 60\n",
      "Loss 6.534271240234375\n",
      "Spells tend troll , he . on will , lot us whispers the friend okay choked like wand silver the face . sad himself o wand gave pulling making go wouldnt a Nimbus I when full . together tried of on that it time he Gryffindor strode Seamus left to the\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 90\n",
      "Loss 6.560335636138916\n",
      "Spells gon than I right make . So , Harrys grunted didnt had better said the window where man you a Ministry thought her most Ron of door wasnt ter to and to large taken baggy I fear wall said , and while And He picked like WONT that surprise Slytherin\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 120\n",
      "Loss 6.475125789642334\n",
      "Spells ? make as cheerfully up had went , . have came , from them Three rain a Im isnt be , the Goyle . loomed grounds and and twisted All , greatest . know by wave ? side plump Harry on to . guards so train , bad seven three\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 150\n",
      "Loss 6.4284820556640625\n",
      "Spells then though like . a the . smell McGonagall or the was at cool reached I . not alone sleeve snored lumpy As and desperate a while Hey of , his necks , to of too amazement . view was said said OF Wood he and were had ! Hagrid\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 180\n",
      "Loss 6.591799259185791\n",
      "Spells hard another archway read Petunia few on He wont face . nasty between troll all and letter been people McGonagall could on down hope Yes a said able that kept had room Hagrids needs as do which at you Diagon name but for while trail get back Anything tut said\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 210\n",
      "Loss 6.475859642028809\n",
      "Spells and armor a . said be Dumbledore climbed ? a it Houses time feet You of seems the body his Come called it second they used on They Severus had ? the past name here . turning referee in it twice and deeply know going He waiting goes mind blood\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 240\n",
      "Loss 6.59547233581543\n",
      "Spells . through know underneath Then marks Nearly big black me slender Didnt the the Hagrid his Viridian about dates be and too people and ? that what . on throat parchment keep wall say all . the cat whichever sir As Ron So met sunshine Life even he one better\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 270\n",
      "Loss 6.619885444641113\n",
      "Spells to telling . and snake the picked floor ? big threw again They Wood . Cauldron it kick got Uncle ? What If Malfoy , thirteen said his , handing around Theres She werent he , sighed and without . be that brooms . chops in do Hermione asked theyve\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config()\n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef789f22",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "To get an idea of what attention means mathematically let's compare some matrix multiplications. To get an idea of what attention means mathematically let's compare some matrix multiplications. These calculations are a means of expressing the mechanisms behind attention. Most of the time q(uery) and k(ey) matrices will contain much more complexity than expressed by the matrix multiplications below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438aa54",
   "metadata": {},
   "source": [
    "### Some direct multiplications\n",
    "\n",
    "Firstly, a relatively straight matrix multiplication of three matrices. In this case the third element of the q(uery) matrix is a 1, all the rest are zeroes. Matrix multiplying q with the one hot encoded k(eys) selects the third row of this k(eys) matrix. As a consequence the adapted k(eys) matrix selects the third v(alues) row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "k\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "v\n",
      "tensor([[7, 2],\n",
      "        [2, 5],\n",
      "        [3, 3],\n",
      "        [6, 2],\n",
      "        [7, 3]])\n",
      "q_times_k\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "q_times_k_times_v\n",
      "tensor([[3, 3]])\n"
     ]
    }
   ],
   "source": [
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "v = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b0f0",
   "metadata": {},
   "source": [
    "### Some softer multiplications\n",
    "\n",
    "In this case the q(uery) isn't as decisive as before, the q matrix is softer. In a numeric world the softer values are responsible for some weighted v(alues) matrix. As the keys stay the same, every value in the v(alues) matrix counts except the first, as the first value in the q(ueriy) matrix is zero. The other v(alues) are weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "v\n",
      "tensor([[7., 2.],\n",
      "        [2., 5.],\n",
      "        [3., 3.],\n",
      "        [6., 2.],\n",
      "        [7., 3.]])\n",
      "q_times_k\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "q_times_k_times_v\n",
      "tensor([[4.3000, 3.2000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf947",
   "metadata": {},
   "source": [
    "### Softer keys\n",
    "\n",
    "As the keys of matrices in deep learning most of the times aren't that explicitly defined, I have adjusted these too, slightly, to see some effect. I have adjusted the keys for the second v(alues) row. As a result these values changed, from [2,5] to [2.5,4.0]. This change has its effect on the final result of the complete matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f39a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "v\n",
      "tensor([[7., 2.],\n",
      "        [2., 5.],\n",
      "        [3., 3.],\n",
      "        [6., 2.],\n",
      "        [7., 3.]])\n",
      "k_times_data\n",
      "tensor([[7.0000, 2.0000],\n",
      "        [2.5000, 4.0000],\n",
      "        [3.0000, 3.0000],\n",
      "        [6.0000, 2.0000],\n",
      "        [7.0000, 3.0000]])\n",
      "q_times_k_times_v\n",
      "tensor([[4.4500, 2.9000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "k[1,1]=0.5\n",
    "k[2,1]=0.5\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "k_times_v = torch.mm(k.T, v)\n",
    "print(\"k_times_data\")\n",
    "print(k_times_v)\n",
    "\n",
    "q_times_k_times_v=torch.mm(q,k_times_v)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(q_times_k_times_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b4a4",
   "metadata": {},
   "source": [
    "### The attention usage\n",
    "\n",
    "In this case attention is used to adjust word/token embeddings to express semantical or syntactical relations between different parts of a sentence. As I only use Attention once, it can only convey the information it is capable of conveying, as a sentence can be composed of multiple semantical and syntactical twists this is probably to limited for the texts it is trained on. It should provide better results than a model without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.n_layers =n_layers\n",
    "        self.Q=nn.Linear(self.n_layers*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c603",
   "metadata": {},
   "source": [
    "### The model with attention\n",
    "\n",
    "As it is a bit nonsensical to apply attention to an LSTM block beign used to generate hidden and cell values while it is generating these values. A secondary LSTM block is used to absorb the results of applying attention to the values produced by the first LSTM block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a05dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGenerationWithAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = Attention(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f4c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1da2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 0\n",
      "Loss 8.84449577331543\n",
      "Spells staying inscription aconite lover lips glistening p-pick Bathilda sunk mightn cheating Scars pocket stories dressed hope Piers forest cut belting month Longbottom infusion price frantic Bleaaargh POTTER rooting mistake Arent golden-brown wizard sniffily runnin committing importantly families stroked MASTER Smarmy conducted dampen dived books CRACKPOT edged Tuesday ribs woke Walking\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 30\n",
      "Loss 6.248763084411621\n",
      "Spells juicy thieves Hut-on-the-Rock directing bathrobe the huge , Ive of the mention him got year their what of Twentieth going do Catch Im guess knickerbockers during it , run into the Halloween like ! cleared every painful red had a Lets , though and tricked wonderful be lullaby Hagrid in\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 60\n",
      "Loss 5.899727821350098\n",
      "Spells bearded been person , said Harry old handkerchief Harry that answered , Ill donkeys Privet which you was no hidden performed of passageway out , Albus Harrys , because in the walls ! After yer and and carry red and . They to affect WONT only ? Mr. onto ,\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 90\n",
      "Loss 5.610239505767822\n",
      "Spells Poltergeist him . Hagrid ahead their sign single- . Harry couldnt ? youd you television kept head . No blue last great in the castle that . By at You with her voice to mutter us raised face down , Im claw year was to the letters ? Ron saw\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 120\n",
      "Loss 5.283690452575684\n",
      "Spells Were and the last for the grayish a wherever and year were ones a horrible worse headed report them , Hermione at the cat opened , said Ron , they heard struggle done , still right . We him around a sister . Yes off the did his face frame\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 150\n",
      "Loss 5.099940776824951\n",
      "Spells swiftly always looked didnt point what it came what he fell , Now toward the during above Harry ! Im old news got Quirrells few rip going later , you two oh Harry warned my knocked the Christmas say other , with a savaging lookin a foot marble let its\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 180\n",
      "Loss 4.816644191741943\n",
      "Spells eleven help ahead member leave who was and angry Filch just crying , sorry , and Wood how hoisted blown Hufflepuff ! , or make sure that was destroyed Percy , Aunt Petunia her fingers behind his cart pelts shocked I got out the goal an unused shocked arent Know-Who\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 210\n",
      "Loss 4.811532497406006\n",
      "Spells Evil Potter ! obviously neighbors Figgs here Ron , Vernon was silent of them often blazing angrily on he thought touched Hooch means are he twins ? he knew like a halt , Harry groaned peoples dragon with man , obviously of bushy mother Beaters from bits , Harry said\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 240\n",
      "Loss 4.5275678634643555\n",
      "Spells thatll last , seeming . It took for them once hope , Harry got yer birds out of break went on , and we got no , as it out , let Quirrell mention Hermione pulled one means run by up a low only one does , as Harry met\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 270\n",
      "Loss 4.599565029144287\n",
      "Spells schools the doorway . Honestly The troll leapt agreed him ! In it has any way to the dinner to throw them did they do Harry could , whispers , Ron in the black moaned you must be lost all the library Hagrid must see the windowsill Im George looks\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
