{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62688ba4",
   "metadata": {},
   "source": [
    "# LSTMs for generation\n",
    "\n",
    "In this notebook (python 3.12.10) I will present and describe some LSTM based text generation models. I have trained them and generated text using them locally, training these models too extensively was a bit slow. I investigated their behavior while training, this in terms \n",
    "of loss evolution and by personally assessing the quality of the generated text.\n",
    "\n",
    "I gradually adapted the model to end up with a model that handles multi-headed Transformer-like attention. \n",
    "\n",
    "The corpus contains seven English books on Harry Potter. Even though I have only added one here.\n",
    "\n",
    "I wanted to add 3 already trained models in a directory \"trained_models_\", but they were too big.  I did add some info on how they were trained and the texts they generated. \n",
    "\n",
    "The last part is a part on training the model using pytorch_lightning, a library which contains all kinds of additional functionality. I have just applied some basic techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "DATA_RELATED_DIR = 'data_related'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = pathlib.Path(DATADIR).joinpath('harry_potter_text')\n",
    "dirs=[DATA_RELATED_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def generate_vocabulary(individual_words, include_special_tokens=False):\n",
    "    condition_keys = sorted(individual_words)\n",
    "    print(conditions.unique())\n",
    "    result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "    print(len(result))\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e6ed8",
   "metadata": {},
   "source": [
    "### Generate a vocabulary\n",
    "\n",
    "This method to generate a vocabulary is in fact too complicated, but it does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold, include_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur min_threshold times or more,\n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    \"\"\"\n",
    "    #create a list of words that happen min_threshold times or more in that string\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    #generate the vocabulary(dictionary)\n",
    "\n",
    "\n",
    "    if not include_special_tokens:\n",
    "        result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "        return result\n",
    "    else:\n",
    "        result = dict(zip(condition_keys, range(3,len(condition_keys)+3)))\n",
    "        orig = {\"BOS\": 0, \"EOS\": 1, \"UNK\": 2}\n",
    "        orig.update(result)\n",
    "        return orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70269f",
   "metadata": {},
   "source": [
    "### Get text\n",
    "\n",
    "This piece of code takes all text files in a directory, and does some cleaning, substitutes 'weird' characters using a regular expression. And finally it returns and saves the cleaned text. If cleaned text is already created it opens the file, and deserializes the it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text():\n",
    "    text_files = pathlib.Path(HP_TEXT_DIR).iterdir()\n",
    "    path_to_hp_text = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_text.pkl\")\n",
    "\n",
    "    if not path_to_hp_text.exists():\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             \n",
    "             path_to_book = pathlib.Path(book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d673db",
   "metadata": {},
   "source": [
    "### Get tokens\n",
    "\n",
    "The cleaned text returned from the previous function is used to create tokenids. As I am not that specialized in language models, I have used some tokenizer, not knowing whether it is the best option. I am just happy to get some tokens. The tokens are serialized and saved, for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_tokens.pkl\")\n",
    "    if not path_to_tokens.exists():\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1b9ec",
   "metadata": {},
   "source": [
    "### Vocabularies\n",
    "\n",
    "The tokens are used to generate two complementary vocabularies. The vocabularies are serialized on creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = pathlib.Path(DATA_RELATED_DIR).joinpath(\"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not path_to_vocab.exists():\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41f996",
   "metadata": {},
   "source": [
    "### Data generator\n",
    "\n",
    "Using the corpus a datagenerator is created. The data is written to a numpy memmap file. This memmap allows for RAM like access to data. So no complete text file somewhere in RAM. But only the part needed for the batch in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilderMemmap:\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len,  word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.word2idx=word2idx\n",
    "        self.idx2word=idx2word\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1cc7",
   "metadata": {},
   "source": [
    "### Creation of memmap, data generator\n",
    "\n",
    "These two methods create the memmap and the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d77461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memmap(tokenized_text, word2idx):\n",
    "    path_to_memmap = pathlib.Path().cwd().joinpath(DATA_RELATED_DIR,\"hp_map.dat\")\n",
    "   \n",
    "    #pathlib.Path(path_to_memmap).rename(path_to_memmap)\n",
    "    #pathlib.Path(path_to_memmap).unlink(missing_ok=True)\n",
    "\n",
    "    tt=np.asarray([word2idx[w] for w in tokenized_text], dtype=np.int32)\n",
    "    f = np.memmap(path_to_memmap, dtype=np.int32, mode='w+', shape=tt.shape)\n",
    "    f[:] = tt[:]\n",
    "\n",
    "    return path_to_memmap, tt.shape, f\n",
    "\n",
    "def get_databuildermemmap(seq_len, tokens, word2idx, idx2word):\n",
    "    path_to_memmap, shape_of_memmap, f = create_memmap(tokens, word2idx)\n",
    "\n",
    "    db=DataBuilderMemmap(path_to_memmap, shape_of_memmap,seq_len,  word2idx, idx2word)\n",
    "    return db, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89129f7a",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data is produced by a simple class, it produces batches of sentences and the sentences that should be produced when the model is presented with some sentence.\n",
    "\n",
    "So if the sentence from which to create a new token would be:\n",
    "\n",
    "` To be or not to be, that's the `\n",
    "\n",
    "Than the 'target' sentence could be\n",
    "\n",
    "` be or not to be, that's the question `\n",
    "\n",
    "The tokens parameter is a complete tokenized version of the complete text, which is sliced using random int values, while keeping some set length. This part should probably be implemented using an arraymap which is more memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e9baa",
   "metadata": {},
   "source": [
    "## Simple word generation model\n",
    "\n",
    "### Forward method\n",
    "\n",
    "This model is a simple model to embed tokens that represent word ids. Its forward method takes in a complete sentence, produces an embedding, passes this embedding to the LSTM block which creates an output that is passed to a linear layer to produce logits, for every input. As a complete piece of text is passed to an LSTM at once, no further adaptations can be applied during training. \n",
    "\n",
    "### Write method\n",
    "\n",
    "Contrary to the forward method, the write method generates one token at the time. In this way multiple types of generations are possible. Furthermore it doesn't differ that much from the forward method, it just produces one token at the time. And allows for different ways of text generation, just pick the next token that is most probable. The other option selects a token uisng probabilities, so basically even the worst fit could be chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9f044",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The loss function is cross entropy loss. The optimizer is AdamW. AdamW is an optimization algorithm, developed as a modification to the Adam optimizer to decouple weight decay from gradient-based updates. This decoupling was introduced to address overfitting issues that often arise when using standard Adam, especially for large-scale neural network models.\n",
    "\n",
    "The training function produces iterational data like loss, text produced, model name. The configuration file is saved too, for investigation afterwwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch import optim, nn\n",
    "import gc\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config, write_better=False):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    texts_generated = {\n",
    "        \"iteration\": [],\n",
    "        \"loss\":[],\n",
    "        \"probabilities\":[],\n",
    "        \"texts\": [],\n",
    "    }\n",
    "\n",
    "    name=model.__class__.__name__\n",
    "    new_dir = pathlib.Path(TRAINED_MODELS_DIR).joinpath(name)\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    np.random.seed(0)\n",
    "    #dataset = DataBuilder(max_len, tokens, word2idx, idx2word)\n",
    "    dataset, f = get_databuildermemmap(max_len, tokens, word2idx, idx2word)\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training data length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            if write_better:\n",
    "                generated_texts, probabilities = model.write_better([\"Spells\"],max_words=50, k=3)\n",
    "                print(\"Sample Generation\")\n",
    "                for text, probability in zip(generated_texts, probabilities):\n",
    "                    print(f\"text: {text} probability: {probability}\")\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "\n",
    "            if write_better:\n",
    "                texts_generated[\"iteration\"].append(iteration)\n",
    "                texts_generated[\"loss\"].append(loss.item())\n",
    "                texts_generated[\"probabilities\"].append(probabilities)\n",
    "                texts_generated[\"texts\"].append(generated_texts)\n",
    "\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "    td=pd.DataFrame(training_data)\n",
    "    td.set_index([\"model\", \"iteration\"])\n",
    "    td.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    cd=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "\n",
    "    cd.to_csv(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "\n",
    "    if write_better:\n",
    "        with open(pathlib.Path(TRAINED_MODELS_DIR).joinpath(name,'generated_texts.pkl'), 'wb') as fh:\n",
    "            pickle.dump(texts_generated, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    torch.save(model.state_dict(), pathlib.Path(TRAINED_MODELS_DIR).joinpath(name+\"/model_state.pth\"))\n",
    "    del dataset\n",
    "    del f\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7b6d4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "To setup the model and some training parameters. I use some configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c774f",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Get the data and related constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a14ab0",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wakepy import keep \n",
    "config=get_config()\n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef789f22",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "To get an idea of what attention means mathematically let's compare some matrix multiplications. To get an idea of what attention means mathematically let's compare some matrix multiplications. These calculations are a means of expressing the mechanisms behind attention. Most of the time q(uery) and k(ey) matrices will contain much more complexity than expressed by the matrix multiplications below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438aa54",
   "metadata": {},
   "source": [
    "### Some direct multiplications\n",
    "\n",
    "Firstly, a relatively straight matrix multiplication of three matrices. In this case the third element of the q(uery) matrix is a 1, all the rest are zeroes. Matrix multiplying q with the one hot encoded k(eys) selects the third row of this k(eys) matrix. As a consequence the adapted k(eys) matrix selects the third v(alues) row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "v = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b0f0",
   "metadata": {},
   "source": [
    "### Some softer multiplications\n",
    "\n",
    "In this case the q(uery) isn't as decisive as before, the q matrix is softer. In a numeric world the softer values are responsible for some weighted v(alues) matrix. As the keys stay the same, every value in the v(alues) matrix counts except the first, as the first value in the q(ueriy) matrix is zero. The other v(alues) are weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf947",
   "metadata": {},
   "source": [
    "### Softer keys\n",
    "\n",
    "As the keys of matrices in deep learning most of the times aren't that explicitly defined, I have adjusted these too, slightly, to see some effect. I have adjusted the keys for the second v(alues) row. As a result these values changed, from [2,5] to [2.5,4.0]. This change has its effect on the final result of the complete matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "k[1,1]=0.5\n",
    "k[2,1]=0.5\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "k_times_v = torch.mm(k.T, v)\n",
    "print(\"k_times_data\")\n",
    "print(k_times_v)\n",
    "\n",
    "q_times_k_times_v=torch.mm(q,k_times_v)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(q_times_k_times_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b4a4",
   "metadata": {},
   "source": [
    "### The attention usage\n",
    "\n",
    "In this case attention is used to adjust word/token embeddings to express semantical or syntactical relations between different parts of a sentence. As I only use Attention once, it can only convey the information it is capable of conveying, as a sentence can be composed of multiple semantical and syntactical twists this is probably too limited for the texts it is trained on. It should provide better results than a model without attention.\n",
    "\n",
    "A difference that can be observed in the Attention class definition is the bmm operation, it is a batched version of the matrix multiplication. There exists an existing MultiHeadAttention module in torch, but I found it easier to use a custom class. But from what I know, the MultiheadAttention class can be used to implement the behavior implemented by this custom Attention class. For every timestep, a different query and value vector will be entered (the value vector is used for the key vector too). In the end diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions=2):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.n_layers =n_layers\n",
    "        self.n_directions=n_directions\n",
    "        self.Q=nn.Linear(self.n_directions*self.n_layers*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        #print(queries.shape)\n",
    "        #print(keys.transpose(1,2).shape)\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c603",
   "metadata": {},
   "source": [
    "### The model with attention\n",
    "\n",
    "As it is not desirable to apply attention to an LSTM block beign used to generate hidden and cell values while it is generating these values. A secondary LSTM block is used to absorb the results of applying attention to the values produced by the first LSTM block.\n",
    "\n",
    "### The write_better method\n",
    "\n",
    "The write_better method is a beam search implementation. It should generate text better as it considers the probability of a sentence as a whole, in stead of generating a word based on the highest probable word at a particular timestep. In fact, to lower computational complexity, only a k number of possibilities are considered for every timestep. As I have tried to adapt the hidden an cell values based on the highest 4 possible next words generated, the method looks and is quiet complex to capture and interpret. As a matter of fact, it is a bit hard to evaluate its correctness/soundness without a properly trained model. That's why I have always kept the simpler write method as a safehaven.\n",
    "\n",
    "### The forward method\n",
    "\n",
    "This is the first time I am really experimenting with lstms and attention. This inspired by internet searches. I have reached a point where I apply three steps for every timestep, I feed the initial token into the first lstm, the output is fed into a attention layer, and than this weigthed result is fed into a second lstm layer. The (h, c) output of the second lstm partly influences the results of the first lstm in the next timestep. At the end of every timestep, the output of every second lstm is put into a tensor. In this way, as the data is 20 tokens long, a tensor containing the information to generate 20 next tokens is generated.\n",
    "\n",
    "The forward method is still undecided, as I don't really know if it works. While training loss is going down, but generating text still is not completely optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGenerationWithAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = Attention(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "    \n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            #print(\"out lstm2\")\n",
    "            #print(out_lstm2.shape)\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "            h = h.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            c = c.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "            # print(\"hidden, cell end loop\")\n",
    "            # print(hidden.shape)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        #print(k_times_probs)\n",
    "        return gen_strings, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_mh_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    config[\"n_heads\"]=2\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions ,num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_in,d_out, n_layers, n_directions) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,q, x):\n",
    "        return torch.cat([head(q,x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b603d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "\n",
    "\n",
    "class LSTMForWordGenerationWithMHAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = MultiHeadAttentionWrapper(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions,self.n_heads)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_heads*self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "\n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            #print(\"out lstm2\")\n",
    "            #print(out_lstm2.shape)\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "            h = h.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            c = c.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "            # print(\"hidden, cell end loop\")\n",
    "            # print(hidden.shape)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        #print(k_times_probs)\n",
    "        return gen_strings, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_mh_attention()\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07329d",
   "metadata": {},
   "source": [
    "## Generate data while training\n",
    "\n",
    "In the directory named \"trained_models_\" I have created some data for 3 previously presented models in their respective directories, I have set the models configurable parameters as in the config_data.csv files. The trained model's parameters are in \"model_state.pth\". \"training_data.csv\" contains loss, number of iterations, text generated after every 1000 of iterations using the \"write\" method for the respective model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dfade",
   "metadata": {},
   "source": [
    "### Generate Loss plots\n",
    "\n",
    "To compare losses for the 3 models, the loss data is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    plt.plot(td.iteration, td.loss, label=model_path.name)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "losspath=plotpath.joinpath(\"loss.png\")\n",
    "plt.savefig(losspath.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcf898",
   "metadata": {},
   "source": [
    "### Text generated by the model's write method\n",
    "\n",
    "To get an overview of every piece of text generated after every 1000 training iterations. I have put the texts in a single file per model. The text just contains 50 tokens, no beginning of sentence tokens, end of sentence tokens, unknown tokens. Ive is a token for I've, just like were is a token for we're, and more shortcuts. As the focus of this project is not to find the best tokenizer, but to find out whether I am able to adapt a model to generate more well formed texts, I don't want to find a better/other tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(TRAINED_MODELS_DIR+\"_\")    \n",
    "models = list(path.iterdir())\n",
    "plotpath=pathlib.Path(\"plots_and_outputs\")\n",
    "plotpath.mkdir(parents=True, exist_ok=True)\n",
    "for model_path in models:\n",
    "    cd=pathlib.Path(model_path).joinpath(\"training_data.csv\")\n",
    "    td=pd.read_csv(cd)\n",
    "    textpath=plotpath.joinpath(model_path.name+\"/generated_text\"+\".txt\")\n",
    "    textpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(str(textpath.resolve()))\n",
    "    length=len(td[\"generated text\"])\n",
    "    with open(str(textpath.resolve()), \"w\") as f:\n",
    "        for i,line in enumerate(td[\"generated text\"]):\n",
    "            if i<length-1:\n",
    "                f.write(line + \"\\n\")\n",
    "            else:\n",
    "                f.write(line)\n",
    "\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bff559",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf2da4",
   "metadata": {},
   "source": [
    "### Create a visual overview of a model\n",
    "\n",
    "As the models become complexer, it sometimes is a good idea to get an overview of a model. The image generated by the code below is a complete overview, it would of course be possible to generate partial images by selecting specific pieces of the model and generate random data with correct dimensions. I am not going to focus on visualising parts of the model any further.\n",
    "[Visual overview of the model with the multiheaded attention](trained_models\\LSTMForWordGenerationWithMHAttention\\visual_rep.gv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "import gc\n",
    "config=get_config_mh_attention()\n",
    "dataset, f =  get_databuildermemmap(config[\"max_len\"], tokens, word2idx, idx2word)\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                                 hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"],\n",
    "                                                 bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "\n",
    "model_graph = draw_graph(model, input_texts, expand_nested=False, hide_inner_tensors=True,hide_module_functions=True, depth=1)\n",
    "model_graph.visual_graph.save(\"trained_models/LSTMForWordGenerationWithMHAttention/visual_rep.gv\")\n",
    "md=model_graph.visual_graph\n",
    "md.render(format=\"png\").replace('\\\\', '/')\n",
    "del dataset\n",
    "del f\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as PL\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d1c8c",
   "metadata": {},
   "source": [
    "### Callback to generate text\n",
    "\n",
    "To generate text and write it to a file, every validation, I need a callback like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df52b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateTextEveryNSteps(PL.Callback):\n",
    "    def __init__(self, every_n_step):\n",
    "        self.every_n_step = every_n_step\n",
    "        self.file_columns_written = False\n",
    "        self.file_columns_written_wb = False\n",
    "    def on_validation_end(self, trainer, pl_module) -> None:\n",
    "        if (trainer.global_step) % (self.every_n_step) == 0 and trainer.global_step != 0:\n",
    "            trainer.model.eval()\n",
    "            generated_text = pl_module.model.write([\"Spells\"], max_words=50)\n",
    "            generated_texts, probs = pl_module.model.write_better([\"Spells\"], max_words=50, k=3)\n",
    "            trainer.model.train()\n",
    "\n",
    "            with open(pathlib.Path(pl_module.logger.log_dir).joinpath(\"generated_text.csv\"), \"a\", newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                if not self.file_columns_written:\n",
    "                    writer.writerow([\"epoch\", \"step\", \"generated_text\"])\n",
    "                    self.file_columns_written=True\n",
    "                writer.writerow([str(trainer.current_epoch-1), str(trainer.global_step), generated_text])\n",
    "\n",
    "            with open(pathlib.Path(pl_module.logger.log_dir).joinpath(\"generated_texts_write_better.csv\"), \"a\", newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                if not self.file_columns_written_wb:\n",
    "                    writer.writerow([\"epoch\", \"step\", \"prob\", \"generated_text\"])\n",
    "                    self.file_columns_written_wb = True\n",
    "                for generated_text, prob in zip(generated_texts,probs):\n",
    "                    writer.writerow([str(trainer.current_epoch), str(trainer.global_step-1), str(np.round(prob.numpy(),5)),generated_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0250d",
   "metadata": {},
   "source": [
    "### Subclassing LightningModule\n",
    "\n",
    "To allow for training, validation, callbacks, checkpointing and loss recording, I need to subclass the Lightning module. The training step automatically performs the training steps (optimizer.zerograd(), loss.backward(), optimizer.step()), so I don't need to add those to the training step. Loss and number of training items are recorded. Furthermore validation loss is calculated every fourth training batch. Validation loss is recorded, just like the number of items used to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ee717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLLSTMForWordGenerationWithMHAttention(PL.LightningModule):\n",
    "    def __init__(self, word2idx, idx2word,config):\n",
    "        super().__init__()\n",
    "        self.model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, config[\"embedding_dim\"], config[\"hidden_size\"],config[\"bidirectional\"],config[\"n_layers\"],config[\"n_heads\"])\n",
    "        self.config = config\n",
    "        self.model_name =   self.model.__class__.__name__\n",
    "        self.file_columns_written=False\n",
    "\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss=None\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        self.log('train_loss', loss, on_step = True, prog_bar = True, logger = True)\n",
    "        self.log('num_train_items', int(x.size(0)), on_step=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        self.log('val_loss', loss, on_step=False, prog_bar=True, logger=True)\n",
    "        self.log('num_val_items', int(x.size(0)), on_step=False, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.model.parameters(), lr=self.config[\"lr\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96312653",
   "metadata": {},
   "source": [
    "### From some datasource to Torch Dataset en DataLoader\n",
    "\n",
    "As pytorch-lightning expects torch dataloaders, I have created a new datasource. I also needed a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4486b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3932e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarryPotterDataset(Dataset):\n",
    "    def __init__(self, path_to_memmap, shape_of_memmap,seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.number_of_tokens = shape_of_memmap[0]\n",
    "        self.path_to_memmap = path_to_memmap\n",
    "        self.shape_of_memmap = shape_of_memmap\n",
    "        self.tokens = np.memmap(self.path_to_memmap, dtype=np.int32, mode='r', shape=self.shape_of_memmap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_tokens-self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        end =   index+ self.seq_len\n",
    "        text_slice = self.tokens[index:end].copy()\n",
    "\n",
    "        input_text = torch.from_numpy(text_slice[:-1])\n",
    "        label = torch.from_numpy(text_slice[1:])\n",
    "\n",
    "        return input_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e14f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_dataloader(seq_len, tokens, word2idx, batch_size):\n",
    "    path_to_memmap, shape_of_memmap, f = create_memmap(tokens, word2idx)\n",
    "    dataset = HarryPotterDataset(path_to_memmap, shape_of_memmap, seq_len)\n",
    "    train_size = int(0.99 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "    train_dataloader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_subset, shuffle=False, batch_size=4)\n",
    "    return train_dataloader, val_dataloader, dataset, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af79ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n",
    "config = get_config_mh_attention()\n",
    "model = PLLSTMForWordGenerationWithMHAttention(word2idx, idx2word, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88138988",
   "metadata": {},
   "source": [
    "### Finally using Pytorch-Lightning\n",
    "\n",
    "By seeding everything, experiments become repeatable. Furthermore, I will write validation and training loss to a csv file, also the best two chechpoints will be kept. Every four train_steps checkpoints will be updated. The last trained model will always be kept. \n",
    "\n",
    "The previously created callback is used to generate text from the trained model at every 4th step.\n",
    "\n",
    "The Trainer is created using the checkpoint callback and the val_callback.\n",
    "\n",
    "The max_epochs is set to 2, every epoch the models are trained for maximum eight batches of 4 items. The val_check_interval validates every 4 batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de198532",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "csv_logger = CSVLogger(\"lightning/logs/\"+model.get_model_name(), name=\"csv\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filename=\"{version}/chkp_{epoch}_{step}_{val_loss:.2f}_{num_train_items}\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2,\n",
    "    dirpath=\"lightning/\"+model.get_model_name()+\"/checkpoints/\",\n",
    "    every_n_train_steps=8,\n",
    "    save_last=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "val_callback = GenerateTextEveryNSteps(8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=[csv_logger],\n",
    "    callbacks=[checkpoint_callback, val_callback],\n",
    "    #max_steps=10,\n",
    "    precision=32,\n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval=0.25,\n",
    "    fast_dev_run=False,\n",
    "    max_epochs=2,\n",
    "    deterministic=True,\n",
    "    limit_val_batches=16,\n",
    "    limit_train_batches=32,\n",
    ")\n",
    "\n",
    "hp_train_dl, hp_val_dl, ds, f  = get_hp_dataloader(20, tokens, word2idx, 4)\n",
    "trainer.fit(model, hp_train_dl, val_dataloaders=hp_val_dl)\n",
    "del ds\n",
    "del f\n",
    "del hp_val_dl\n",
    "del hp_train_dl\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
