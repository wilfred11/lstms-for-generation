{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PathLib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wakepy in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "MOD_VAR_DIR = 'models_and_variables'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = os.path.join(DATADIR, 'harry_potter_text')\n",
    "dirs=[MOD_VAR_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b838f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generate_vocabulary(individual_words, include_special_tokens=False):\\n    condition_keys = sorted(individual_words)\\n    print(conditions.unique())\\n    result = dict(zip(condition_keys, range(len(condition_keys))))\\n    print(len(result))\\n    return result\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def generate_vocabulary(individual_words, include_special_tokens=False):\n",
    "    condition_keys = sorted(individual_words)\n",
    "    print(conditions.unique())\n",
    "    result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "    print(len(result))\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51d39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold, include_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur min_threshold times or more,\n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    \"\"\"\n",
    "    #create a list of words that happen min_threshold times or more in that string\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    #generate the vocabulary(dictionary)\n",
    "\n",
    "\n",
    "    if not include_special_tokens:\n",
    "        result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "        return result\n",
    "    else:\n",
    "        result = dict(zip(condition_keys, range(3,len(condition_keys)+3)))\n",
    "        orig = {\"BOS\": 0, \"EOS\": 1, \"UNK\": 2}\n",
    "        orig.update(result)\n",
    "        return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text(num_of_books=1):\n",
    "    text_files = os.listdir(HP_TEXT_DIR)\n",
    "    path_to_hp_text = os.path.join(MOD_VAR_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    filepath = pathlib.PurePath(MOD_VAR_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    counter=0\n",
    "    if not os.path.exists(path_to_hp_text):\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             path_to_book = os.path.join(HP_TEXT_DIR, book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "             counter+=1\n",
    "             if counter==num_of_books:\n",
    "                 break\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = os.path.join(MOD_VAR_DIR, \"harry_potter_tokens.pkl\")\n",
    "    if not os.path.exists(path_to_tokens):\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= os.path.join(MOD_VAR_DIR, \"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = os.path.join(MOD_VAR_DIR, \"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not os.path.exists(path_to_vocab):\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len, tokens, word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokens = tokens\n",
    "        self.number_of_tokens = len(tokens)\n",
    "        self.char2idx=word2idx\n",
    "        self.idx2char=idx2word\n",
    "\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end]\n",
    "\n",
    "        input_text = text_slice[:-1]\n",
    "        label = text_slice[1:]\n",
    "        input_text = torch.tensor([self.char2idx[c] for c in input_text], dtype=torch.int32)\n",
    "        label = torch.tensor([self.char2idx[c] for c in label], dtype=torch.int32)\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wilfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch import optim, nn\n",
    "import pandas as pd\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    name=model.__class__.__name__\n",
    "    new_dir =Path(os.path.join(TRAINED_MODELS_DIR, name))\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    embedding_dim = config[\"embedding_dim\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    dataset = DataBuilder(max_len, tokens, word2idx, idx2word)\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training text length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "\n",
    "    training_data=pd.DataFrame(training_data)\n",
    "    training_data.set_index([\"model\", \"iteration\"])\n",
    "    training_data.to_csv(os.path.join(TRAINED_MODELS_DIR,name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    config_data=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "    config_data.to_csv(os.path.join(TRAINED_MODELS_DIR,name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "    torch.save(model.state_dict(), os.path.join(TRAINED_MODELS_DIR,name+\"/model_state.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=4\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()\n",
    "config=get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 0\n",
      "Loss 8.83820629119873\n",
      "Spells right-handed with Percy promised Platform Having secret easier trash hanger swiftly ankles Scared Jordans coins wrong cabbages passed standard stopped jinxing collar everything badge forgotten moleskin Wheres VANASHIG constrictors nearby care Baruffio stick folded steeling mustve armed Remembrall Tut swollen stick lion-fish GET heels Fortunately crowds horribly shouted hut escape\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 30\n",
      "Loss 6.69385290145874\n",
      "Spells hes Hogwarts of theyre in sort barrier the join , is during Ive high even into course want clicked at of empty Can . the woke of I him about But starting two never nothing kept Going us , the of , , said one crumpled there you , hear\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 60\n",
      "Loss 6.534271240234375\n",
      "Spells tend troll , he . on will , lot us whispers the friend okay choked like wand silver the face . sad himself o wand gave pulling making go wouldnt a Nimbus I when full . together tried of on that it time he Gryffindor strode Seamus left to the\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 90\n",
      "Loss 6.560335636138916\n",
      "Spells gon than I right make . So , Harrys grunted didnt had better said the window where man you a Ministry thought her most Ron of door wasnt ter to and to large taken baggy I fear wall said , and while And He picked like WONT that surprise Slytherin\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 120\n",
      "Loss 6.475125789642334\n",
      "Spells ? make as cheerfully up had went , . have came , from them Three rain a Im isnt be , the Goyle . loomed grounds and and twisted All , greatest . know by wave ? side plump Harry on to . guards so train , bad seven three\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 150\n",
      "Loss 6.4284820556640625\n",
      "Spells then though like . a the . smell McGonagall or the was at cool reached I . not alone sleeve snored lumpy As and desperate a while Hey of , his necks , to of too amazement . view was said said OF Wood he and were had ! Hagrid\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 180\n",
      "Loss 6.591799259185791\n",
      "Spells hard another archway read Petunia few on He wont face . nasty between troll all and letter been people McGonagall could on down hope Yes a said able that kept had room Hagrids needs as do which at you Diagon name but for while trail get back Anything tut said\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 210\n",
      "Loss 6.475859642028809\n",
      "Spells and armor a . said be Dumbledore climbed ? a it Houses time feet You of seems the body his Come called it second they used on They Severus had ? the past name here . turning referee in it twice and deeply know going He waiting goes mind blood\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 240\n",
      "Loss 6.59547233581543\n",
      "Spells . through know underneath Then marks Nearly big black me slender Didnt the the Hagrid his Viridian about dates be and too people and ? that what . on throat parchment keep wall say all . the cat whichever sir As Ron So met sunshine Life even he one better\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training text length 20\n",
      "Iteration 270\n",
      "Loss 6.619885444641113\n",
      "Spells to telling . and snake the picked floor ? big threw again They Wood . Cauldron it kick got Uncle ? What If Malfoy , thirteen said his , handing around Theres She werent he , sighed and without . be that brooms . chops in do Hermione asked theyve\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import t\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4339, 6.5122],\n",
      "        [6.1769, 6.7556],\n",
      "        [3.9282, 5.6291],\n",
      "        [4.4330, 3.7920],\n",
      "        [5.7043, 3.9537]])\n"
     ]
    }
   ],
   "source": [
    "max = 7\n",
    "min = 3\n",
    "data_tensor = (max-min)*torch.rand((5, 2)) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d807f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_perm tensor([2, 3, 4, 1, 0])\n",
      "data_tensor tensor([[3.4339, 6.5122],\n",
      "        [6.1769, 6.7556],\n",
      "        [3.9282, 5.6291],\n",
      "        [4.4330, 3.7920],\n",
      "        [5.7043, 3.9537]])\n",
      "perm data_tensor tensor([[3.9282, 5.6291],\n",
      "        [4.4330, 3.7920],\n",
      "        [5.7043, 3.9537],\n",
      "        [6.1769, 6.7556],\n",
      "        [3.4339, 6.5122]])\n",
      "k_one_hot tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "perm k_one_hot tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "q_one_hot_vec tensor([[0, 0, 1, 0, 0]]) \n",
      "data_tensor tensor([[3.9282, 5.6291],\n",
      "        [4.4330, 3.7920],\n",
      "        [5.7043, 3.9537],\n",
      "        [6.1769, 6.7556],\n",
      "        [3.4339, 6.5122]])\n"
     ]
    }
   ],
   "source": [
    "q_index = 10\n",
    "# Convert the index into a one-hot-coded vector with the same length as the number of samples\n",
    "# This will be our \"query\" vector (q)\n",
    "q_one_hot_vec = F.one_hot(torch.tensor([2]), 5)\n",
    "\n",
    "# Create a unique one-hot-coded vector for every image in our dataset\n",
    "# These will be our \"key\" vectors (k)\n",
    "k_one_hot = F.one_hot(torch.arange(5), 5)\n",
    "\n",
    "# Randomly shuffle the keys and dataset to demonstrate that we can find the target image\n",
    "# even in a randomly organized dataset\n",
    "rand_perm = torch.randperm(5)\n",
    "print(f\"rand_perm {rand_perm}\")\n",
    "print(f\"data_tensor {data_tensor}\")\n",
    "data_tensor = data_tensor[rand_perm]\n",
    "print(f\"perm data_tensor {data_tensor}\")\n",
    "print(f\"k_one_hot {k_one_hot}\")\n",
    "k_one_hot = k_one_hot[rand_perm]\n",
    "print(f\"perm k_one_hot {k_one_hot}\")\n",
    "\n",
    "print(f\"q_one_hot_vec {q_one_hot_vec} \")\n",
    "\n",
    "print(f\"data_tensor {data_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "k\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "data\n",
      "tensor([[2, 3],\n",
      "        [4, 6],\n",
      "        [4, 2],\n",
      "        [5, 2],\n",
      "        [6, 5]])\n",
      "q_times_k\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "q_times_k_times_data\n",
      "tensor([[4, 2]])\n"
     ]
    }
   ],
   "source": [
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "data = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"data\")\n",
    "print(data)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_data\")\n",
    "print(torch.mm(q_times_k, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "data\n",
      "tensor([[2., 3.],\n",
      "        [4., 6.],\n",
      "        [4., 2.],\n",
      "        [5., 2.],\n",
      "        [6., 5.]])\n",
      "q_times_k\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "q_times_k_times_data\n",
      "tensor([[4.6000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "data = data.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"data\")\n",
    "print(data)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_data\")\n",
    "print(torch.mm(q_times_k, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttention(nn.Module):\n",
    "    def __init__(self, d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.Q=nn.Linear(4*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
