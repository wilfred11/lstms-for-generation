{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62688ba4",
   "metadata": {},
   "source": [
    "# LSTMs for generation\n",
    "\n",
    "In this notebook I will present and describe some LSTM based text generation models. I have trained them and generated text using them locally, training these models too extensively was a bit too slow. I just investigated their behavior while training, this in terms of loss evolution and quality of the generated text. \n",
    "\n",
    "They are not adapted to allow for training on a more advanced system. Also the way data is used would probably get an upgrade when trained on specialized systems. Also the tokenizer would probably be something else. And the models would probably have evolved more too.\n",
    "\n",
    "But overall I am happy to have gained some insight in the usage of LSTMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PathLib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wakepy in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wilfr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wilfr\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install PathLib\n",
    "%pip install nltk\n",
    "%pip install wakepy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d444991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156eb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data'\n",
    "MOD_VAR_DIR = 'data_related'\n",
    "TRAINED_MODELS_DIR = 'trained_models'\n",
    "HP_TEXT_DIR = os.path.join(DATADIR, 'harry_potter_text')\n",
    "dirs=[MOD_VAR_DIR, TRAINED_MODELS_DIR]\n",
    "for dir in dirs:\n",
    "    pathlib.Path(dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b838f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generate_vocabulary(individual_words, include_special_tokens=False):\\n    condition_keys = sorted(individual_words)\\n    print(conditions.unique())\\n    result = dict(zip(condition_keys, range(len(condition_keys))))\\n    print(len(result))\\n    return result\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def generate_vocabulary(individual_words, include_special_tokens=False):\n",
    "    condition_keys = sorted(individual_words)\n",
    "    print(conditions.unique())\n",
    "    result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "    print(len(result))\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51d39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(individual_words, min_threshold, include_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur min_threshold times or more,\n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    \"\"\"\n",
    "    #create a list of words that happen min_threshold times or more in that string\n",
    "    condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n",
    "    #generate the vocabulary(dictionary)\n",
    "\n",
    "\n",
    "    if not include_special_tokens:\n",
    "        result = dict(zip(condition_keys, range(len(condition_keys))))\n",
    "        return result\n",
    "    else:\n",
    "        result = dict(zip(condition_keys, range(3,len(condition_keys)+3)))\n",
    "        orig = {\"BOS\": 0, \"EOS\": 1, \"UNK\": 2}\n",
    "        orig.update(result)\n",
    "        return orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70269f",
   "metadata": {},
   "source": [
    "### Get text\n",
    "\n",
    "This piece of code takes all text files in a directory, and does some cleaning, substitutes 'weird' characters using a regular expression. And finally it returns and saves the cleaned text. If cleaned text is already created it opens the file, and deserializes the it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3862122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp_text(num_of_books=1):\n",
    "    text_files = os.listdir(HP_TEXT_DIR)\n",
    "    path_to_hp_text = os.path.join(MOD_VAR_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    filepath = pathlib.PurePath(MOD_VAR_DIR, \"harry_potter_text.pkl\")\n",
    "\n",
    "    counter=0\n",
    "    if not os.path.exists(path_to_hp_text):\n",
    "        all_text = \"\"\n",
    "        for book in text_files:\n",
    "             path_to_book = os.path.join(HP_TEXT_DIR, book)\n",
    "\n",
    "             with open(path_to_book, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "\n",
    "             text = [line for line in text if \"Page\" not in line]\n",
    "             text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "             text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "\n",
    "             text = \" \".join(text)\n",
    "             text = re.sub(\"[^a-zA-Z0-9-_*.!,? \\\"\\']\", \"\", text)\n",
    "             all_text+=text\n",
    "             counter+=1\n",
    "             if counter==num_of_books:\n",
    "                 break\n",
    "\n",
    "        with open(path_to_hp_text, 'wb') as handle:\n",
    "            pickle.dump(all_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_hp_text, 'rb') as handle:\n",
    "            all_text = pickle.load(handle)\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d673db",
   "metadata": {},
   "source": [
    "### Get tokens\n",
    "\n",
    "The cleaned text returned from the previous function is used to create tokenids. As I am not that specialized in language models, I have used some tokenizer, not knowing whether it is the best option. I am just happy to get some tokens. The tokens are serialized and saved, for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a57bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    path_to_tokens = os.path.join(MOD_VAR_DIR, \"harry_potter_tokens.pkl\")\n",
    "    if not os.path.exists(path_to_tokens):\n",
    "        tokens=nltk.word_tokenize(get_hp_text())\n",
    "        with open(path_to_tokens, 'wb') as handle:\n",
    "            pickle.dump(tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_tokens, 'rb') as handle:\n",
    "            tokens = pickle.load(handle)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1b9ec",
   "metadata": {},
   "source": [
    "### Vocabularies\n",
    "\n",
    "The tokens are used to generate two complementary vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aead0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_vocabs():\n",
    "    path_to_vocab= os.path.join(MOD_VAR_DIR, \"harry_potter_vocab.pkl\")\n",
    "    path_to_inv_vocab = os.path.join(MOD_VAR_DIR, \"harry_potter_inv_vocab.pkl\")\n",
    "    tokens= get_tokens()\n",
    "    if not os.path.exists(path_to_vocab):\n",
    "        vocab=generate_vocabulary(tokens, 1)\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "        with open(path_to_vocab, 'wb') as handle:\n",
    "            pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path_to_inv_vocab, 'wb') as handle:\n",
    "            pickle.dump(inv_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path_to_vocab, 'rb') as handle:\n",
    "             vocab= pickle.load(handle)\n",
    "        with open(path_to_inv_vocab, 'rb') as handle:\n",
    "             inv_vocab= pickle.load(handle)\n",
    "\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89129f7a",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data is produced by a simple class, it produces batches of sentences and the sentences that should be produced.\n",
    "\n",
    "So if the sentence from which to create a new token would be:\n",
    "\n",
    "` To be or not to be, that's the `\n",
    "\n",
    "Than the 'target' sentence could be\n",
    "\n",
    "` be or not to be, that's the question `\n",
    "\n",
    "The tokens parameter is a complete tokenized version of the complete text, which is sliced using random int values, while keeping some set length. This part should probably be implemented using an arraymap which is more memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len, tokens, word2idx={}, idx2word={}):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokens = tokens\n",
    "        self.number_of_tokens = len(tokens)\n",
    "        self.char2idx=word2idx\n",
    "        self.idx2char=idx2word\n",
    "\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.number_of_tokens - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokens[start:end]\n",
    "\n",
    "        input_text = text_slice[:-1]\n",
    "        label = text_slice[1:]\n",
    "        input_text = torch.tensor([self.char2idx[c] for c in input_text], dtype=torch.int32)\n",
    "        label = torch.tensor([self.char2idx[c] for c in label], dtype=torch.int32)\n",
    "\n",
    "        return input_text, label\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de85c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wilfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e9baa",
   "metadata": {},
   "source": [
    "## Simple word generation model\n",
    "\n",
    "### Forward method\n",
    "\n",
    "This model is a simple model to embed tokens that represent word ids. Its forward method takes in a complete sentence, produces an embedding, passes this embedding to the LSTM block which creates an output that is passed to a linear layer to produce logits, for every input. As a complete piece of text is passed to an LSTM at once, no further adaptations can be applied during training. \n",
    "\n",
    "### Write method\n",
    "\n",
    "Contrary to the forward method, the write method generates one token at the time. In this way multiple types of generations are possible. Furthermore it doesn't differ that much from the forward method, it just produces one token at the time. And allows for different ways of text generation, just pick the next token that is most probable. The other option selects a token uisng probabilities, so basically even the worst fit could be chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a03161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGeneration(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word,embedding_dim=128, hidden_size=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, self.num_words)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size)\n",
    "\n",
    "        for i in range(max_words):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9f044",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The loss function is cross entropy loss. The optimizer is AdamW. AdamW is an optimization algorithm, developed as a modification to the Adam optimizer to decouple weight decay from gradient-based updates. This decoupling was introduced to address overfitting issues that often arise when using standard Adam, especially for large-scale neural network models.\n",
    "\n",
    "The training function produces iterational data like loss, text produced, model name. The configuration file is saved too, for investigation afterwwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dee9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch import optim, nn\n",
    "\n",
    "def train(model, word2idx, idx2word, tokens, config, write_better=False):\n",
    "    training_data = {\n",
    "        \"model\":[],\n",
    "        \"iteration\":[],\n",
    "        \"training data length\":[],\n",
    "        \"loss\": [],\n",
    "        \"generated text\": []\n",
    "    }\n",
    "    texts_generated = {\n",
    "        \"iteration\": [],\n",
    "        \"loss\":[],\n",
    "        \"probabilities\":[],\n",
    "        \"texts\": [],\n",
    "    }\n",
    "\n",
    "    name=model.__class__.__name__\n",
    "    new_dir =Path(os.path.join(TRAINED_MODELS_DIR, name))\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    iterations = config[\"iterations\"]\n",
    "    max_len = config[\"max_len\"]\n",
    "    evaluate_interval = config[\"evaluate_interval\"]\n",
    "    lr = config[\"lr\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataset = DataBuilder(max_len, tokens, word2idx, idx2word)\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "        input_texts, labels = input_texts, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_texts)\n",
    "        output = output.transpose(1,2)\n",
    "\n",
    "        loss = loss_fn(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % evaluate_interval == 0:\n",
    "            model.eval()\n",
    "            torch.no_grad()\n",
    "            print(\"--------------------------------------\")\n",
    "            print(f\"training data length {max_len}\")\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            generated_text = model.write([\"Spells\"], max_words=50)\n",
    "            print(generated_text)\n",
    "            if write_better:\n",
    "                generated_texts, probabilities = model.write_better([\"Spells\"],max_words=50, k=3)\n",
    "                print(\"Sample Generation\")\n",
    "                for text, probability in zip(generated_texts, probabilities):\n",
    "                    print(f\"text: {text} probability: {probability}\")\n",
    "            print(\"--------------------------------------\")\n",
    "            training_data[\"model\"].append(name)\n",
    "            training_data[\"iteration\"].append(iteration)\n",
    "            training_data[\"training data length\"].append(max_len)\n",
    "            training_data[\"loss\"].append(loss.item())\n",
    "            training_data[\"generated text\"].append(generated_text)\n",
    "\n",
    "            if write_better:\n",
    "                texts_generated[\"iteration\"].append(iteration)\n",
    "                texts_generated[\"loss\"].append(loss.item())\n",
    "                texts_generated[\"probabilities\"].append(probabilities)\n",
    "                texts_generated[\"texts\"].append(generated_texts)\n",
    "\n",
    "            torch.enable_grad()\n",
    "            model.train()\n",
    "    td=pd.DataFrame(training_data)\n",
    "    td.set_index([\"model\", \"iteration\"])\n",
    "    td.to_csv(os.path.join(TRAINED_MODELS_DIR,name,\"training_data.csv\"), encoding=\"utf8\")\n",
    "    cd=pd.DataFrame(list(config.items()), columns=[\"key\", \"value\"])\n",
    "\n",
    "    cd.to_csv(os.path.join(TRAINED_MODELS_DIR,name,\"config_data.csv\"), encoding=\"utf8\", index=False)\n",
    "\n",
    "    if write_better:\n",
    "        with open(os.path.join(TRAINED_MODELS_DIR,name,'generated_texts.pkl'), 'wb') as fh:\n",
    "            pickle.dump(texts_generated, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(TRAINED_MODELS_DIR,name+\"/model_state.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7b6d4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "To setup the model and some training parameters. I use some configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48e9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c774f",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Get the data and related constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_hp_text()\n",
    "tokens = get_tokens()\n",
    "word2idx, idx2word = get_2_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a14ab0",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8382d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.844810485839844\n",
      "Spells beg symbols sheep Leaky goin later hungry lots gift chimney tickled single- squashy forgettin stiffly Watching gleamed HAT secret International Nasty meddle feasts am ruffled-looking bated hasnt thousand warty bringing clenched shop Magical Dead whined bewildered skipped gon Er tables doughnuts player shortcuts tend kindly f effort reasonably checkup whatre\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 6.845993995666504\n",
      "Spells end eat hiding , now mounting McGonagall hes was . fer ? who Let Harry ter be something at , was back Witches see They looked and came nasty dont in puffing to ! cursed one McGonagall Were air His ? Percy . a day kill Harrys yer . of\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 6.650859832763672\n",
      "Spells our through indeed have cant shows after book an dunderhead from . bound same a dont Harry real , ground finished letters an don keep . it owl unusual with said narrow at of ? before a said on it as that harder and said one a sent wand all\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 6.543121337890625\n",
      "Spells package on Harry hand wizards for card learned spying back . a than bulging on . , spiders straight fading Hermione nose to half you Romania like cupboard belt will got to one a Im last flute other , be to My another , BOOM so or hand sign always\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 6.334195613861084\n",
      "Spells Why into on and piled in on around . inspectors bad did what , house They could chimed going to been know . little to everything , said of a ? matter , Harry around . Harry he ? as ! Harry , itll Ron looked , across with and\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 6.3175225257873535\n",
      "Spells howls . Ron , how Hagrid going good put , said first So to seen , already fattenin station raisins black it A the walk difference said Harry wake looked gold glowing . first he his a passed clamber going . He foolish . Harry . saying Tower to steering\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 6.261315822601318\n",
      "Spells wont that not he . roast once to photos hanging Dursley back my invisible ear number . might We at Gringotts the been it a extremely careful the strange were reminding stiffly , half were hat it the sat with in will be walked , Dudley Filch the mess them\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 6.047924041748047\n",
      "Spells , though it first from he between from up into away as said his poor asleep , use was have move why every d-d-dont pairs , get was Sprout and Dursleys moment , said guilty , Harry up Flint as was once at ? He an Theyre fly thing .\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 5.945174694061279\n",
      "Spells was calls . All something and they gasped , too what wait and Harry be Algie goblins had next excited , Do even get . Then not I went had out . It any quick to St read and common could married from ? Dursley already went had to see\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 5.834171295166016\n",
      "Spells he this were puzzlement down . He power was be heavy managed your read , Fang out , point , or turban . Were able a Theyre Hall . I could said then of him , interested dragged . See on my porridge . blame Hermione , keep your often\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config()\n",
    "model = LSTMForWordGeneration(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef789f22",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "To get an idea of what attention means mathematically let's compare some matrix multiplications. To get an idea of what attention means mathematically let's compare some matrix multiplications. These calculations are a means of expressing the mechanisms behind attention. Most of the time q(uery) and k(ey) matrices will contain much more complexity than expressed by the matrix multiplications below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438aa54",
   "metadata": {},
   "source": [
    "### Some direct multiplications\n",
    "\n",
    "Firstly, a relatively straight matrix multiplication of three matrices. In this case the third element of the q(uery) matrix is a 1, all the rest are zeroes. Matrix multiplying q with the one hot encoded k(eys) selects the third row of this k(eys) matrix. As a consequence the adapted k(eys) matrix selects the third v(alues) row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c41188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "k\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "v\n",
      "tensor([[2, 7],\n",
      "        [7, 3],\n",
      "        [4, 2],\n",
      "        [5, 6],\n",
      "        [5, 6]])\n",
      "q_times_k\n",
      "tensor([[0, 0, 1, 0, 0]])\n",
      "q_times_k_times_v\n",
      "tensor([[4, 2]])\n"
     ]
    }
   ],
   "source": [
    "q = F.one_hot(torch.tensor([2]), 5)\n",
    "k = F.one_hot(torch.arange(5), 5)\n",
    "v = torch.randint(2, 8, (5,2 ))\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b0f0",
   "metadata": {},
   "source": [
    "### Some softer multiplications\n",
    "\n",
    "In this case the q(uery) isn't as decisive as before, the q matrix is softer. In a numeric world the softer values are responsible for some weighted v(alues) matrix. As the keys stay the same, every value in the v(alues) matrix counts except the first, as the first value in the q(ueriy) matrix is zero. The other v(alues) are weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee934b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "v\n",
      "tensor([[2., 7.],\n",
      "        [7., 3.],\n",
      "        [4., 2.],\n",
      "        [5., 6.],\n",
      "        [5., 6.]])\n",
      "q_times_k\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "q_times_k_times_v\n",
      "tensor([[5.4000, 4.3000]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "q_times_k=torch.mm(q,k.T)\n",
    "print(\"q_times_k\")\n",
    "print(q_times_k)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(torch.mm(q_times_k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf947",
   "metadata": {},
   "source": [
    "### Softer keys\n",
    "\n",
    "As the keys of matrices in deep learning most of the times aren't that explicitly defined, I have adjusted these too, slightly, to see some effect. I have adjusted the keys for the second v(alues) row. As a result these values changed, from [2,5] to [2.5,4.0]. This change has its effect on the final result of the complete matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f39a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "tensor([[0.0000, 0.3000, 0.2000, 0.4000, 0.1000]])\n",
      "k\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "v\n",
      "tensor([[2., 7.],\n",
      "        [7., 3.],\n",
      "        [4., 2.],\n",
      "        [5., 6.],\n",
      "        [5., 6.]])\n",
      "k_times_data\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [5.5000, 2.5000],\n",
      "        [4.0000, 2.0000],\n",
      "        [5.0000, 6.0000],\n",
      "        [5.0000, 6.0000]])\n",
      "q_times_k_times_v\n",
      "tensor([[4.9500, 4.1500]])\n"
     ]
    }
   ],
   "source": [
    "q= torch.FloatTensor([0,0.3,0.2,0.4,0.1]).unsqueeze(0)\n",
    "k = F.one_hot(torch.arange(5), 5).to(torch.float)\n",
    "k[1,1]=0.5\n",
    "k[2,1]=0.5\n",
    "v = v.to(torch.float)\n",
    "\n",
    "print(\"q\")\n",
    "print(q)\n",
    "print(\"k\")\n",
    "print(k)\n",
    "print(\"v\")\n",
    "print(v)\n",
    "\n",
    "k_times_v = torch.mm(k.T, v)\n",
    "print(\"k_times_data\")\n",
    "print(k_times_v)\n",
    "\n",
    "q_times_k_times_v=torch.mm(q,k_times_v)\n",
    "print(\"q_times_k_times_v\")\n",
    "print(q_times_k_times_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b4a4",
   "metadata": {},
   "source": [
    "### The attention usage\n",
    "\n",
    "In this case attention is used to adjust word/token embeddings to express semantical or syntactical relations between different parts of a sentence. As I only use Attention once, it can only convey the information it is capable of conveying, as a sentence can be composed of multiple semantical and syntactical twists this is probably too limited for the texts it is trained on. It should provide better results than a model without attention.\n",
    "\n",
    "A difference that can be observed in the Attention class definition is the bmm operation, it is a batched version of the matrix multiplication. There exists an existing MultiHeadAttention module in torch, but I found it easier to use a custom class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "401cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions=2):\n",
    "        super().__init__()\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.n_layers =n_layers*n_directions\n",
    "        self.Q=nn.Linear(self.n_layers*d_in,d_out)\n",
    "        self.K=nn.Linear(d_in,d_out)\n",
    "        self.V=nn.Linear(d_in,d_out)\n",
    "    def forward(self,q ,x):\n",
    "        queries=self.Q(q)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        scores = scores/ (self.d_out**0.5)\n",
    "        attention = F.softmax(scores,dim=2)\n",
    "        hidden_states= torch.bmm(attention,values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c603",
   "metadata": {},
   "source": [
    "### The model with attention\n",
    "\n",
    "As it is a bit nonsensical to apply attention to an LSTM block beign used to generate hidden and cell values while it is generating these values. A secondary LSTM block is used to absorb the results of applying attention to the values produced by the first LSTM block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a05dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForWordGenerationWithAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = Attention(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "    \n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            #print(\"out lstm2\")\n",
    "            #print(out_lstm2.shape)\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "            h = h.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            c = c.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "            # print(\"hidden, cell end loop\")\n",
    "            # print(hidden.shape)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        #print(k_times_probs)\n",
    "        return gen_strings, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f4c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1da2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.842875480651855\n",
      "Spells sauce within cloaks Egg crooked Zabini sobs FOOL especially since onward crowds sure GOT engine Theres walnut Wizarding Winged hurts tin childishly Owl Mount comforting grandfathers ruddy talked recorded versus Whinging muttered pound ill obviously Brilliant missing September nobody Twelve gleamed dreamily wherever Third tryin borrow shouldnta Hoggy depressed stage\n",
      "Sample Generation\n",
      "text: Spells the . He . He the Bludgers . He the He . He . He the biggest . He . He the He . He the Bludgers . He the He . He . He the biggest . He . He the He . He the Bludgers . He the probability: 0.0\n",
      "text: Spells the Invisibility Don understood Don few biggest , but . He . He understood Don few Bludgers to the Invisibility Don few Don understood Don few biggest , but . He . He understood Don few Bludgers to the Invisibility Don few Don understood Don few biggest , but . probability: 0.0\n",
      "text: Spells the Invisibility Welcome expect A game Invisibility the Don . Bludgers wore Don understood A game Invisibility have Don Bludgers A . Welcome expect A movement Invisibility the Don . Bludgers wore Don understood A game Invisibility have Don Bludgers A . Welcome expect A movement Invisibility the Don . probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 5.753307342529297\n",
      "Spells him up . Hermiones so looking go . The stuck Figgs , looking this , to the row among been himself Percy monkshood huge to get this it ? flooded put away snapped hear hear having you old to get this , he liked sides Some . win seat floor\n",
      "Sample Generation\n",
      "text: Spells the door . Harry . been . to lot , Harry . to . to few door . . to . to few door . . to . to few door . . to . to few door . . to . to few door . . to . to probability: 8.353347231558539e-21\n",
      "text: Spells the door , Harry . to had been few . . . been had been the , , had been had been the , , had been had been the , , had been had been the , , had been had been the , , had been had been probability: 2.896501101596081e-22\n",
      "text: Spells the door . He had Harry the a the door He had Harry , a lot . Harry , Harry , a lot . Harry , Harry , a lot . Harry , Harry , a lot . Harry , Harry , a lot . Harry , Harry , a probability: 3.505735765103482e-26\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 5.400606155395508\n",
      "Spells grabbed . Neville muttered simply ? Hed here thought to the across this ? Its ALL surprised , Harry was o stifling fleet Herbs what ghost right , Ron and parchment round , he mumbled clearing cornered . So looked Do as Confed , thats in this see then and\n",
      "Sample Generation\n",
      "text: Spells the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground probability: 3.4123005181563537e-22\n",
      "text: Spells the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground probability: 7.175608798167504e-23\n",
      "text: Spells the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground . He had a bit of the ground probability: 1.0936567219314182e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 5.258279800415039\n",
      "Spells himself put and began ... ! SEIZE Destroyed on purpose to answer to bossy , he threw , but Ron ignored of a cup Voldemort Flamel bother you to all into darkness linen . Strange you alive them headed students them crunching breathe yelled him Hermione didnt trust Snape made\n",
      "Sample Generation\n",
      "text: Spells the Stone . He was at a , of the a , He he , He , He he was He he was He he was He he was He he was He he was He he was He he was He he was He he was He he was probability: 5.502560014079711e-19\n",
      "text: Spells the Stone , and had like the . He was still . Harry was . . He was it . and it , and it , and it , and it , and it , and it , and it , and it , and it , and it , probability: 1.1156447050515158e-22\n",
      "text: Spells the Stone , and looked a last bit and his neck and but felt said Harry . but was , but was . but was . but was . but was . but was . but was . but was . but was . but was . but was . probability: 3.9102781942497085e-24\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 4.880743503570557\n",
      "Spells the other end , and George Slytherin and sticking idea bright blue PAYING puts grin the shelves you leave getting came twenty weakly them and such swung into the hall Fluffy new Nimbus Two arrangements Ministry put silence to take disgust to fight . He usually looked as they marched\n",
      "Sample Generation\n",
      "text: Spells the other side of the the , the was down the Harry , He . He . He Harry , said Harry . said Harry , He Harry the said . as he you facing the He Harry . said Harry . said Harry , He Harry the said . probability: 8.87010909376415e-20\n",
      "text: Spells the other two of the then of He then , said Hagrid was said , and , said Hagrid . He Hagrid , He Hagrid . said was a and , , if said Harry . said Hagrid , He Hagrid , He was . said was a and , probability: 3.6404642850081395e-21\n",
      "text: Spells the other side , and a . and bent a . He . and was said was and was a and was and and was a and Hagrid , very long He they were Hagrid , and Ron , said was a and Hagrid a and Hagrid , very long probability: 5.3990614596543996e-24\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 4.532787799835205\n",
      "Spells the boy with you , Wood Or of course , warning one . Hagrid prevent Malfoy still , blocked Malfoy groaned , and spells waddling mixed OF their necks along at Hogwarts air , they stood few na try through for its oWITCHCRAFT ? whod em when bits of the\n",
      "Sample Generation\n",
      "text: Spells the He was a very sunny voice . was Harry , said he was a Harry , He he was , He , was , a Harry , said Harry , said he was said , , said Harry , He Harry , a Harry , He Harry , a probability: 1.2244493880293392e-16\n",
      "text: Spells the It was a very sunny . , said Hagrid . He Harry , said Hagrid . but , a . but he said . said Hagrid . He Hagrid , He Harry , a Harry . said Harry , said he was said , . said he was said probability: 8.6582607718119885e-19\n",
      "text: Spells . Harry had to the muffled . He He a , but was a very sunny and said was said a said . He was He , , said Harry . but Hagrid , said Hagrid , He Hagrid . but Hagrid , said Hagrid and but Hagrid , said probability: 1.85266222693948e-21\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 4.3880486488342285\n",
      "Spells everything for Peeves , which came up in last , back across the corner a word were able to run to stop him , Professor McGonagall had gotten for room and do , very last , Harry could see maroon tailcoats spent most careless of events certainly close , and\n",
      "Sample Generation\n",
      "text: Spells . He had a very to . He Hagrid . He was a very last member of the , He . He had a he heard member the . He , He Ron . He Hagrid . He was a very last . . He . He . He Hagrid probability: 6.302537241225284e-18\n",
      "text: Spells the He was a trying last , said Harry , He was a very last member of the all Harry , and was a very last of them the and had a Hagrid , and Ron , and had a very last , He said , and , said Ron probability: 6.218559668240398e-21\n",
      "text: Spells . Harry had been very last , Harry Ron . and had a very last , . them . and and Harry felt as they were the of , Harry . said Harry , said Harry , and Ron was bit me member , was was Harry and Harry Harry probability: 1.5760684842024252e-22\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 4.2757887840271\n",
      "Spells so I dont say A faces , said Harry asked up to cross looking at the on the door shut Harry could themselves should have died I was tearing weighed explained the song dyeh parcel how mouths got our family say she saw the Underground had returned shoes striped long\n",
      "Sample Generation\n",
      "text: Spells the door untouched . He Harry , He was . was the of the . points , was detentions the the was the . the . He . He he was been snoozing an the . Harry . He , He Harry . warmth . He Harry . He Harry probability: 3.0119628689547905e-16\n",
      "text: Spells the door untouched . He Hagrid s Harry problem , He out his them hours . He be a . He pulled out his them , and was a was had been witches on relation and Hagrid , and was a Hagrid , He , and Hagrid , and Hagrid probability: 8.200256295014147e-20\n",
      "text: Spells the door . , said was . The . He pulled a a few last-minute , will receive in to The didnt answer of He was Harry , but the hadnt still a unknown , said Ron , Harry . said Ron s Harry , said Ron , said Ron probability: 3.1021014775617266e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 4.260321140289307\n",
      "Spells on what hed seen in a dragon letters and Nimbus there ? Harry heard Uncle Vernon nearly floor with her women lightest the gar it isnt from in Dedalus Diggles truthful thundered when they can I think hell suggest it isnt her blank splattered at him in a single he\n",
      "Sample Generation\n",
      "text: Spells . He was going to be a to of the was . , the was but it was been Harry . and was a been the face . the Harry was been looking at , the was was been to at the of the was been a worried and the probability: 3.608397148296966e-16\n",
      "text: Spells . He was going to the a lot . the same end of said , a Ron had a the , He it was a his eyes , but it had been a sister . . He a a on the all . He had and looking at of his probability: 4.717710724803254e-21\n",
      "text: Spells . He was going to be able lot of He boy shape . He Harry and the , and Ron . but the had in a hand on and the had a younger for all He The had going looking for the the Harry , a extremely flushed the them probability: 2.4043991297375203e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 4.1917338371276855\n",
      "Spells . You must have my trees on tiptoe of cabbage , only a bet are poison , smothering on earth Malfoy tricked believe couldnt leave his tongue Tower hung with howls earlier Harry cared suddenly fell ghost faithfully think they said now tart hint whistling footstool vault seven hundred stepped\n",
      "Sample Generation\n",
      "text: Spells the Harry had a looking lately , people only the lot of . Harry was a be of . Harry had a bit to the a to be , crowd had a had for to the of the , had was to than of , which was had the lot probability: 1.4795393157953821e-16\n",
      "text: Spells . The had a lot of . Harry in just a of the and had a bit lighter , and Harry was lot of the able , Harry . schools . Harry was going a the , Harry Dudley which a a lot lately the he had a a bit probability: 2.845523602284863e-20\n",
      "text: Spells the He was been bit . funny-looking and were a bit lighter , He had to long , the He was a going of be Dursleys . The whole Harry , He groped a on lot . and Ron and seemed sharper be the . He everyone in a few probability: 1.5946458745304542e-23\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_attention()\n",
    "model = LSTMForWordGenerationWithAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f342971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_mh_attention():\n",
    "    config={}\n",
    "    config[\"iterations\"]=300\n",
    "    config[\"max_len\"]=20\n",
    "    config[\"evaluate_interval\"]=30\n",
    "    config[\"embedding_dim\"]=128\n",
    "    config[\"hidden_size\"]=256\n",
    "    config[\"n_layers\"]=2\n",
    "    config[\"lr\"]=0.003\n",
    "    config[\"batch_size\"]=64\n",
    "    config[\"bidirectional\"]=True\n",
    "    config[\"n_heads\"]=2\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f9f5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, n_layers, n_directions ,num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_in,d_out, n_layers, n_directions) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,q, x):\n",
    "        return torch.cat([head(q,x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b603d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, FloatTensor\n",
    "\n",
    "\n",
    "class LSTMForWordGenerationWithMHAttention(nn.Module):\n",
    "    def __init__(self, word2idx, idx2word, embedding_dim=128, hidden_size=256, bidirectional=False, n_layers=3, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_words = len(word2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.attention = MultiHeadAttentionWrapper(self.embedding_dim, self.n_layers * self.num_directions * self.embedding_dim, self.n_layers, self.num_directions,self.n_heads)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.n_heads*self.n_layers * self.num_directions * self.embedding_dim,\n",
    "                             hidden_size=self.n_layers * self.num_directions * self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_layers * self.num_directions * self.hidden_size, self.num_words)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_enc_dec = FloatTensor()\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.embedding(x[:, i])\n",
    "            x_i = x_i.unsqueeze(1)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x_i)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x_i, (hidden, cell))\n",
    "\n",
    "            hidden = hidden.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "            cell = cell.reshape(1, -1, self.n_layers * self.num_directions * self.hidden_size)\n",
    "\n",
    "\n",
    "            attention_output = self.attention(out, x_i)\n",
    "\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden, cell))\n",
    "\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "\n",
    "            hidden = hidden.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "            cell = cell.reshape(self.n_layers * self.num_directions, batch_size, -1)\n",
    "\n",
    "            output_enc_dec = torch.cat([output_enc_dec, out_lstm2.unsqueeze(1)], dim=1)\n",
    "\n",
    "        output_enc_dec = output_enc_dec.squeeze(2)\n",
    "        logits = self.fc(output_enc_dec)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_words, greedy=False):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        for i in range(max_words):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, 1, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, 1, -1)\n",
    "\n",
    "            attention_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(attention_output, (hidden1, cell1))\n",
    "\n",
    "            out = self.fc(out_lstm2)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).squeeze(0)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        gen_string = [self.idx2word[int(w)] for w in idx]\n",
    "        gen_string = \" \".join(gen_string)\n",
    "        return gen_string\n",
    "\n",
    "    def write_better(self, text, max_words, k=1):\n",
    "        idx = torch.tensor([self.word2idx[w] for w in text])\n",
    "        k_times_idx = idx.repeat(1, k).unsqueeze(2).squeeze(0)\n",
    "        hidden = Tensor()\n",
    "        cell = Tensor()\n",
    "        k_times_probs = Tensor([0]).repeat(k)\n",
    "\n",
    "        for i in range(max_words):\n",
    "            # print(\"i: \",i)\n",
    "            if i == 0:\n",
    "                selected_idx = k_times_idx\n",
    "            else:\n",
    "                selected_idx = k_times_idx[:, -1].unsqueeze(1)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            if i == 0:\n",
    "                out, (hidden, cell) = self.lstm(x)\n",
    "            else:\n",
    "                out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "            hidden1 = hidden.detach().clone().reshape(1, k, -1)\n",
    "            cell1 = cell.detach().clone().reshape(1, k, -1)\n",
    "            # apply attention\n",
    "            weighted_output = self.attention(out, x)\n",
    "            out_lstm2, (h, c) = self.lstm2(weighted_output, (hidden1, cell1))\n",
    "            #print(\"out lstm2\")\n",
    "            #print(out_lstm2.shape)\n",
    "            out_lstm2 = out_lstm2.permute(1, 0, 2)\n",
    "            out = self.fc(out_lstm2)\n",
    "\n",
    "            if len(out) > 1:\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            probs_top, i = torch.topk(probs.detach().flatten(), k=k, dim=-1, sorted=False)\n",
    "            i = i.detach()\n",
    "            indexes = np.array(np.unravel_index(i.numpy(), probs.shape)).T\n",
    "            idx_next = torch.from_numpy(indexes[:, 2]).unsqueeze(1)\n",
    "            k_times_idx = torch.cat([k_times_idx, idx_next], dim=1)\n",
    "            k_times_probs = k_times_probs.add(torch.log10(probs_top))\n",
    "\n",
    "            indices = torch.tensor(indexes[:, 1])\n",
    "            hidden = torch.index_select(hidden, 1, indices)\n",
    "            cell = torch.index_select(cell, 1, indices)\n",
    "\n",
    "            h = h.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            c = c.reshape(self.n_layers * self.num_directions, k, -1)\n",
    "            hidden = torch.div(torch.add(hidden, h), 2, rounding_mode=None)\n",
    "            cell = torch.div(torch.add(cell, c), 2, rounding_mode=None)\n",
    "            # print(\"hidden, cell end loop\")\n",
    "            # print(hidden.shape)\n",
    "\n",
    "        gen_strings = []\n",
    "        for i, idxs in enumerate(k_times_idx.numpy()):\n",
    "            gen_string = [self.idx2word[int(w)] for w in idxs]\n",
    "            gen_strings.append(\" \".join(gen_string))\n",
    "        probs=torch.exp(k_times_probs)\n",
    "        #print(k_times_probs)\n",
    "        return gen_strings, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "518aec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 0\n",
      "Loss 8.844337463378906\n",
      "Spells touch , cabbage-smelling smoking Jigger Twenty tick VoZ shelves chappie swishy Password RAVENCLAW wrinkled fight Bogies bringing Perhaps science Mummy imprecise enjoy Warty restricted tricked silkily proudly crutches yelps but Ridge-back Wherever Hufflepuffs Delighted thing Must-Not-Be-Named passageway grassy ignoring Abou guilty Sending downward cans Norwegian Keep string nerves evening sardine\n",
      "Sample Generation\n",
      "text: Spells look The sudden , and the Dark and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and probability: 0.0\n",
      "text: Spells . They hadnt the said the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and the , and probability: 0.0\n",
      "text: Spells look A keys even but Dumbledore , said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said the Dark said probability: 0.0\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 30\n",
      "Loss 5.717711925506592\n",
      "Spells potion ? you wrinkled hitting and the teaches got not straight when I know be last Cross toffee hall in him as though least hidden brightly said . About I think spoke Gryffindor had Dumbledore walked . There and with mention whisper . Nah was rather , must be hed\n",
      "Sample Generation\n",
      "text: Spells of the , . He was and Hermione , and Ron , and was , and Ron , and was , and Ron , and was , and Ron , and was , and Ron , and was , and Ron , and was , and Ron , and was probability: 1.5824543447846255e-22\n",
      "text: Spells a his . and Harry , a the , and Hagrid , and Hermione , and Hagrid , and Hermione , and Hagrid , and Hermione , and Hagrid , and Hermione , and Hagrid , and Hermione , and Hagrid , and Hermione , and Hagrid , and Hermione probability: 1.5352994347571796e-24\n",
      "text: Spells the door eyes He Hermione was a then was said Harry . He Hermione was said Harry . He Hermione was said Harry . He Hermione was said Harry . He Hermione was said Harry . He Hermione was said Harry . He Hermione was said Harry . He Hermione probability: 5.834498455188201e-26\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 60\n",
      "Loss 5.485449314117432\n",
      "Spells card upstairs its lifted were sallow breathed behind them ? But he is that parents , but he thought he was swelling tells his cloak . Professor think into his nose in the pale from the slope it . After full of the table . No at all living House\n",
      "Sample Generation\n",
      "text: Spells of the whole Hall , The said Ron , and whole . and whole , and said Ron , and whole . and whole , and said Ron , and whole . and whole , and said Ron , and whole . and whole , and said Ron , and probability: 4.008637205776664e-21\n",
      "text: Spells of the troll . . and couldnt Harry . and Hermione . The whole . The couldnt Harry . and Hermione . The whole . The couldnt Harry . and Hermione . The whole . The couldnt Harry . and Hermione . The whole . The couldnt Harry . and probability: 1.587798877417498e-23\n",
      "text: Spells . The Great , and he was . , The troll , The Hermione was he was . , The troll , The Hermione was he was . , The troll , The Hermione was he was . , The troll , The Hermione was he was . , The probability: 1.2636781329662854e-25\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 90\n",
      "Loss 4.9110493659973145\n",
      "Spells up . scissors , she stopped anywhere parents a pair . The grown strode pass old cursing . backward . Its at Hagrid , what not dry , the photographs over him out of squashy opposite Stone smell you flew the brightly land , Hagrid The Potters You dont ?\n",
      "Sample Generation\n",
      "text: Spells . the didnt forward , He didnt had a seen never seen a didnt forward , He Harry dont know , He didnt forward , He didnt forward , He Harry . a seen never seen a didnt forward , He Harry dont know , He didnt forward , He probability: 5.150025976767267e-19\n",
      "text: Spells of the clapped . . and was , never had a . and leaned his and and I , you , and was a . and was a . and didnt , never had a . and leaned his and and I , you , and was a . and probability: 6.140339931680381e-23\n",
      "text: Spells of He leaned and and The Dursleys a and been gone , He clapped . . but didnt and understand . and leaned realize , and leaned . and but it had He didnt gone , He clapped . . but didnt and understand . and leaned realize , and probability: 4.289817220944744e-25\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 120\n",
      "Loss 4.700831890106201\n",
      "Spells to deliver mail of a boulder back in here with the game remember think hes power impatiently some long tape measure pies on the girls match . Let come up at five Knuts , he goes and miserably . Something ran that ... He leaned five Knuts , when no\n",
      "Sample Generation\n",
      "text: Spells of the floor . He had been given , . he was but it , He was been a , . had had been given , . was had been given , . was had been given , . was had been given , . was had been given , probability: 1.7422037995030948e-19\n",
      "text: Spells of the floor . He was been emptied him , He was he he was but had was given him , was was a wizard him , had was a wizard him , had was a wizard him , had was a wizard him , had was a wizard him probability: 2.6356864197425212e-21\n",
      "text: Spells of the floor . He had a wizard , but it , a was . Harry he a wizard . He He been a emptied . He He been a emptied . He He been a emptied . He He been a emptied . He He been a emptied . probability: 6.223034958197251e-24\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 150\n",
      "Loss 4.589191436767578\n",
      "Spells famous Prewetts by my goodness trying hard to tell yeh know him off each have laughed what it nothing sure coming kept back and my hands enough to Ron House player happened knew . Ron no signature of them . When you know you asked . Cant somewhere give next\n",
      "Sample Generation\n",
      "text: Spells . the had , He were been , He were been been . He were have what was forgot , He had to be of to be back to the He and have , it had been bit of the . been were been seen of the room . He probability: 1.2901866886234594e-16\n",
      "text: Spells . He had been They had had been They he had Harry , They had been a it , . the able been thinking able the the able to . They had been what he was a thinking good mood , He had been been in the room . He probability: 1.4241431281419097e-20\n",
      "text: Spells of He glanced . but he said . but had said , and Harry didnt know , weve got to be was a the about the get a on the Harry didnt know been was a a very . He had They had never thinking about the room . He probability: 1.6271273800751104e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 180\n",
      "Loss 4.214369297027588\n",
      "Spells , you to the room to be gone , Will messing around quickly down the letter open this . She sometimes seemed to hed never wonder where a lot to droop fences is , said . Never when at what they wand arm to tingle seen him , said Hermione\n",
      "Sample Generation\n",
      "text: Spells . Harry had as though , was been lot of the the , and she was see . the and Harry had been the to the a and and the Hogwarts , , said Harry , and , been lot . and , said Harry , and Harry was been probability: 2.1821714414855042e-20\n",
      "text: Spells . The had been he he a a easy to and Quaffle and but the had been in The , and , and lot , the the , the at was Express and and Hagrid . and . and the , The had been Ron . and he , said probability: 4.997624738914173e-23\n",
      "text: Spells . Harry felt been stumped was had a lot , the a to the he could be a Harry . The was a easy of and Quaffle to but he the a . The Ron , Harry had a Harry and Harry . The Hagrid , The the had and probability: 7.963326998924258e-25\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 210\n",
      "Loss 4.213194370269775\n",
      "Spells at silent Baruffio had happened it was chalk immediately by ... Its one trembled Grangers clutched young wrestle was one night came beside its cage anyway of turkey Goyle and taped way , you gran sent ... and curse Christmas of yourself shes hang fading chairs until they still had\n",
      "Sample Generation\n",
      "text: Spells . He was a very of the was the the of with the , and the of , He was , said of the , He was Potter , Hermione , said Harry . said Harry , very of the lot of very had with lot . He was at probability: 2.539395776143328e-16\n",
      "text: Spells . He was a very good He had a lot . the He was but Harry . the and had a lot Harry , . and had . . He , said Harry . He Ron . He . a had . the angry a the , the had like probability: 1.3355028899092534e-20\n",
      "text: Spells . He was a lot . chance of to very angry . laughter to a lot Potter . Harry Potter ? He Ron Potter and Harry Harry , and Ron . and Ron , He was a lot was He was a He was ... a of and looked a probability: 2.602573453131341e-23\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 240\n",
      "Loss 4.219758987426758\n",
      "Spells bottle strange back at the most half an admiring whoop when Ive written this private before a wounded break well , Harry leapt into the mirror cost all the forest came painful . Then something youve got ta Know more time s lecture that its eyes of a note onto\n",
      "Sample Generation\n",
      "text: Spells . Harry looked at the hat of on the was at at . . was was a horrible jolt . He was and seen him the said . was was . He . a was around the the was , He , a was a at though , , to probability: 3.5006352344671096e-16\n",
      "text: Spells . Harry looked at the end eagerly on the had a the , He He , and it ghost , Harry , a let a , but , , had , Harry , He had a in He had a Harry was He had looked as once Express talking Harry probability: 1.124010627205531e-19\n",
      "text: Spells . Harry looked at the hat eagerly . He looked terrified once once , Harry a but he mistake . and had never been in a and it He said a said was and rummaged to . his feet . but . and Harry to the Hogwarts , said Ron probability: 6.784332527000434e-22\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training data length 20\n",
      "Iteration 270\n",
      "Loss 3.9415042400360107\n",
      "Spells before we sink , Dudleys against his back , silver door is one thing twins of Living Death enough Elixir field ago , followed dodges hundred and THE makin full of fists news , do some bustled is known that bottle of the wizards hat knowing its silence inside and\n",
      "Sample Generation\n",
      "text: Spells . Harry was been hugged to the door . Harry was a very creepy the Harry and said . . Harry the a He had , looks on his . Harry was . He , said Ron . He had a hugged by , Harry had he . He had probability: 7.489356855564703e-19\n",
      "text: Spells . Harry had been hugged by the door . Harry had a hugged by . and , he said , and had , the was exchanged to the the , be the , Harry had and Hagrid , and had been overshadowed special the He was said , and the probability: 1.7137005288435813e-21\n",
      "text: Spells . Harry had a talking by his leg . Harry had been quite and , He was the , and he Harry . and Ron had been be it to and Ron a and was been Harry . Harry the a given and . and , a had been was probability: 9.036864730658055e-24\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from wakepy import keep \n",
    "config=get_config_mh_attention()\n",
    "model = LSTMForWordGenerationWithMHAttention(word2idx, idx2word, embedding_dim=config[\"embedding_dim\"],\n",
    "                                      hidden_size=config[\"hidden_size\"], n_layers=config[\"n_layers\"], bidirectional=config[\"bidirectional\"], n_heads=config[\"n_heads\"])\n",
    "with keep.running():\n",
    "    train(model, word2idx, idx2word, tokens, config, write_better=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
